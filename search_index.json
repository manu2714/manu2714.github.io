[["relación-entre-una-variable-cualitativa-y-otra-cuantitativa-ii-diseños-longitudinales.html", "3 Relación entre una variable cualitativa y otra cuantitativa (II): Diseños longitudinales", " 3 Relación entre una variable cualitativa y otra cuantitativa (II): Diseños longitudinales La principal característica de este tipo de diseños es que se administran todos los valores del factor (VI) a cada uno de los sujetos. Por tanto, se disponen de varias medidas repetidas no independientes por cada sujeto presente en la investigación. La estructura del diseño básico con 3 momentos de observación sería la siguiente: Figura 3.1: Esquema del diseño unifactorial de medidas repetidas Supongamos que estamos interesados en presentar imágenes relacionadas con emociones de alegría, tristeza o neutra y ver cómo afectan a una respuesta psico-fisiológica. Para realizar este estudio podríamos haber creado 3 grupos de individuos y asignarle al azar las imágenes de un contenido distinto. Por tanto, habría un grupo en el que sólo verían imágenes de alegría, otro imágenes de tristeza y otro imágenes de tipo neutro. Sin embargo, también cabe la posibilidad de seleccionar un único grupo y presentarle en momentos distintos cada una de las imágenes (alegría, tristeza y neutras). La ventaja de actuar así es que reduciríamos el número de sujetos presentes en el estudio con lo que se facilitaría el costo de la investigación. En cambio, tendríamos la dificultad de que las observaciones registradas en cada individuo no son independientes pudiéndose producir efectos de secuenciación que pueden ser de dos tipos generales: 1) efectos de arrastre como consecuencia de aplicar un tratamiento sin que haya desaparecido el efecto de otro aplicado previamente, y 2) efectos de la práctica resultado de la mejora de la respuesta de los sujetos como consecuencia de la repetición (Ato &amp; Vallejo, 2015; Pardo &amp; San Martin, 2010). Una forma de eliminar estos efectos de secuenciación es mediante el contrabalanceo que consiste en administrar los tratamientos en orden distintos a cada participante en la investigación. Este contrabalanceo puede aplicarse aleatoriamente pudiendo ser completo o incompleto. Para profundizar en estos procedimientos es conveniente consultar alguna obra de metodología general (León &amp; Montero, 2002). El árbol de decisión para estos ensayos aparece en la siguiente figura. Figura 3.2: Árbol de decisión del diseño univariable de medidas repetidas "],["modelo-anova-1-factor-de-medidas-repetidas-a1mr.html", "3.1 Modelo ANOVA 1 factor de medidas repetidas (A1MR)", " 3.1 Modelo ANOVA 1 factor de medidas repetidas (A1MR) La estrategia de análisis para este tipo de diseños si se cumple el supuesto de normalidad de los residuales parte del siguiente modelo: \\[ Y_{ij} = \\mu + s_i + \\alpha_{j} + \\epsilon_i \\] donde \\(Y_{ij}\\) es la puntuación del sujeto i en la condición j; \\(\\mu\\) es la media total del diseño; \\(s_i\\) son efectos aleatorios del individuo i (\\(s_i = \\mu_i - \\mu\\)) y se suponen distribuidos según una ley normal (NID(0,\\(\\sigma^2\\))), \\(\\alpha_j = \\mu_j - \\mu\\) son efectos fijos constantes para cada nivel de tratamiento independientemente de los sujetos; \\(\\epsilon_i\\) es \\(Y_{ij} - s_i-\\alpha_{j}\\) son efectos residuales aleatorios específicos para cada puntuación. En este modelo se asumen que las covarianzas entre los tratamientos y entre los individuos no existen. La hipótesis nula de este modelo es: \\[ H_0: \\alpha_1 = \\alpha_1 = \\cdots = \\alpha_J =0 \\] Por tanto, el modelo asociado a esta hipótesis (modelo reducido) es el siguiente: \\[ Y_{ij} = \\mu + s_i + \\epsilon_i \\] Los estimadores por máxima verosimilitud de los componentes del modelo son: \\[ \\hat{\\mu} = \\bar{Y}_{++} \\] \\[ \\hat{\\alpha}_j = \\bar{Y}_{+j} - \\bar{Y}_{++} \\] \\[ \\hat{s}_i = \\bar{Y}_{i+} - \\bar{Y}_{++} \\] La variabilidad total de este estudio puede descomponerse en distintos elementos: 1) varianza intersujetos: debida a los tratamientos, 2) varianza intrasujeto debida a cada uno de los sujetos, y 3) varianza del error la que se da entre cada puntuación y sus correspondientes medias marginales. Estas tres fuentes de variación pueden calcularse mediante las siguientes expresiones: Varianza intersujetos: \\[ MC_A = n\\sum_{j}(\\bar{Y}_{+j}-\\bar{Y})^2/(J-1) \\] Varianza intrasujetos: \\[ MC_S = J\\sum_{i}(\\bar{Y}_{i+}-\\bar{Y})^2/(n-1) \\] Varianza del error: \\[ MC_E = \\sum_{i}\\sum_{j}(Y_{ij}- \\bar{Y}_{i+} - \\bar{Y}_{+j}-\\bar{Y})^2/[(J-1)(n-1)] \\] Al igual que ocurría en el diseño de 1 factor completamente aleatorizado, puede compararse la variabilidad debida a los tratamientos con la variabilidad debida al error mediante el estadístico F: \\[ F = \\frac{MC_A}{MC_E} \\sim \\mathcal{F}(j-1,[(j-1)(n-1)]) \\] donde \\(\\mathcal{F}\\) es la distribución teórica. Con este estadístico podemos contrastar que las J medias son iguales. "],["ejemplo-1-de-diseño-unifactorial-de-medidas-repetidas.html", "3.2 Ejemplo 1 de diseño unifactorial de medidas repetidas", " 3.2 Ejemplo 1 de diseño unifactorial de medidas repetidas A continuación, presentamos los resultados de un estudio en el que se quiso estudiar si el contenido emocional de una imagen provocaba cambios en una respuesta psicofisiológica: Tabla 3.1: Datos del ejemplo 3.1 tristeza neutra alegria 72 73 80 64 77 84 70 83 90 62 71 86 Tabla 3.1: Correlaciones del ejemplo 3.1 tristeza neutra alegria tristeza 1.00 0.37 -0.20 neutra 0.37 1.00 0.67 alegria -0.20 0.67 1.00 Figura 3.3: Boxplot del ejemplo 3.1 Calculamos las medias cuadráticas del diseño y tenemos: Varianza intersujetos: \\[ MC_A = n\\sum_{j}(\\bar{Y}_{+j}-\\bar{Y}_{++})^2/(j-1) = 4((-9)^2 + 0^2 + 9^2)/2 = 648/2 = 324 \\] Varianza intrasujetos: \\[ MC_S = J\\sum_{i}(\\bar{Y}_{i+}-\\bar{Y}_{++})^2/(n-1) = 3((-1)^2 + (-1)^2 + 5^2 + (-3)^2)/3 = 108/3 = 36 \\] Varianza del error: \\[ MC_E = \\sum_{i}\\sum_{j}(Y_{ij}- \\bar{Y}_{i+}- \\bar{Y}_{+j}-\\bar{Y}_{++})^2/[(j-1)(n-1)] = \\\\ ((72-75-67 + 76)^2 + \\cdots + (86-73-85 + 76)^2)/(2*3) = 96/6 = 16 \\] \\[ F = \\frac{MC_A}{MC_E}= \\frac{324}{16}= 20.25 \\] La región crítica en este caso corresponde a \\(P(\\mathcal{F}_{2,6} &gt; F)\\) = 0.002. Por tanto, rechazamos la hipótesis nula de que las medias de las distintas condiciones son iguales. Estos resultados coinciden con los obtenidos mediante el programa R. ## $univariate.tests ## Sum Sq num Df Error SS den Df F value Pr(&gt;F) ## (Intercept) 69312 1 108 3 1925.33 2.606e-05 *** ## emocion 648 2 96 6 20.25 0.002148 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 "],["comprobación-de-los-supuestos-1.html", "3.3 Comprobación de los supuestos", " 3.3 Comprobación de los supuestos Ya hemos mencionado que para que pueda aplicarse el modelo estadístico de A1FMR es necesario que se cumplan las condiciones de independencia, normalidad y homogeneidad de varianzas 1. Ahora bien, en este tipo de diseño la independencia hace referencia a que las medidas de un mismo sujeto no estén relacionadas. Asimismo, el efecto de la VI debe ser independiente de la interacción del sujeto con el tratamiento. Cuando se cumplen estos supuestos, el estadístico F se distribuye como un estadístico \\(\\mathcal{F}_{gl_1,gl_2}\\) donde \\(gl_1 = j - 1\\) y \\(gl_2 = (j - 1)(n - 1)\\). Estos supuestos están relacionados con las características de la matriz de varianzas-covarianzas del diseño que contiene la varianza de cada medida en la diagonal y las covarianzas entre cada par de medidas. Esta característica de la matriz se denomina simetría compuesta y decimos que la matriz es esférica cuando las varianzas y covarianzas son iguales. Sin embargo, es muy poco probable que los datos de las investigaciones psicológicas que utilizan este tipo de diseños lo cumplan, ya que es razonable considerar que los errores de un sujeto (y sus medidas) estén correlacionados entre sí en algún grado. Por tanto, para poder aplicar el modelo estadístico necesitamos relajar estos supuestos. Para ello, se considera suficiente que las covarianzas entre las medidas sean iguales. Más concretamente, es suficiente que las varianzas de las diferencias entre cada par de medidas sean iguales 2. Si se cumple esto, el estadístico F sigue la distribución teórica con \\(\\mathcal{F}_{gl_1,gl_2}\\) donde \\(gl_1 = j - 1\\) y \\(gl_2 = (j - 1)(n - 1)\\). En el caso de que el estadístico F no siga la distribución anteriormente mencionada se vuelve liberal (aumenta la probabilidad de cometer errores tipo I), y puede demostrarse que sigue una distribución \\(\\mathcal{F}_{gl_1,gl_2}\\) donde \\(gl_1 = \\epsilon(j - 1)\\) y \\(gl_2 = \\epsilon(j - 1)(n - 1)\\) (Maxwell &amp; Delaney, 2004). El valor de \\(\\epsilon\\), es desconocido y necesitamos estimarlo a partir de nuestros datos. Se han propuesto varias alternativas para estimarlo. Una de ellas, postulada por Greenhouse-Geisser, proponen una estimación de \\(\\hat{\\epsilon}\\) que es más conservadora que la propuesta por Huyndt-Feldt (\\(\\tilde{\\epsilon}\\)). No obstante, el estadístico F es el más conservador de los tres por lo que si éste estadístico es significativo los otros dos también lo serán. En el ejemplo anterior los valores de \\(\\epsilon\\) son los siguientes: ## $sphericity.tests ## Test statistic p-value ## emocion 0.66667 0.66667 ## $pval.adjustments ## GG eps Pr(&gt;F[GG]) HF eps Pr(&gt;F[HF]) ## emocion 0.75 0.006540799 1.333333 0.0021483 ## attr(,&quot;na.action&quot;) ## (Intercept) ## 1 ## attr(,&quot;class&quot;) ## [1] &quot;omit&quot; Observamos que el valor obtenido por Geiser-Greenhouse (0.75) es menor que el de Huyndt-Feldt (1.33 \\(\\simeq\\) 1). En ambos casos, los estadísticos nos llevan a rechazar la hipótesis nula de igualdad de medias. El resultado denominado sphericity.test con valor de 0.66 es el test de Mauchly, indicándonos que se acepta la hipótesis de esfericidad. Hay ocasiones en las que existen más medidas repetidas que sujetos. En estos casos, Pardo &amp; San Martin (2010) recomiendan utilizar el estadístico F modificando los grados de libertad con el menor valor de \\(\\epsilon\\). Si no se rechaza la hipótesis con esta opción, utilizar las estimaciones de \\(\\epsilon\\) propuestas anteriormente (la de Geisser-Greenhouse o Huyndt-Feldt, cualquiera de ellas vale). Pardo &amp; San Martin (2010) y Ato &amp; Vallejo (2015) señalan con acierto que los modelos ANOVA son muy robustos en situaciones donde las variables no siguen una distribución normal salvo en aquellas en las que los diseños no están balanceados o las distribuciones son muy asimétricas. Sin embargo, mantendremos aquí el cumplimiento de las dos condiciones con objeto de facilitar el aprendizaje.↩︎ Si en el diseño sólo hay dos medidas por sujeto siempre se cumple el supuesto de esfericidad.↩︎ "],["tamaño-del-efecto-2.html", "3.4 Tamaño del efecto", " 3.4 Tamaño del efecto Las medidas propuestas para el modelo de A1CA (\\(\\eta^2\\) corregida y \\(\\omega^2\\)), también son válidas aquí. La preferida por los investigadores (Pardo &amp; San Martin, 2010) es 3: \\[ \\omega^2 = \\frac{\\sigma_{\\alpha}^2}{\\sigma_{s}^2+\\sigma_{\\alpha}^2+\\sigma_{e}^2} \\] cuando se compara el efecto relativo del tratamiento con respecto a los efectos totales del diseño. \\(\\sigma_{\\alpha}^2\\) es la varianza debida a los tratamientos, \\(\\sigma_{s}^2\\) es la varianza debida a los sujetos y \\(\\sigma_{e}^2\\) es la varianza debida al error. Maxwell &amp; Delaney (2004) también proponen utilizar la medida de \\(\\omega^2\\) parcial: \\[ \\omega^2 = \\frac{\\sigma_{\\alpha}^2}{\\sigma_{\\alpha}^2+\\sigma_{e}^2} \\] \\(\\omega^2\\) parcial también puede calcularse a partir del estadístico F 4: \\[ \\omega^2 = \\frac{(j -1)(F - 1)}{(j - 1)(F - 1) + nj} \\] A partir de \\(\\omega^2\\) parcial también puede calcularse la medida de tamaño del efecto de Cohen: \\[ \\hat{\\delta} = \\sqrt{\\frac{\\hat{\\omega}^2}{( 1 - \\hat{\\omega}^2)}} \\] Al igual que en el tema anterior los valores para considerar un efecto bajo, medio o grande son 0.01, 0.06 y 0.14 respectivamente. En el caso de \\(\\hat{\\delta}\\) sus valores son 0.1, 0.25 y y 0.4 respectivamente. Aplicado a los datos del estudio de las emociones: \\[ \\omega^2 = \\frac{(3 -1)(20.25 - 1)}{(3 - 1)(20.25 - 1) + 4*3} =0.76 \\] \\[ \\hat{\\delta} = \\sqrt{\\frac{0.76}{( 1 - 0.76)}} = 1.79 \\] Ambos estadísticos indican que la relación entre el contenido emocional de las imágenes y la respuesta psicofisiológica es muy grande. Otra medida con resultados semejantes a \\(\\omega^{2}\\) es eta cuadrado generalizada (\\(\\eta_g^{2}\\)).↩︎ En el caso de que este cálculo resulte negativo se considera que \\(\\omega^2\\) vale cero↩︎ "],["potencia-de-la-prueba.html", "3.5 Potencia de la prueba", " 3.5 Potencia de la prueba La potencia de la prueba nos permite determinar la probabilidad de que en un estudio se rechace la hipótesis nula cuando es falsa.En un diseño A1MR depende de varios factores: 1) el nivel \\(\\alpha\\) de significación, la correlación entre las VVDD, el tamaño del efecto que se desea contrastar, el número de condiciones en el diseño, el valor estimado de \\(\\epsilon\\) y el número de sujetos. Aplicado al estudio de las emociones anteriormente presentado tenemos que hemos utilizado \\(\\alpha\\) = 0.05, la correlación media entre las distintas medidas es 0.37, el tamaño del efecto medido con el estadístico \\(\\delta\\) es 1.79, tenemos 3 condiciones, el valor más conservador de \\(\\epsilon\\) es 0.75 y hemos estudiado a 4 sujetos. Bajo estas condiciones y utilizando el programa Gpower (http://www.gpower.hhu.de) obtenemos un valor de potencia de 0.9949. Por lo general, suele considerarse razonable un valor de potencia de 0.8. Si hubiéramos utilizado tres sujetos la potencia también seguiría siendo razonable (0.901). "],["comparaciones-múltiples.html", "3.6 Comparaciones múltiples", " 3.6 Comparaciones múltiples La estrategia para realizar las comparaciones múltiples es similar a la utilizada en la estrategia transversal: \\[ \\phi_i = \\sum_{j} c_{j}Y_{ij} \\] donde \\(Y_{ij}\\) son las medidas de los individuos y \\(c_j\\) son los coeficientes multiplicadores. En el caso concreto, de las comparaciones a posteriori Pardo &amp; San Martin (2010) proponen utilizar la prueba t para cada una de las diferencias. Así, por ejemplo, en nuestro caso la comparación entre la condición de tristeza y neutra será: ## ## Paired t-test ## ## data: datos$tristeza and datos$neutra ## t = -3.182, df = 3, p-value = 0.05002 ## alternative hypothesis: true mean difference is not equal to 0 ## 95 percent confidence interval: ## -18.001317453 0.001317453 ## sample estimates: ## mean difference ## -9 Para el caso donde se compara la condición de tristeza y alegría será: ## ## Paired t-test ## ## data: datos$alegria and datos$tristeza ## t = 5.1962, df = 3, p-value = 0.01385 ## alternative hypothesis: true mean difference is not equal to 0 ## 95 percent confidence interval: ## 6.975683 29.024317 ## sample estimates: ## mean difference ## 18 Finalmente la comparación de la condición de neutra y alegría será: ## ## Paired t-test ## ## data: datos$alegria and datos$neutra ## t = 4.5, df = 3, p-value = 0.02049 ## alternative hypothesis: true mean difference is not equal to 0 ## 95 percent confidence interval: ## 2.635107 15.364893 ## sample estimates: ## mean difference ## 9 Aplicando la corrección de Bonferroni (p = 0.017), encontramos que la condición de alegría es la que muestra diferencias significativas con respecto a las otras dos. En este tipo de diseños cabe cualquier otro tipo de combinación lineal. Así, por ejemplo, podríamos plantearnos comparar si las medidas de la condición de tristeza y la de neutra tomadas conjuntamente muestran diferencias significativas con respecto a la condición de alegría. En este caso, la comparación a realizar tendría los siguientes coeficientes 5: \\[ \\phi_i = \\frac{1}{2}Y_{iT} + \\frac{1}{2}Y_{iN} - (1)Y_{iA} \\] Una vez calculado el contraste se aplicaría la prueba t de Student para 1 muestra 6. Los resultados son los siguientes: ## ## Shapiro-Wilk normality test ## ## data: cont ## W = 0.94466, p-value = 0.683 ## ## One Sample t-test ## ## data: cont ## t = -5.5114, df = 3, p-value = 0.01176 ## alternative hypothesis: true mean is not equal to 0 ## 95 percent confidence interval: ## -21.29537 -5.70463 ## sample estimates: ## mean of x ## -13.5 Atendiendo a los resultados, podemos considerar que la condición de alegría presenta cambios en la respuesta psicofisiológica significativos con respecto a las otras dos condiciones tomadas conjuntamente. Obsérvese que la suma de los coeficientes (1/2 + 1/2 - 1) suman cero.↩︎ Si no se cumple el supuesto de normalidad habría que aplicar la prueba W de Wilcoxon.↩︎ "],["prueba-de-friedman.html", "3.7 Prueba de Friedman", " 3.7 Prueba de Friedman Cuando la variable no cumple el supuesto de normalidad o la VD está medida de manera ordinal lo recomendable es utilizar la prueba de Friedman para estudiar si existen diferencias significativas entre las distintas condiciones. Esta prueba es una extensión de la prueba W de Wilcoxon para dos medidas.Para poder utilizar esta prueba las respuestas deben ser variables continuas y estar medidas por lo menos en una escala ordinal. La hipótesis nula que se contrasta es que las respuestas asociadas a cada uno de los “tratamientos” tienen la misma distribución de probabilidad o distribuciones con la misma mediana, frente a la hipótesis alternativa de que por lo menos la distribución de una de las respuestas difiere de las demás. Rechazar ésta hipótesis implica que los centros de la distribución proceden de poblaciones distintas. Para contrastar esta hipótesis hay que transformar las puntuaciones de cada individuo en puntuaciones ordenadas (rangos). Si hay empates las puntuaciones se sustituyen por el rango medio. 3.7.1 Ejemplo de diseño unifactorial de medidas repetidas no paramétrico Si aplicamos esta técnica a los datos del ejemplo 1: Tabla 3.2: Datos del ejemplo 1 tristeza neutra alegria 72 73 80 64 77 84 70 83 90 62 71 86 Los rangos por filas de estas observaciones son: Tabla 3.3: Rangos del ejemplo 3.1 V1 V2 V3 1 2 3 1 2 3 1 2 3 1 2 3 El estadístico Q de Friedman se calcula mediante la siguiente expresión: \\[ Q = \\frac{12}{nj(j+1)} \\sum_{1}^{j}R_{+j}^2 - 3n(j+1) \\] donde \\(R_{+j}^2\\) son los suma de los rangos por columna. El valor aplicado a nuestros datos es: \\[ Q = \\frac{12}{4*3(3+1)}(4^2 + 8^2 + 12^2) - 3*4*4 = \\frac{224}{4}-48 = 8 \\] tenemos que P(\\(\\chi^2(2)\\) &gt; Q) = 0.018 por lo que rechazamos la hipótesis nula de que los rangos medios de cada condición proceden de la misma población. Utilizando el programa R obtenemos los mismo resultados: ## ## Friedman rank sum test ## ## data: as.matrix(datos) ## Friedman chi-squared = 8, df = 2, p-value = 0.01832 "],["modelo-a1fmr-con-el-programa-jamovi.html", "3.8 Modelo A1FMR con el programa JAMOVI", " 3.8 Modelo A1FMR con el programa JAMOVI Tenemos que seleccionar Analysis + Repeated Measures ANOVA tal y como aparece en la figura que se presenta a continuación. Figura 3.4: Selección del modelo A1FMR en JAMOVI Definimos las variables e introducimos cada variable en su lugar. Asimismo, damos nombre a la variable dependiente: Figura 3.5: Definición de las variables en JAMOVI Para comprobar la normalidad de las variables tenemos que utilizar el módulo de “Exploración”. Figura 3.6: Supuesto de normalidad en JAMOVI La esfericidad se comprueba con la opción “Comprobación de supuestos”: Figura 3.7: Supuesto de esfericidad en JAMOVI Si se encuentran diferencias significativas utilizamos Bonferroni como procedimiento de comparaciones múltiples tal y como aparece en la siguiente figura. Figura 3.8: Comparaciones múltiples en JAMOVI "],["prueba-de-friedman-con-el-programa-jamovi.html", "3.9 Prueba de Friedman con el programa JAMOVI", " 3.9 Prueba de Friedman con el programa JAMOVI Seleccionamos la prueba Friedman: Figura 3.9: Prueba de Friedman en JAMOVI Introducimos las variables y marcamos las comparaciones múltiples de Durbin-Conover. Figura 3.10: Comparaciones post hoc de Friedman en JAMOVI "],["modelo-a1fmr-con-el-programa-spss.html", "3.10 Modelo A1FMR con el programa SPSS", " 3.10 Modelo A1FMR con el programa SPSS Tenemos que seleccionar Analizar + Modelo lineal general + Medidas repetidas tal y como aparece en la figura siguiente: Figura 3.11: Selección del modelo A1FMR en SPSS Le damos nombre al factor, introducimos el número de niveles y pulsamos el botón de añadir Figura 3.12: Modelo A1FMR en SPSS: Identificación de los factores y niveles Incluimos los tres niveles y marcamos el botón de Opciones. Este botón da lugar a otra ventana en la que podremos obtener descriptivos de cada variable, y pruebas para el diagnóstico del modelo. Marcando el botón de Gráficos podemos obtener el gráfico de las medias Figura 3.13: Modelo A1FMR en SPSS: Definición de los niveles El estudio descriptivo nos muestra los promedios de cada condición, su desviación estándar y el número de casos válidos Figura 3.14: Modelo A1FMR en SPSS: Descriptivos Introducimos el factor en el cuadro de Mostrar medias y elegimos la corrección de Bonferroni. Figura 3.15: Modelo A1FMR en SPSS: Corrección de Bonferroni Observamos que el estadístico de Mauchly nos indica que se cumplen los supuestos de esfericidad. En la tabla, además del estadístico W de Mauchly (W = 0.667), aparecen la aproximación Chi-cuadrado, sus grados de libertad (gl = 2) y la significación (p = 0.667). También aparecen las estimaciones de \\(\\epsilon\\) de Geisser-Greenhouse (\\(\\hat{\\epsilon}\\) = 0.75, el de Huyndt-Feldt \\(\\tilde{\\epsilon}\\) = 1 y la estimación del limite inferior = 0.5). Figura 3.16: Modelo A1FMR en SPSS: Prueba de Mauchly Dado que se cumplen los supuestos de esfericidad, consideramos válida la prueba F Figura 3.17: Modelo A1FMR en SPSS: Prueba F Observamos que existen diferencias significativas entre las imágenes que representaban emociones de alegría con las que representaban emociones neutras Figura 3.18: Modelo A1FMR en SPSS: Post hoc 3.10.1 Conclusiones según normas APA: Se encontró que hubo diferencias significativas entre las distintas emociones (F(2,6) = 20.25, p = .002, \\(\\eta_{parcial}^{2}\\) corregida = 0.44 indicando un efecto grande). La respuesta psicofisiológica fue significativamente menor cuando se presentaron estímulos alegres que cuando se presentaron los estímulos neutros (diferencia = -18, p = .042) "],["prueba-de-friedman-con-el-programa-spss.html", "3.11 Prueba de Friedman con el programa SPSS", " 3.11 Prueba de Friedman con el programa SPSS Esta disponible en Analizar + Pruebas no parametricas + Cuadro de diálogos antiguos + K muestras relacionadas Figura 3.19: Prueba de Friedman en SPSS Se introducen las variables en el cuadro central. Por defecto aparece marcada la opción de la prueba de Friedman. En el botón de Exacta podemos decidir si la significación se calcula mediante aproximación asintótica, de manera exacta o mediante simulación de montecarlo. El defecto es mediante aproximación asintótica. Figura 3.20: Prueba de Friedman en SPSS: Variables Los resultados indican que hubo diferencias significativas. Si aparecen diferencias significativas hay que realizar pruebas a posteriori mediante la prueba de Wilcoxon para dos muestras si la variable diferencia no sigue una ley normal o la prueba T para una muestra si las diferencias siguen una ley normal. En la figura 14 los resultados de la prueba de Friedman se incluyen, los rangos medios de cada una de las medidas, el número de casos válidos (N = 4), el valor del estadístico Friedman (Chi-cuadrado = 8), sus grados de libertad (gl = 2), y su significación asintótica (0.018) Figura 3.21: Prueba de Friedman en SPSS 3.11.1 Conclusiones Se encontró que la respuesta psicofisiológica varió en función del estímulo presentado (\\(\\chi^{2}\\)(2,N =4)= 8, p = 0.018). Los contrastes a posteriori no mostraron esas diferencias significativas una vez aplicada la corrección de Bonferroni. "]]
