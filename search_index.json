[["index.html", "Análisis de datos en psicología II Prólogo", " Análisis de datos en psicología II Manuel Morales Ortiz 2023-10-06 Prólogo Este documento es el resultado de las notas de clase de la asignatura “Diseño y Análisis de datos II” existente en el plan de estudios de graduado en psicología en la Universidad de Sevilla. El enfoque de estos apuntes está dirigido a aquel usuario que no tiene un conocimiento profundo de matemáticas, pero que ha superado un primer curso de análisis de datos donde se hayan estudiado los contenidos relacionados con la estadística descriptiva y los fundamentos de la inferencia estadística. La enseñanza de las materias de análisis de datos suelen ir asociadas al uso de un determinado programa informático que facilite el cálculo de los resultados que de otra manera podrían resultar tediosos. La elección del programa informático debe tener en cuenta la audiencia a la que va dirigido. En el campo de la psicología está muy extendido el uso del programa IBM SPSS (IBM-Corp., 2023), siendo el programa estadístico de referencia en una gran parte de las universidades. Sin embargo, la necesidad de disponer de licencia de pago implica que los futuros usuarios tendrán dificultades para disponer de dicho programa una vez que abandonen la universidad. Para solucionar este problema se suele recurrir a programas de libre distribución como el programa R (R Core Team, 2016). El inconveniente que presenta este programa es su curva de aprendizaje. Dado que el periodo de enseñanza es muy limitado en los planes de estudio, iniciarse en este programa supone una reducción del tiempo disponible para la enseñanza de los contenidos propios de la asignatura. Para solventar este problema hemos recurrido al programa JAMOVI (The Jamovi Team, 2022). Este programa permite aprovechar la flexibilidad y posibilidades de R utilizando un sistema de ventanas más adecuado a las habilidades de los estudiantes de psicología. Este documento se compone de 6 capítulos. En el primero se realiza una introducción a la lógica de la investigación científica. También se realiza un recordatorio de los fundamentos de la inferencia estadística con aplicación de algunas técnicas estadísticas básicas. En los capítulos 2 a 5 se presenta la técnica de ANOVA para estudiar relaciones entre variables (relaciones entre 1 variable independiente y una dependiente en los temas 2 y 3 y relaciones con más de una VI en los temas 4 y 5). En el tema 6 se desarrollan contenidos relacionados con los modelos de regresión lineal. Este trabajo ha sido posible gracias a muchas personas. En primer lugar, a toda la comunidad R por los desarrollos del programa R y en particular a Xie (2023) por el desarrollo de librerías como knitr o bookdown que nos han resultado muy útiles para la redacción de este trabajo. Asimismo, este trabajo se ha visto beneficiado del trabajo de Luque Calvo (2017). Asimismo, a mis alumnos que han sido los mejores revisores del trabajo aquí expuesto. La elaboración de este trabajo se ha realizado bajo las condiciones de la licencia de Creative Commons https://creativecommons.org/share-your-work/public-domain/cc0: Agosto de 2023. "],["introducción-a-los-procedimientos-de-investigación-en-psicología.html", "1 Introducción a los procedimientos de investigación en Psicología", " 1 Introducción a los procedimientos de investigación en Psicología El objetivo de toda investigación psicológica es la obtención de conocimiento válido. Para ello, es necesario que ese conocimiento se haya obtenido mediante un procedimiento sistemático que llamamos método científico. Precisamente, el origen de la Psicología como ciencia se encuentra en la utilización de un método diferente al filosófico para el estudio de la vida mental (Anguera et al., 1995). Aunque hay ocasiones en las que se ha señalado la existencia de distintos métodos de investigación en Psicología, conviene aclarar que compartimos la opinión de que la lógica de la investigación científica configura un único método de investigación, existiendo varias estrategias o procedimientos para alcanzar estos objetivos. Son varios los criterios utilizados para clasificar las distintas estrategias de investigación.(Anguera et al., 1995). Uno de los más relevantes es la existencia de manipulación o no de la variable independiente (VI). Mediante este criterio distinguimos entre los estudios manipulativos y los no manipulativos. Dentro de los manipulativos incluimos los diseños experimentales y los cuasiexperimentales. Entre los no manipulativos distinguimos los selectivos (Ato &amp; Vallejo (2015) los denomina asociativos) de los observacionales. En este tema vamos a estudiar algunos diseños de investigación así como las técnicas estadísticas asociadas a los mismos. Realizaremos una clasificación de los diseños de investigación y posteriormente pasaremos a estudiar los conceptos básicos relacionados con la inferencia estadística. "],["lógica-de-la-investigación-científica.html", "1.1 Lógica de la investigación científica", " 1.1 Lógica de la investigación científica La investigación científica intenta resolver problemas mediante un procedimiento que parte de la formulación de una serie de hipótesis. Con objeto de contrastar dichas hipótesis, el investigador diseña un estudio en el que recogerá información que le permitirá tomar decisiones acerca de la validez de las mismas. La generación de problemas en la investigación científica no obedece a ninguna estrategia sistemática, sino que en muchos casos es el resultado de la casualidad o el azar. Sin embargo, generar problemas relevantes mejora cuando se dispone de un profundo conocimiento de la temática sobre la que se investiga. Aunque la inspiración no tiene reglas, resulta conveniente estar actualizado en el campo de investigación de interés con el que se podrá evaluar la idoneidad y relevancia del problema planteado. El diseño de investigación si tiene una sistemática bien consolidada. Consiste en un plan estructurado de acción que pretende realizar las acciones comparativas (medir la covariación o concomitancia entre dos fenómenos), en condiciones de validez. Aunque existen muchos criterios de clasificación (estrategia, número de variables, grado de intervención, etc.), aquí vamos a centrarnos en el que consideramos más relevante y que se relaciona con el tipo de estrategia seguida por el investigador para realizar las acciones comparativas en base a los objetivos de investigación. Se distinguen dos estrategias: Transversal: Utiliza grupos diferentes para estudiar cada uno de los valores de la variable independiente. Supongamos que queremos conocer qué procedimiento es el mejor para conseguir reducir peso. Tenemos cuatro condiciones (sin tratamiento, dieta, fármacos y dieta + ejercicio). Seleccionamos una muestra de individuos y asignamos aleatoriamente cada uno de ellos a uno de los grupos. A cada individuo se le mide la VD (peso) antes de comenzar el tratamiento y después de recibir el tratamiento. Comparamos la diferencia de peso en cada una de las cuatro condiciones. Longitudinal: Utiliza un único grupo que recibe todos los valores de la variable independiente en momentos distintos. Supongamos que queremos estudiar cómo influye el paso del tiempo en el recuerdo. Para ello, se le pide a los individuos que lean una historia durante 15 minutos. Pasada una hora se les indica que escriban todo lo que recuerdan de la historia. Asimismo, se le vuelve a pedir que escriban todo lo que recuerdan al cabo de un día, una semana y un mes. Por lo general, estas dos estrategias suelen aparecer conjuntamente en muchos diseños de investigación. "],["validez-de-las-investigaciones.html", "1.2 Validez de las investigaciones", " 1.2 Validez de las investigaciones La validez de un estudio depende la naturaleza de las variables y sus definiciones, del diseño de investigación y de la técnica estadística utilizada. 1.2.1 Validez de las variables y definiciones En toda investigación científica se trabaja con conceptos que deben ser operacionalizados para que puedan ser medidos y presenten un significado unívoco. Los problemas de validez surgen cuando las operaciones realizadas no tienen nada que ver con el constructo estudiado o su vinculación es parcial. Las variables pueden medirse mediante cuatro escalas de medida (nominal, ordinal, intervalo y razón). La elección del tipo de medida determinará el tipo de análisis de los datos. Hay ocasiones en las que se han utilizado los términos de cualitativo y cuantitativo para clasificar los distintos tipos de variables (Ato &amp; Vallejo, 2007). Dentro de las variables cualitativas suelen diferenciarse las variables en función de que su escala de medida sea nominal u ordinal. Las variables cuantitativas se distinguen en función de que su dominio sea discreto o continuo. Otra clasificación frecuente en Psicología es la distinción entre variable independiente (VI) o variable predictora (VP), variable dependiente (VD) y variables extrañas (VVEE). En ocasiones se se denomina a la VD variable criterio en los contextos en los que no hay manipulación en el diseño de investigación. En estos estudios a la VD también se le denomina variable de respuesta (VR) y a la VI variable predictora. 1.2.2 Validez del diseño de investigación Con el diseño tenemos que asegurarnos de que las variables extrañas están debidamente controladas (validez interna). Para ello, resulta conveniente utilizar técnicas de control tales como la manipulación de la VI, la aleatorización, el mantenimiento constante de dichas variables o el control estadístico. Asimismo, es importante considerar la generalización de los resultados del estudio a otras situaciones. En este caso, estamos hablando de validez externa. 1.2.3 Validez de conclusión estadística La elección de la técnica estadística también influye en la validez de la investigación. Una técnica mal seleccionada puede llevarnos a sacar conclusiones inadecuadas. Afecta a la determinación de la existencia de covariación entre las variables y a su grado. El procedimiento para obtener conclusiones estadísticas es el contraste de hipótesis. Este procedimiento consiste en la toma de decisiones sobre dos hipótesis rivales y excluyentes en base a la probabilidad calculada de un estadístico. Más adelante se desarrollará este concepto para su completa comprensión. Entre los errores que se cometen mediante la contrastación de hipótesis es el considerar que existe relación cuando no existe (error tipo I o riesgo \\(\\alpha\\)), o considerar que no existe relación cuando la hay (error tipo II o riesgo \\(\\beta\\)). Otro error posible es la sobre-estimación o infra-estimación de la relación. Posibles factores que pueden afectar a la conclusión estadística es la baja potencia y la violación de los supuestos. También es importante considerar la magnitud del efecto y la significación práctica y clínica de los resultados. 1.2.4 Contraste de hipótesis estadísticas Dentro de la inferencia estadística buscamos realizar comparaciones y establecer relaciones mediante la estimación de parámetros y el contraste de hipótesis. El primer procedimiento puede hacerse mediante la estimación puntual o por intervalos. En cambio, el contraste de hipótesis es la toma de decisiones acerca de si un conjunto de datos corrobora una hipótesis. Supone cubrir una serie de etapas: 1) Formulación de las hipótesis: Se plantean la hipótesis nula (\\(H_0\\)) que considera que las variaciones existentes entre las condiciones se debe al azar y la hipótesis alternativa (\\(H_1\\)) que niega la hipótesis nula y considera que las variaciones en una variable están relacionadas con los cambios en la otra. Las hipótesis pueden ser bidireccionales (no conocemos el sentido de la relación como por ejemplo \\(\\mu_1 = \\mu_2\\)) o unidireccionales (el investigador plantea donde se encontrarán las diferencias como por ejemplo \\(\\mu_1 \\leq \\mu_2\\) ). 2) Definición del estadístico de contraste: Su cálculo supone la realización de un estudio empírico en el que se ha extraído una muestra aleatoria de la población. Este estadístico debe tener una distribución de probabilidad conocida. 3) Regla de decisión: Se basa en la compatibilidad que existe entre la hipótesis nula y los datos empíricos (Pardo &amp; San Martin, 2010). Permite determinar la probabilidad de que \\(H_0\\) sea cierta en base a nuestros resultados. Para determinar el grado de apoyo de \\(H_0\\) se divide a la distribución teórica en dos regiones (región de confianza y región crítica). La región de confianza es la zona de la distribución teórica en la que se acepta \\(H_0\\). Existe consenso en considerar que esta región alcance el 95% de probabilidad. La región crítica es la zona complementaria a la región de confianza (por lo que cubre el 5% de la distribución) y si el estadístico pertenece a esa zona se rechaza \\(H_0\\). Si la hipótesis es bilateral la región crítica estará a ambos lados de la distribución teórica. En cambio, si la hipótesis es unilateral la región de confianza estará situada en una de las colas de las distribución. Ejemplo 1.1: A continuación, se presentan los resultados de un estudio en el que se quiso comparar el aprendizaje de los niños dependiendo del tipo de método (fonético versus global). Cada niño sólo fue entrenado con un único método y se quería conocer si había diferencias significativas entre ambos métodos. Nos estamos planteando una hipótesis bidireccional: Tabla 1.1: Datos del ejemplo 1.1 Fonetico 6 6 3 4 2 5 7 Global 4 5 6 7 8 4 8 \\(H_0\\): No hay diferencias entre los métodos de aprendizaje (\\(\\mu_{F} = \\mu_G\\)) \\(H_1\\): El aprendizaje depende del método (\\(\\mu_{F} \\neq \\mu_G\\)) Para contrastar esta hipótesis necesitamos un estadístico con distribución de probabilidad conocida. En este caso, es el estadístico t de Student: \\[ \\bar{Y}_{\\bar{Y}_{1} - \\bar{Y}_{2}}= \\bar{Y}_{1} - \\bar{Y}_{2} \\] \\[ Var(\\bar{Y}_{1} - \\bar{Y}_{2}) = \\sigma_{1}^{2}/n_{1} + \\sigma_{2}^{2}/n_{2} \\] \\[ t = \\frac{\\bar{Y}_{\\bar{Y}_{1} - \\bar{Y}_{2}}}{Var(\\bar{Y}_{1} - \\bar{Y}_{2})} \\sim t(n-2) \\] En nuestro ejemplo, el valor del estadístico t puede obtenerse con el programa R: ## ## Two Sample t-test ## ## data: rendimiento by metodo ## t = -2.5955, df = 12, p-value = 0.02342 ## alternative hypothesis: true difference in means between group 1 and group 2 is not equal to 0 ## 95 percent confidence interval: ## -4.2044435 -0.3669851 ## sample estimates: ## mean in group 1 mean in group 2 ## 4.142857 6.428571 Encontramos que el valor del estadístico t vale -2.596 y su valor de probabilidad es 0.023. En base a estos resultados podemos rechazar \\(H_0\\). Esto significa que el estadístico t se aleja bastante de la predicción establecida mediante la hipótesis nula. Es decir, existe muy poca compatibilidad entre nuestros datos y \\(H_0\\). Para aceptar estos resultados debemos confirmar que se cumplen los supuestos de la prueba (normalidad de las muestras y homogeneidad de las varianzas 1): ## ## Shapiro-Wilk normality test ## ## data: rendimiento[1:7] ## W = 0.92025, p-value = 0.4713 ## ## Shapiro-Wilk normality test ## ## data: rendimiento[8:14] ## W = 0.91511, p-value = 0.4324 Tabla 1.2: Prueba de homogeneidad de varianzas del ejemplo 1.1 Df F value Pr(&gt;F) group 1 0.06 0.8106346 12 NA NA Ejemplo 1.2: A continuación, se presentan los resultados de un estudio en el que se quiso comparar el aprendizaje de los niños dependiendo del tipo de método (fonético versus global). Cada niño sólo fue entrenado con un único método y se esperaba encontrar un mayor rendimiento con el método global. Aquí nos planteamos una hipótesis unidireccional: Tabla 1.1: Datos del ejemplo 1.1 Fonetico 6 6 3 4 2 5 7 Global 4 5 6 7 8 4 8 \\(H_0\\): No hay diferencias en el aprendizaje (\\(\\mu_{F} \\geq \\mu_G\\)) \\(H_1\\): El aprendizaje es mejor con el método global (\\(\\mu_{F} &lt; \\mu_G\\)) Para contrastar esta hipótesis necesitamos un estadístico con distribución de probabilidad conocida. En este caso, el estadístico es el mismo y tiene el mismo valor. Sólo cambia su probabilidad: ## ## Two Sample t-test ## ## data: rendimiento by metodo ## t = -2.5955, df = 12, p-value = 0.01171 ## alternative hypothesis: true difference in means between group 1 and group 2 is less than 0 ## 95 percent confidence interval: ## -Inf -0.7161774 ## sample estimates: ## mean in group 1 mean in group 2 ## 4.142857 6.428571 Ejemplo 1.3: Con los mismos datos del ejemplo anterior supongamos que el investigador está interesado en conocer si la media del rendimiento en la población de los estudiantes es 6. Para ello formula las siguientes hipótesis: \\(H_0\\): \\(\\mu_{rendimiento}\\) = 6 \\(H_1\\): \\(\\mu_{rendimiento} \\neq\\) 6 Como no conocemos el sentido de la dirección asumimos que la hipótesis es bilateral. Para contrastar esta hipótesis necesitamos conocer si la distribución de la variable sigue una ley normal. En caso de que se cumpla esta hipótesis realizaremos el contraste con la prueba t para una muestra. Si no se cumple, usaremos la prueba de Wilcoxon para una muestra. El estadístico de Shapiro-Wilks indica que se cumple la normalidad de la variable por lo que usamos la prueba t. shapiro.test(rendimiento) ## ## Shapiro-Wilk normality test ## ## data: rendimiento ## W = 0.92677, p-value = 0.2747 t.test(rendimiento, mu = 6) ## ## One Sample t-test ## ## data: rendimiento ## t = -1.3512, df = 13, p-value = 0.1997 ## alternative hypothesis: true mean is not equal to 6 ## 95 percent confidence interval: ## 4.143709 6.427720 ## sample estimates: ## mean of x ## 5.285714 Aceptamos la hipótesis de que el valor medio de la variable rendimiento es 6 en la población. Ejemplo 1.4: Supongamos que construimos un examen para evaluar los conocimientos de los estudiantes de Psicología en la materia de Diseño y Análisis de Datos II. El examen tendrá 10 preguntas con 3 opciones de respuesta y solo una correcta. Queremos conocer cuántas preguntas debe responder correctamente el alumno para estar seguros de que domina la materia y de que no ha respondido por azar. Las hipótesis en este contraste estadístico serán: \\(H_0\\): El alumno no sabe por lo que responde al azar (\\(\\pi_{acierto} \\leq\\) 0.333) \\(H_1\\): El alumno sabe. No responde al azar (\\(\\pi_{acierto} &gt;\\) 0.333) Ahora necesitamos determinar cuántas preguntas suponen responder por azar. Está claro si el alumno acierta sólo 1 pregunta la probabilidad de que haya respondido por azar 0 ó 1 es muy alta. En la siguiente figura se aprecia que los valores más probables están comprendidos entre 2 y 4: Figura 1.1: Distribución binomial # Probabilidad igual a 2: dbinom(2,10,1/3) ## [1] 0.1950922 # Probabilidad igual a 3: dbinom(3,10,1/3) ## [1] 0.2601229 # Probabilidad igual a 4: dbinom(4,10,1/3) ## [1] 0.2276076 Por tanto, necesitamos encontrar el número de preguntas acertadas que tengan una probabilidad de ser acertadas por azar \\(\\leq\\) 0,05. Esto se consigue con 6 preguntas acertadas. Con 5 preguntas acertadas estaríamos por encima del nivel de riesgo establecido por convención del 5%: # Probabilidad menor o igual a 6: pbinom(6,10,1/3) [1] 0.9803384 # Probabilidad de la región crítica con 6 preguntas: 1 - pbinom(6,10,1/3) [1] 0.01966164 # Probabilidad de la región crítica con 5 preguntas: 1 - pbinom(5,10,1/3) [1] 0.07656353 No obstante, el criterio de 0,05 es un criterio establecido arbitrariamente y que supone la ausencia de factores contaminantes que estarían actuando en la situación real de examen y que podrían afectar al resultado del examen. Un profesor algo más exigente podría considerar la necesidad de aprobar acertando 7 preguntas. En este caso, la probabilidad de que un estudiante acertara por azar sería: # Probabilidad menor o igual a 7: pbinom(7,10,1/3) [1] 0.996596 # Probabilidad de la región crítica con 7 preguntas: 1 - pbinom(7,10,1/3) [1] 0.003403953 En este caso no es necesario calcular la normalidad de los errores, ya que si las muestras son normales también serán normales los errores. Esto se verá con más claridad en el próximo tema.↩︎ "],["potencia-de-un-contraste.html", "1.3 Potencia de un contraste", " 1.3 Potencia de un contraste Dado que él área de una distribución teórica vale 1 sabemos que 1 - \\(\\alpha\\) es la probabilidad asociada a la región de confianza. O lo que es lo mismo, la probabilidad de aceptar \\(H_0\\) cuando es cierta. Del mismo modo, 1 - \\(\\beta\\) es la probabilidad de rechazar \\(H_0\\) cuando es falsa. A esta probabilidad se le llama potencia de la prueba. Figura 1.2: Errores en la inferencia estadística Pardo &amp; San Martin (2010) consideran que la potencia de un contraste hace referencia “a la sensibilidad del contraste para detectar como falsa una hipótesis nula que realmente lo es. Cuanto mayor es la potencia, mayor es la de que una hipótesis nula falsa sea reconocida como tal. Si la potencia es baja, también será baja la probabilidad de detectar un efecto.” La potencia de la prueba depende del nivel de riesgo \\(\\alpha\\), del error típico de la distribución teórica y del valor real de \\(H_1\\). Puede aumentarse modificando cualquiera de los tres factores mencionados, pero una forma sencilla de aumentarla es incrementando el tamaño de la muestra. Asimismo, mejorar el diseño del estudio o utilizar medidas con mayor fiabilidad y validez también son estrategias posibles para mejorar la potencia estadística del contraste. "],["tamaño-del-efecto.html", "1.4 Tamaño del efecto", " 1.4 Tamaño del efecto Hemos visto anteriormente que para determinar la potencia de la prueba un elemento necesario es fijar el valor real de \\(H_1\\). Como este valor no se conoce a priori debemos determinarlo. Pues bien, la distancia del valor del estadístico entre \\(H_0\\) y el valor real de \\(H_1\\) es lo que se denomina tamaño del efecto. Cuando se realizan comparaciones entre variables el tamaño del efecto hace referencia a la magnitud de la diferencia, mientras que cuando se estudian relaciones el tamaño del efecto se refiere a la intensidad de la relación. Dado que la potencia de un contraste depende del tamaño muestral podemos encontrar diferencias de medias muy pequeñas que sean significativas. Bastará con tener un tamaño de muestra suficientemente grande. Por tanto, la significación estadística permite contrastar hipótesis, pero no es un buen indicador de la relevancia de la relación. Existen una gran variedad de medidas del tamaño del efecto. Pardo &amp; San Martin (2010) consideran que todas ellas pueden clasificarse en dos grandes tipos: 1) basadas en la estandarización de la diferencia entre las medias y 2) las basadas en la proporción de varianza explicada. En los siguientes temas se estudiarán algunos de ellos. "],["contrastes-de-hipótesis-en-los-programas-estadísticos.html", "1.5 Contrastes de hipótesis en los programas estadísticos", " 1.5 Contrastes de hipótesis en los programas estadísticos 1.5.1 Contrastes para 1 variable El árbol de decisión para estos contrastes es el siguiente: Figura 1.3: Contrastes para 1 variable 1.5.1.1 Prueba t para 1 muestra mediante el programa JAMOVI Se selecciona el módulo de “Pruebas T” y marcamos la opción ”Prueba T para una muestra”. En la ventana de la prueba T para una muestra aparece por defecto seleccionada la prueba T y marcando la opción de “Comprobación de supuestos” puede obtenerse el resultado de la prueba de normalidad. Para realizar el análisis es necesario indicar el valor de prueba en la sección de “Hipótesis” (en nuestro ejemplo se ha introducido el valor de 47). Por defecto, aparece marcada la opción del contraste bilateral aunque existen opciones para estudiar los contrastes unilaterales. Asimismo, tendríamos que marcar la opción de “Rangos de Wilcoxon” en el caso de que no se acepte la hipótesis de normalidad. Figura 1.4: Prueba t para una muestra en JAMOVI Figura 1.5: Prueba t para una muestra en JAMOVI 1.5.1.2 Prueba t para 1 muestra en el programa SPSS 1.5.1.2.1 Prueba de normalidad de Shapiro-Wilk Para determinar la normalidad será necesario aplicar la prueba de Shapiro-Wilks. En el programa SPSS se hace marcando Analizar + Explorar + Gráficos + Gráficos con pruebas de normalidad. Los resultados obtenidos pueden observarse en las siguientes imágenes. Figura 1.6: Prueba de normalidad Shapiro-Wilks en el programa SPSS Figura 1.7: Prueba de normalidad en SPSS Figura 1.8: Prueba de normalidad en SPSS Figura 1.9: Resultados de la prueba de normalidad La prueba t para 1 muestra se realiza marcando Analizar + Comparar medias + Prueba T para una muestra: Figura 1.10: Prueba t para una muestra en el programa SPSS Figura 1.11: Prueba t para una muestra en el programa SPSS Será necesario introducir la VD a estudiar en el cuadro de “Variable a contrastar”. El valor del contraste es cero por defecto, pero habrá que introducir el valor de la hipótesis a estudiar. 1.5.1.2.2 Prueba de Wilcoxon para una muestra Necesitamos marcar Analizar + Pruebas no paramétricas + Cuadro de diálogos antiguos + 2 muestras relacionadas. Observamos que el programa SPSS nos pide introducir dos variables, ya que esta prueba está diseñada para estudiar la relación entre dos variables. Sin embargo, puede utilizarse introduciendo la variable en cuestión y como segunda variable una nueva que hemos creado con el valor de contraste de nuestras hipótesis (en la figura aparece con el nombre de Mdn): Figura 1.12: Prueba de Wilcoxon en el SPSS Figura 1.13: Prueba de Wilcoxon en el SPSS 1.5.2 Contrastes para 1 VI cualitativa y una VD cuantitativa El árbol de decisión para los diseños en los que solo hay 1 medida por sujeto (muestras independientes) es: Figura 1.14: Técnicas para dos muestras independientes 1.5.2.1 Prueba t para muestras independientes en JAMOVI Se selecciona la opción de muestras independientes en el modulo de “Pruebas T”. Se introduce la variable dependiente (cuantitativa) en el cuadro de “Variables dependientes” y la variable cualitativa en el cuadro de “Variable de agrupación”. Se seleccionan las opciones para comprobar los supuestos de la prueba y se elige la prueba (T de student, T de Welch o U de Mann-Whitney), dependiendo de los supuestos que se cumplan. Figura 1.15: Prueba t para muestras independientes en JAMOVI 1.5.2.2 Prueba t para muestras independientes en SPSS Se realiza marcando Analizar + Comparar medias + Prueba T para muestras independientes: Figura 1.16: Prueba t para muestras independientes en SPSS Figura 1.17: Prueba t para muestras independientes en SPSS 1.5.2.3 Prueba U de Mann-Whitney para muestras independientes en SPSS Se obtiene marcando Analizar + Pruebas no paramétricas + Cuadro de diálogos antiguos + 2 muestras independientes. Figura 1.18: Prueba U para muestras independientes en SPSS Figura 1.19: Prueba U para muestras independientes en SPSS 1.5.3 Pruebas para muestras relacionadas (2 medidas por unidad de observación) El árbol de decisión para este apartado es el siguiente: Figura 1.20: Pruebas para dos muestras relacionadas La variable sobre la que se realiza el contraste es la diferencia entre las medidas de los dos momentos. Por tanto, el análisis de estos diseños se convierte en el análisis sobre 1 muestra que ya se ha estudiado anteriormente. 1.5.3.1 Prueba t para muestras relacionadas en el programa JAMOVI Se selecciona la opción de “Muestras Apareadas” en el modulo de “Pruebas T”. Se introduce las dos variables dependientes (cuantitativas) en el cuadro de *“Variables Apareadas”. Se selecciona la opción “Comprobación de supuestos” de la prueba y se elige la prueba (T de student o W de Wilcoxon), dependiendo de que se cumpla el supuesto de normalidad. Estos resultados son los mismos que si se hubieran calculado la diferencia entre las dos medidas y se hubiera aplicado la prueba T para una muestra. Figura 1.21: Prueba t para muestras relacionadas en JAMOVI 1.5.3.2 Prueba t para muestras relacionadas en el programa SPSS Se realiza marcando Analizar + Comparar medias + Prueba T para muestras relacionadas: Figura 1.22: Prueba t para muestras relacionadas en SPSS Figura 1.23: Prueba t para muestras relacionadas en SPSS 1.5.3.2.1 Prueba de Wilcoxon para 2 muestras relacionadas Necesitamos marcar Analizar + Pruebas no paramétricas + Cuadro de diálogos antiguos + 2 muestras relacionadas. Figura 1.24: Prueba de Wilcoxon para muestras relacionadas en SPSS Figura 1.25: Prueba de Wilcoxon para muestras relacionadas en SPSS "],["relación-entre-una-variable-cualitativa-y-otra-cuantitativa-i-diseños-transversales.html", "2 Relación entre una variable cualitativa y otra cuantitativa (I): Diseños transversales", " 2 Relación entre una variable cualitativa y otra cuantitativa (I): Diseños transversales El objetivo de este tema es introducir las técnicas de análisis de los diseños que estudian las relaciones entre una variable independiente (VI) cualitativa y una variable dependiente (VD) cuantitativa mediante la comparación de grupos. En el caso más sencillo tendremos un factor (variable cualitativa) con dos valores (niveles o condiciones). Esto se traduce en la creación de dos grupos. A uno de ellos se le denomina grupo control y al otro grupo experimental. Ato &amp; Vallejo (2015) distinguen dos tipos de factores: 1) de tratamiento: son valores que asigna el investigador a los grupos y 2) de clasificación: cuando la administración no está controlada por el investigador (p.ej., el género). En este tipo de estudios se pretende buscar diferencias entre las distintas condiciones (valores de la VI). Establecer estas diferencias puede hacerse de muchas formas. Por ejemplo, en el caso del estudio de la dieta, podemos preguntarnos por el número de individuos que bajaron de peso o por el promedio de pérdida de peso en el grupo. Dependiendo de la medida que utilicemos, las técnicas estadísticas serán distintas. Asimismo, no todas las diferencias van a ser importantes. Por ejemplo, si encontramos que el grupo de dieta mostró una diferencia media de 10 gramos con el grupo que no recibió tratamiento podemos considerar que no es una diferencia relevante. Determinar si una diferencia será significativa o no será misión de la técnica estadística utilizada. Por último, las diferencias que encontremos pueden ser de tipo exploratorio (p. ejemplo, cuando se establecen relaciones de covariación sin pretensión de causalidad al no poder garantizarse la eliminación de explicaciones alternativas mediante el control de VVEE), o de tipo explicativo donde la covariación entre la VI y la VD se ha establecido en un contexto en el que se han utilizado procedimientos de control de VVEE para eliminar hipótesis rivales. En este tipo de diseños pueden utilizarse diversas técnicas de control que van desde el mantenimiento constante de la VE, la eliminación, la equiparación de valores mediante el método de bloqueo y/o el control estadístico. Ato &amp; Vallejo (2015) consideran que la tradición skinneriana basada en estudios de laboratorio con un gran control de las condiciones ambientales usa fundamentalmente la eliminación y el mantenimiento constante como formas de control. Por el contrario, en otras áreas de la Psicología, es más frecuente el uso del control estadístico y, en particular, de la aleatorización dentro del contexto denominado experimento aleatorio. Aplicado a este tipo de diseños el experimento aleatorio en su forma más pura consiste en seleccionar aleatoriamente los valores de la VI y asignar aleatoriamente los participantes a las distintas condiciones del estudio. Con ello, se espera que el azar neutralice cualquier VE presente en el estudio. En otros casos, la aleatorización no será completa y se restringirá en algún modo. "],["criterios-de-selección-de-la-técnica-estadística.html", "2.1 Criterios de selección de la técnica estadística", " 2.1 Criterios de selección de la técnica estadística En la siguiente figura aparece el árbol de decisión para elegir la técnica estadística adecuada cuando el diseño es transversal: Figura 2.1: Clasificación de las técnicas de análisis Como puede observarse en el diagrama son dos criterios los que nos permiten determinar la técnica estadística adecuada. En primer lugar, será necesario determinar si las muestras de los distintos grupos proceden de una distribución normal. Para ello, puede utilizarse la prueba de Shapiro-Wilks. Si no existe evidencia de que las muestras estudiadas procedan de una población normal o que la distribución de los errores es normal se utilizará la técnica de Kruskal-Wallis. En el caso de que haya información suficiente para asumir el supuesto de normalidad será necesario determinar si las varianzas de los grupos son homogéneas o no. Para ello, puede utilizarse la prueba de Levene. En el caso de que no se cumpla este supuesto se utilizará la prueba de Brown-Forsythe 2. Si los datos de nuestro estudio cumplen todos los supuestos utilizaremos la técnica de Anova de 1 factor completamente aleatorizado. Otra posibilidad es utilizar la técnica de Welch.↩︎ "],["anova-de-1-factor-completamente-aleatorizado.html", "2.2 ANOVA de 1 factor completamente aleatorizado", " 2.2 ANOVA de 1 factor completamente aleatorizado Se asume el supuesto de que los distintos grupos representan muestras aleatorias que proceden de poblaciones normales con igual media y varianza y que los errores siguen una distribución normal. La formulación del modelo estadístico es: \\[ Y_{ij} = \\mu_{i} + \\epsilon_i \\] donde \\(Y_{ij}\\) es la variable dependiente para el sujeto i del grupo j y \\(\\mu_{i}\\) es el valor esperado (la media). El último elemento del modelo \\(\\epsilon_{ij}\\) es el llamado error o término residual. Como en todo contraste estadístico, se plantean dos hipótesis, \\(H_0\\) o hipótesis nula y \\(H_1\\), hipótesis alternativa. En este modelo se definen de la siguiente forma: \\(H_{0}\\): \\(\\mu_{1} = \\mu_{2} = \\cdots \\mu_{j}\\) (todas las medias son iguales) \\(H_{1}\\): \\(\\mu_{i} \\neq \\mu_{j}\\) (i \\(\\neq\\) j) (no todas las medias son iguales) El contraste compara las diferencias entre medias muestrales con la variabilidad experimental para decidir si ésta ha podido generar esas diferencias o no. Tanto la diferencia entre las medias (MCA) como la variabilidad experimental (varianza intrasujeto ó MCE) son dos formas de estimar la varianza poblacional. Bajo el supuesto de hipótesis nula, ambas varianzas deben ser iguales. La comparación de ambas varianzas sigue una distribución de probabilidad (distribución F) que permite realizar el contraste estadístico: \\[ F = \\frac{MC_{A}}{MC_{E}} \\sim F(gl_{MC_{A}},gl_{MC_{E}}) \\] Se rechaza \\(H_{0}\\) si el estadístico F cae dentro de la región crítica; en caso contrario, se mantiene. Si se rechaza esta hipótesis se concluye que no todas las medias son iguales. Ejemplo 2.1 : A continuación, se presentan los datos de un estudio en el que se quiso comparar los resultados en una prueba de atención de tres grupos de 5 niños (A = sanos, B = con tumor astrocitoma y C = con tumor meduloblastoma) Tabla 2.1: Datos del ejemplo 2.1 A B C s1 30 16 10 s2 35 5 7 s3 15 22 15 s4 21 23 6 s5 24 22 12 Figura 2.2: Boxplot de los resultados del ejemplo 2.1 Tabla 2.2: Resultados del ejemplo 2.1 Df Sum Sq Mean Sq F value Pr(&gt;F) grupo 2 562.5333 281.26667 6.426504 0.0126711 Residuals 12 525.2000 43.76667 NA NA Los resultados de aplicar el modelo estadístico permiten rechazar la hipótesis nula de que no hay diferencias entre los grupos. Pueden obtenerse los valores de los parámetros que aparecen en el modelo: \\[ Y_{ij} = \\mu_{i} + \\epsilon_i \\] \\(Y_{ij}\\) es cada una de las puntuaciones. Así, por ejemplo, \\(Y_{1A}\\) será el valor del sujeto 1 en el grupo de los sanos \\(Y_{1A}\\) = 30. Asimismo, \\(Y_{1B}\\) será la puntuación del primer sujeto del grupo astrocitoma (\\(Y_{1B}\\) = 16). \\(\\mu_i\\) será la media del grupo. Por tanto, \\(\\mu_1\\) será para el grupo de los individuos sanos 25. Para el grupo astrocitoma será 17.6. Por último, para el grupo meduloblastoma tendremos 10; \\(\\epsilon_{ij}\\) es un valor específico para cada individuo. Es la diferencia entre la puntuación predicha por el modelo y la puntuación obtenida por el individuo. Así, para el sujeto 1 del grupo sano la puntuación predicha será \\(Y_{1Apred}\\) = 25 y su error \\(\\epsilon_{ij}\\) = 30 - 25 = 5. Otra forma de plantear este modelo es descomponiendo la puntuación de los sujetos en dos elementos: 1) la variabilidad explicada por el modelo y 2) la variabilidad no explicada por el modelo (error). De este modo, la predicción del modelo (media del grupo) puede descomponerse en dos elementos (media total + diferencia de la media del grupo con respecto a la media total): \\[ Y_{ij} = \\mu_{..} + (\\mu_j - \\mu_{..}) + \\epsilon_{ij} \\] donde \\(\\mu_{..}\\) es la media de todas las puntuaciones; \\((\\mu_j - \\mu_{..})\\) es la diferencia de la media del grupo y la media total y \\(\\epsilon_{ij}\\) es el error. En el caso de la primera puntuación del grupo sano sería: 30 = 17.5333333 + 7.4666667+ 5 "],["comprobación-de-los-supuestos.html", "2.3 Comprobación de los supuestos", " 2.3 Comprobación de los supuestos Es necesario determinar si las muestras son independientes. Esto se asume siempre y cuando hayan sido extraídas al azar 3. Asimismo, se necesita determinar si las muestras proceden de una población normal con la misma varianza. También tendremos que comprobar que la distribución de los errores sigue una ley normal. Prueba de normalidad para cada uno de los grupos: Grupo 1: ## ## Shapiro-Wilk normality test ## ## data: aciertos[1:5] ## W = 0.98829, p-value = 0.9734 Grupo 2: ## ## Shapiro-Wilk normality test ## ## data: aciertos[6:10] ## W = 0.78479, p-value = 0.06056 Grupo 3: ## ## Shapiro-Wilk normality test ## ## data: aciertos[11:15] ## W = 0.95695, p-value = 0.7866 Prueba de homogeneidad de varianzas: Tabla 2.3: Prueba de Levene del ejemplo 2.1 Df F value Pr(&gt;F) group 2 0.4599212 0.6420136 12 NA NA Ato &amp; Vallejo (2015) proponen utilizar el test de Durbin-Watson para comprobar la independencia de los residuales del modelo.↩︎ "],["tamaño-del-efecto-1.html", "2.4 Tamaño del efecto", " 2.4 Tamaño del efecto El tamaño del efecto pretende determinar la importancia del efecto encontrado, ya que es bastante posible encontrar diferencias significativas, que son insustanciales, cuando los tamaños muestrales son muy grandes. Es decir, encontrar diferencias significativas no siempre implica que existan diferencias entre las medias a nivel poblacional. Aunque existen distintas medidas del tamaño del efecto, aquí vamos a considerar la medida de eta parcial corregida al cuadrado y omega cuadrado que es la medida preferida por distintos investigadores (Pardo &amp; San Martin, 2010). El estadístico eta cuadrado parcial corregida se define como: \\[ \\hat{\\eta}^2_{pcorregida} = 1 - [(1 - \\hat{\\eta}^{2}(N - 1)/(N - J))] \\] donde, \\[ \\hat{\\eta}^{2} = \\frac{\\sum{n_j(\\bar{Y}_j - \\bar{Y})^2}}{(Y_{ij} - \\bar{Y})^2} = \\frac{(J - 1)MCA}{(J - 1) MCA + (N - J)MCE} \\] donde N es el número total de sujetos estudiados; J, el número de grupos; MCA, la media cuadrática debida a los tratamientos (predicha por el modelo), y MCE es la media cuadrática debida al error. El valor de \\(\\eta^{2}\\) representa el grado de asociación (no sólo lineal) entre la VI y la VD. En el caso del diseño que nos ocupa \\(\\eta^{2}\\) = \\(R_{XY}\\) (coeficiente de correlación). Por tanto, puede interpretarse como la proporción de varianza compartida por las dos variables. Es decir, indica como se reduce la incertidumbre acerca de la VD en base al conocimiento del valor que presenta el individuo en la VI. El estadístico omega cuadrado se define como: \\[ \\hat{\\omega}^{2} = \\frac{(J - 1)(MCA-MCE)}{(J - 1)MCA + (N - J+ 1)MCE} \\] Los valores 0.01, 0.06 y 0.14 representan tamaños de efecto bajo, medio y grande respectivamente para ambas medidas. Si el valor de \\(\\omega^{2}\\) es menor de 0 consideramos, por convención, que su valor es cero. Aplicado a los datos del ejemplo 2.1 tenemos: \\[ \\hat{\\eta}^{2} = \\frac{(J - 1)MCA}{(J - 1) MCA + (N - J)MCE} = \\frac{(3 - 1)281.27}{(3 - 1) 281.27 + (15 - 3)43.77} = 0.517 \\] \\[ \\hat{\\omega}^{2} = \\frac{(J - 1)(MCA-MCE)}{(J - 1)MCA + (N - J+ 1)MCE} = \\frac{(3 - 1)(281.27-43.77)}{(3 - 1)281.27 + (15 - 3+ 1)43.77} = 0.458 \\] Observamos que el valor de \\(\\hat{\\eta}^{2}\\) es mayor que el de \\(\\hat{\\omega}^{2}\\), ya que presenta estimaciones sesgadas del tamaño del efecto. No obstante, considerando el valor de \\(\\hat{\\omega}^{2}\\) podemos considerar que los tamaños de efecto en este estudio son grandes aplicando el criterio propuesto por Cohen (0.01, 0.06 y 0.14). Cuando no se cumplen los supuestos del ANOVA, Ato &amp; Vallejo (2015) proponen utilizar una medida directa de comparación de medias definida como \\(r = z\\sqrt{N}\\) donde z es la aproximación normal del estadístico U del contraste Mann-Whitney o del contraste de Kruskall-Wallis. Los criterios de clasificación son 0.1 (bajo), 0.3 (medio) y 0.5 (alto). También recomiendan el estadístico Delta de Cliff. "],["comparaciones-a-posteriori.html", "2.5 Comparaciones a posteriori", " 2.5 Comparaciones a posteriori Si el contraste estadístico indica que existen diferencias significativas entre las variables resulta necesario determinar cuáles son esas diferencias. Para ello, se han propuesto distintos métodos. Aquí solo mencionaremos algunos de ellos: el procedimiento basado en el criterio de Bonferroni, el criterio de Sidak y la prueba de Tukey. 2.5.1 Criterio de Bonferroni En un determinado estudio existen \\({2 \\choose n}\\) parejas de medias que han de ser comparadas. En el caso del ejemplo 1, tenemos que decidir si \\(\\mu_A \\neq \\mu_B\\), \\(\\mu_A \\neq \\mu_C\\) ó \\(\\mu_C \\neq \\mu_B\\). Si solo tuviéramos que realizar una comparación la tasa de error sería \\(\\alpha\\) (error tipo I). Si \\(\\alpha\\) vale 0,05 la probabilidad de cometer un error tipo I será 0,05. Sin embargo, cuando realizamos más de una comparación esta probabilidad será diferente. Mientras mayor sea el número de comparaciones a realizar, mayor será la probabilidad de rechazar la hipótesis nula cuando es verdadera (error tipo I). En nuestro caso, si tenemos que realizar 3 comparaciones la probabilidad de no cometer un error tipo I será: \\[ P(\\text{ningún error tipo I}) = {3 \\choose 0}.05^0.95^3 = 0.857 \\] Por lo que cometer algún error tipo I será 1 - 0,857 = 0,143. Es decir, la tasa de error tipo I aumenta a medida que aumentamos el número de comparaciones. En general, podemos considerar que la probabilidad de cometer al menos un error tipo I en k comparaciones utilizando \\(\\alpha\\) en cada comparación es: \\[ P(un\\; error\\; tipo\\; I\\;o\\;más) = 1 - (1 - \\alpha)^k \\] El criterio de Bonferroni es uno de los procedimientos utilizados para corregir la tasa de error. Consiste en dividir el nivel de significación \\(\\alpha\\) (habitualmente \\(\\alpha\\) = 0,05) por el número de comparaciones que pueden realizarse en el diseño. En nuestro ejemplo será \\(\\alpha_{CB}\\) = 0,05/3 = 0,01667. Esta corrección tiene sentido en el caso de que se aplique varias veces una misma prueba estadística en un estudio, ya que la probabilidad de rechazar \\(H_0\\) aumenta a medida que se incrementan las comparaciones posibles. Un criterio ligeramente diferente es el propuesto por Sidak. Este método es algo menos conservador que el de Bonferroni, ya que se cumple que \\(\\alpha_{CB} \\leq \\alpha_{CS}\\) Con ambos criterios se asume que las comparaciones son independientes. \\[ \\alpha_{CS} = 1 - (1 - \\alpha)^{1/k} \\] Aplicado a los datos del ejemplo 2.1 tenemos: Comparación del grupo A con el C: ## ## Two Sample t-test ## ## data: a and c ## t = 3.8991, df = 8, p-value = 0.004551 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## 6.128625 23.871375 ## sample estimates: ## mean of x mean of y ## 25 10 Comparación del grupo C con el B: ## ## Two Sample t-test ## ## data: c and b ## t = -2.0197, df = 8, p-value = 0.0781 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## -16.277442 1.077442 ## sample estimates: ## mean of x mean of y ## 10.0 17.6 Encontramos que sólo existen diferencias significativas entre el grupo A de los individuos sanos y el grupo C de los niños con el tumor de meduloblastoma. Como el nivel de riesgo es igual a 0,005, menor que el criterio de significación de Bonferroni (0.0167), concluimos que existen diferencias significativas entre ambos grupos. Aplicando el criterio de Sidak obtendríamos las mismas conclusiones en este caso: \\[ \\alpha_{CS} = 1 - (1 - \\alpha)^{1/k} = 1 - (1 - 0,05)^{1/3} = 0.01695 \\] 2.5.2 Prueba de Tukey Se basa en la distribución del rango estudentizado. Esta distribución tiene en cuenta el número de comparaciones entre medias existentes en el diseño. Se parte de la obtención de lo que Tukey denominó diferencia de medias significativas (DMS) que es la diferencia mínima que debe existir entre dos medias para considerar que sus medias muestrales son distintas: \\[ DMS_{Tukey} = q_{J,N-J; 1- \\alpha_{F}}\\sqrt{MCE/n} \\] donde q es el cuantil de la distribución del rango estudentizado que corresponde a un nivel de significación \\(\\alpha_F\\) con J medias y N - J grados de libertad. Aplicado al ejemplo 2.1 los resultados serían los siguientes: ## Tukey multiple comparisons of means ## 95% family-wise confidence level ## ## Fit: aov(formula = aciertos ~ grupo) ## ## $grupo ## diff lwr upr p adj ## B-A -7.4 -18.5626 3.7626 0.2213224 ## C-A -15.0 -26.1626 -3.8374 0.0097002 ## C-B -7.6 -18.7626 3.5626 0.2058874 En este caso también encontramos diferencias significativas entre los grupos A y C. "],["comparaciones-de-tendencia.html", "2.6 Comparaciones de tendencia", " 2.6 Comparaciones de tendencia Cuando la variable factor (VI) del estudio admite la ordenación, se pueden realizar las denominadas comparaciones de tendencia que nos indica el tipo de relación existente entre las variables. Pueden compararse tantas tendencias como número de niveles de factor menos 1 (J - 1). La tendencia más básica es la lineal que supone incrementos (o decrementos) constantes en los distintos valores del factor. Sin embargo, también es posible encontrar tendencias de tipo cuadrático, cúbico, etc. En la siguiente figura se muestran algunas de estas posibilidades. Figura 2.3: Tipos de tendencia Para determinar la existencia de una tendencia es necesario realizar comparaciones entre las medias de los niveles del factor. Estas medias se multiplican por una serie de coeficientes que varían dependiendo de la tendencia que queremos contrastar y de los niveles que presenta el factor. Así, por ejemplo, si la VI tiene 3 niveles los coeficientes para las tendencias lineal y cuadrática podrían ser: \\[ \\psi_{lineal} = (-1)\\mu_1 + 0\\mu2 + (1)\\mu_3 \\] \\[ \\psi_{cuad} = (1)\\mu_1 + (-2)\\mu2 + (1)\\mu_3 \\] Con la primera comparación se está definiendo una tendencia lineal ascendente y con la segunda una tendencia cuadrática. Para contrastar el tipo de tendencia existente en los datos se parte de la hipótesis nula de que no existe tendencia alguna: \\[ \\psi_{lineal} = 0 \\] El rechazo de esta hipótesis supone la existencia de tendencia y aceptación de la hipótesis alternativa \\(H_1: \\psi_{lineal} \\neq 0\\)). "],["estadísticos-f-robustos-brown-forsythe-y-welch.html", "2.7 Estadísticos F robustos: Brown-Forsythe y Welch", " 2.7 Estadísticos F robustos: Brown-Forsythe y Welch Cuando no se cumple el supuesto de homogeneidad de varianzas puede utilizarse el estadístico de Brown-Forsythe: \\[ F_{BF} = \\frac{\\sum n_j(\\bar{Y}_j - \\bar{Y})^2}{\\sum (S_{j}^2 - n_jS_{j}^2/N)} \\] Este estadístico se distribuye según el modelo de probabilidad F con J-1 y gl grados de libertad. Y gl se obtiene mediante la siguiente expresión: \\[ gl = \\frac{1}{\\sum k_{j}^2/(n_j - 1)} \\;\\; con \\; k_{j}^2 = \\frac{S_{j}^2 - n_jS_{j}^2/N}{\\sum (S_{j}^2 - n_jS_{j}^2/N)} \\] La propuesta de Welch es otra alternativa cuando no se cumple el supuesto de homogeneidad de varianzas. Su fórmula es: \\[ F_{W} = \\frac{\\sum w_j(\\bar{Y}_j - \\bar{Y^{*}})^2/(J-1)}{1+2(J-2)\\Lambda/3} \\] donde \\[ w_j = \\frac{n_j}{S_j^{2}} \\] \\[ \\bar{Y}^{*} = \\sum{w_j}{\\bar{Y}}/\\sum{w_j} \\] \\[ \\Lambda = \\frac{3\\sum{[1-(w_j/\\sum{w_j})]}^{2}/(n_j-1)}{J^{2} - 1} \\] 2.7.1 Comparaciones a posteriori Existen diferentes métodos para realizar las comparaciones a posteriori en el caso de que no se cumpla la homogeneidad de varianzas. Cuando los tamaños muestrales son grandes varios autores recomiendan el método de Games y Howell (Pardo &amp; San Martin, 2010): \\[ DMS_{GH} = q_{J,gl; 1-\\alpha_F}\\sqrt{(S_{j}^2/n_{j} + S_{j&#39;}^2/n_{j&#39;})/2} \\] donde q es el valor de la distribución del rango estudentizado. Si los tamaños muestrales son pequeños se recomienda usar la DMS propuesta por Dunnet: \\[ DMS_{T3-Dunnet} = q_{J,gl; 1-\\alpha_F}\\sqrt{S_{j}^2/n_{j} + S_{j&#39;}^2/n_{j&#39;}} \\] donde q es el valor de la distribución del rango máximo estudentizado. Este estadístico esta basado en el estadístico T2 de Tamhane. La diferencia es que el estadístico T2 utiliza la distribución T de Student y la desigualdad de Sidak para controlar la tasa de error. "],["prueba-de-kuskal-wallis-kw.html", "2.8 Prueba de Kuskal-Wallis (KW)", " 2.8 Prueba de Kuskal-Wallis (KW) Esta prueba no necesita que se cumplan los supuestos de normalidad y homogeneidad propias del ANOVA. Además, puede utilizarse cuando los datos son de tipo ordinal. Este método pretende contrastar la hipótesis de que J muestras aleatorias independientes proceden de la misma población (o de distintas poblaciones). Para ello, se asignan rangos desde 1 hasta N al conjunto de las \\(Y_{ij}\\) observaciones del diseño como si fuera un única muestra (si hay empates se aplica el promedio de los rangos implicados). Al final se obtiene un rango \\(R_{ij}\\) para cada una de las observaciones. Figura 2.4: Esquema del diseño para la prueba de Kruskall-Wallis El estadístico H de Kruskal-Wallis viene definido de la siguiente forma: \\[ H = \\frac{12}{N(N+1)}\\sum_{j=1}^{J}\\frac{R_j}{n_j} - 3(N - 1) \\] Bajo la hipótesis nula de que las J poblaciones tienen la misma forma, el estadístico H se distribuye según un modelo de probabilidad ji-cuadrado con J - 1 grados de libertad. El rechazo de esta hipótesis supone que los J promedios comparados no son iguales. Ejemplo 2.2: Supongamos que estamos interesados en el estudio de los 3 métodos de enseñanza para mejorar el rendimiento de los escolares. Los resultados fueron los siguientes: Tabla 2.4: Datos del ejemplo 2.2 A B C s1 26 22 7 s2 35 5 7 s3 17 22 18 s4 23 21 6 s5 24 20 12 Figura 2.5: Boxplot del ejemplo 2.2 Observamos que los grupos son bastante asimétricos. En particular, se tiene que el grupo B muestra una distribución muy alejada de la normalidad. Esto puede comprobarse mediante el estadístico de Shapiro-Wilks: Media del grupo A: ## ## Shapiro-Wilk normality test ## ## data: aciertos2[1:5] ## W = 0.94691, p-value = 0.7151 Media del grupo B: ## ## Shapiro-Wilk normality test ## ## data: aciertos2[6:10] ## W = 0.64863, p-value = 0.002565 Media del grupo C: ## ## Shapiro-Wilk normality test ## ## data: aciertos2[11:15] ## W = 0.82664, p-value = 0.1313 En este caso es adecuado aplicar el estadístico de Kruskal- Wallis. Kruskal-Wallis rank sum test data: aciertos2 by grupo Kruskal-Wallis chi-squared = 7.6473, df = 2, p-value = 0.02185 Tabla 2.5: Rangos del ejemplo 2.2 A B C s1 14 10.5 3.5 s2 15 1.0 3.5 s3 6 10.5 7.0 s4 12 9.0 2.0 s5 13 8.0 5.0 En la tabla 2.5 aparece el rango para cada una de las puntuaciones. Calculando las medias de los rangos de cada uno de los grupos observamos que las menores puntuaciones están en el grupo C seguido del grupo B: Media del grupo A: ## [1] 12 Media del grupo B: ## [1] 7.8 Media del grupo C: ## [1] 4.2 Al igual que en el caso del ANOVA, para conocer cuáles son las diferencias significativas entre los grupos será necesario realizar una prueba post-hoc. 2.8.1 Comparaciones a posteriori Cuando no se cumplen los supuestos de aplicación del ANOVA las comparaciones a posteriori se pueden realizar mediante la técnica U de Mann-Whitney (MW)4. Esta prueba sirve, al igual que la prueba T de Student para comparar dos muestras independientes. Resulta útil cuando no se cumple la hipótesis de normalidad o los datos son ordinales. Sean dos muestras aleatorias de tamaño \\(n_1\\) y \\(n_2\\). Sea \\(R_{i}\\) el rango asignado a cada una de las puntuaciones cuando se toman todas en conjunto como si fuera una única muestra. A continuación, se calculan los siguientes estadísticos \\(S_1\\) y \\(S_2\\) que corresponden a la suma de los rangos de las observaciones del grupo 1 y grupo 2 respectivamente: \\[ S_1 = \\sum_{i=1}^{n_1}R_{i1} \\qquad y \\qquad S_2 = \\sum_{i=1}^{n_2}R_{i2} \\] Se verifica que la suma de los N rangos vale (Pardo &amp; San Martin, 2010): \\[ S_1 + S_2 = \\frac{N(N+1)}{2} \\] Si se asume que las dos muestras proceden de la misma población cabe esperar que \\(S_1\\) y \\(S_2\\) sean iguales o parecidos y que solo muestren pequeñas variaciones debidas al azar. Por tanto, el estadístico U sería cualquiera de los dos sumandos. Por ejemplo, U = \\(S_1\\). Una vez definido el estadístico, el paso siguiente es determinar su distribución de probabilidad. Esta distribución no es muy complicada en el caso de que tengamos pocos valores y podremos calcular el valor de probabilidad exacto. Cuando el número de valores es relativamente grande será necesario realizar la siguiente aproximación: \\[ Z = \\frac{U - \\mu_U}{\\sigma_U} \\qquad donde \\qquad \\mu_U = \\frac{n_1(N+1)}{2} \\qquad y \\qquad \\sigma_U = \\sqrt{n_1n_2(N+1)/12} \\] Ejemplo En el ejemplo 2.2 que se estudió cuando se introdujo la técnicas KW encontramos que había diferencias significativas por lo que será necesario determinar las diferencias entre los distintos métodos de enseñanza. Necesitamos determinar que métodos son los que muestran diferencias significativas: Prueba de U de Mann-whitney para métodos A y B: ## ## Wilcoxon rank sum test with continuity correction ## ## data: b[1:5] and b[6:10] ## W = 21, p-value = 0.09369 ## alternative hypothesis: true location shift is not equal to 0 Tamaño del efecto para métodos A y B: ## ## Cliff&#39;s Delta ## ## delta estimate: 0.68 (large) ## 95 percent confidence interval: ## lower upper ## -0.2847567 0.9604032 Prueba de U de Mann-whitney para métodos A y C: ## ## Wilcoxon rank sum test with continuity correction ## ## data: b[1:5] and b[11:15] ## W = 24, p-value = 0.02118 ## alternative hypothesis: true location shift is not equal to 0 Tamaño del efecto para métodos A y C: ## ## Cliff&#39;s Delta ## ## delta estimate: 0.92 (large) ## 95 percent confidence interval: ## lower upper ## 0.5171092 0.9891504 Prueba de U de Mann-whitney para métodos B y C: ## ## Wilcoxon rank sum test with continuity correction ## ## data: b[6:10] and b[11:15] ## W = 20, p-value = 0.1412 ## alternative hypothesis: true location shift is not equal to 0 Tamaño del efecto para métodos B y C: ## ## Cliff&#39;s Delta ## ## delta estimate: 0.6 (large) ## 95 percent confidence interval: ## lower upper ## -0.4370519 0.9522030 Si consideramos un criterio de significación \\(\\alpha\\) = 0,05 tenemos que corregir el azar debido a que hemos realizado más de un test. Aplicando el criterio de Bonferroni (\\(\\alpha_C\\) = 0,05/3 = 0.0167) concluimos que no hay ninguna diferencia significativa. La misma conclusión obtendríamos si aplicáramos la corrección de Sidak (\\(\\alpha_C\\) = 0,017). El problema de utilizar la prueba de U de Mann-Whitney para realizar las comparaciones a posteriori es que no se mantienen los rangos del conjunto de observaciones, produciéndose un reajuste de los rangos en cada una de las comparaciones dos a dos. Existen varias alternativas a este procedimiento como la prueba de Dunn, la de Conover, el test de Nemenyi o la prueba de Dwass-Steel-Critchlow-Fligner (DSCF). En el programa de JAMOVI las comparaciones a posteriori se realizan mediante la prueba de DSCF. Se considera que un contraste es significativo si satisface: \\[ W_j = -\\frac{n_i(ni+n_j+1)}{2}/\\frac{n_in_j}{24}\\times \\Lambda &gt; q_{\\alpha,k}, \\quad \\text{para} \\quad 1\\leq i\\leq j\\leq k \\] donde: \\[ \\Lambda = n_i+n_j -1-\\frac{\\sum_{b=1}^{g_{ij}}{(t_{b}-1)tb(t_{b}+1)}}{(n_i +n_j)(n_i+n_j -1)} \\] \\(q_{\\alpha,k}\\) es una cuantila de la distribución normal de los rangos para k grupos, \\(n_i\\) es el número de sujetos del i-ésimo grupo, \\(n_j\\) es el número de sujetos del j-ésimo grupo, \\(t_b\\) es el número de ocurrencias en el rango b y \\(W_{ij}\\) es la suma de los rangos para el i-ésimo grupo donde los rangos se han comparado con el grupo j. Los resultados de aplicar esta prueba aparecen a continuación. Encontramos que sólo aparecen diferencias entre los grupos A y C. ## A B ## B 0.176 - ## C 0.042 0.256 JAMOVI utiliza la prueba de Dwass-Steel-Critchlow-Fligner↩︎ "],["anova-de-un-factor-con-jamovi.html", "2.9 ANOVA de un factor con JAMOVI", " 2.9 ANOVA de un factor con JAMOVI Este programa incluye un módulo específico para las técnicas de ANOVA donde aparecen dos posibilidades para realizar el ANOVA de 1 factor para muestras independientes: 1) Opción one-way ANOVA de un factor y 2) opción ANOVA. Asimismo, incluye en ese módulo la posibilidad de realizar el ANOVA no paramétrico de Kruskal-Wallis. Figura 2.6: Opciones ANOVA en el programa JAMOVI En la opción “ANOVA de un factor” es posible elegir entre la prueba de Fisher que asume homogeneidad de varianzas y la prueba de Welch que no asume igualdad de las varianzas. En esta opción se pueden introducir más de una VD. Figura 2.7: Opciones ANOVA en el programa JAMOVI Asimismo, con esta opción podemos elegir entre distintos métodos de comparación a posteriori tanto para la situación en la que existe homogeneidad de varianzas (Tuckey) como en el caso de que no exista homogeneidad de varianzas (Games-Howell). Figura 2.8: Opciones ANOVA en el programa JAMOVI Si se elige la opción de “ANOVA” sólo estará disponible la posibilidad de comparar grupos con varianzas homogéneas. Sin embargo, esta opción presenta la ventaja de obtener diferentes medidas del tamaño del efecto y pueden incluirse varias VVII para estudiar el efecto de la interacción entre ellas. Figura 2.9: Opciones ANOVA en el programa JAMOVI Si se elige la opción de ANOVA no paramétrico se obtendrá el valor del estadístico K-W así como la medida de tamaño de efecto \\(\\epsilon^{2}\\) y las comparaciones a posteriori de Dwass-Steel-Critchlow-Fligner. Figura 2.10: Opciones para ANOVA no paramétrico en el programa JAMOVI "],["anova-de-un-factor-con-spss.html", "2.10 ANOVA de un factor con SPSS", " 2.10 ANOVA de un factor con SPSS Este análisis puede realizarse en el programa SPSS con dos procedimientos: Procedimiento de Comparar medias y procedimiento de Modelo lineal general. Si se utiliza la primera opción hay que seguir la siguiente secuencia Analizar + Comparar medias + ANOVA de un factor: Figura 2.11: ANOVA de un factor en SPSS Este procedimiento dispone de varias opciones. Pulsando en la opción de “Contrastes” se puede realizar comparaciones de tendencias (polinómicas). Figura 2.12: Prueba t para muestras relacionadas en SPSS También se pueden realizar comparaciones a posteriori tanto cuando se cumple el supuesto de igualdad de varianzas como en el caso de que no se cumpla este supuesto. Figura 2.13: Comparaciones a posteriori para ANOVA de 1 factor en SPSS Pulsando el botón de Opciones podemos acceder a los descriptivos, a la prueba de homogeneidad de varianzas y a las pruebas de Brown-Forsythe y Welch. Figura 2.14: Botón de opciones de ANOVA de un factor en SPSS Si se utiliza el procedimiento de Modelo lineal general hay que seguir la siguiente secuencia Analizar + Modelo lineal general + Univariado. Mediante este procedimiento también se dispone de las opciones anteriormente vistas. Para realizar la prueba de Kruskal-Wallis hay que seguir la siguiente secuencia Analizar + Pruebas no paramétricas + muestras independientes. El programa realiza de manera automática el contraste de Kruskal-Wallis y las comparaciones a posteriori usando la corrección de Bonferroni. Figura 2.15: Prueba de Kruskal-Wallis en SPSS Figura 2.16: Comparaciones en la prueba Kruskal-Wallis en SPSS "],["relación-entre-una-variable-cualitativa-y-otra-cuantitativa-ii-diseños-longitudinales.html", "3 Relación entre una variable cualitativa y otra cuantitativa (II): Diseños longitudinales", " 3 Relación entre una variable cualitativa y otra cuantitativa (II): Diseños longitudinales La principal característica de este tipo de diseños es que se administran todos los valores del factor (VI) a cada uno de los sujetos. Por tanto, se disponen de varias medidas repetidas no independientes por cada sujeto presente en la investigación. La estructura del diseño básico con 3 momentos de observación sería la siguiente: Figura 3.1: Esquema del diseño unifactorial de medidas repetidas Supongamos que estamos interesados en presentar imágenes relacionadas con emociones de alegría, tristeza o neutra y ver cómo afectan a una respuesta psico-fisiológica. Para realizar este estudio podríamos haber creado 3 grupos de individuos y asignarle al azar las imágenes de un contenido distinto. Por tanto, habría un grupo en el que sólo verían imágenes de alegría, otro imágenes de tristeza y otro imágenes de tipo neutro. Sin embargo, también cabe la posibilidad de seleccionar un único grupo y presentarle en momentos distintos cada una de las imágenes (alegría, tristeza y neutras). La ventaja de actuar así es que reduciríamos el número de sujetos presentes en el estudio con lo que se facilitaría el costo de la investigación. En cambio, tendríamos la dificultad de que las observaciones registradas en cada individuo no son independientes pudiéndose producir efectos de secuenciación que pueden ser de dos tipos generales: 1) efectos de arrastre como consecuencia de aplicar un tratamiento sin que haya desaparecido el efecto de otro aplicado previamente, y 2) efectos de la práctica resultado de la mejora de la respuesta de los sujetos como consecuencia de la repetición (Ato &amp; Vallejo, 2015; Pardo &amp; San Martin, 2010). Una forma de eliminar estos efectos de secuenciación es mediante el contrabalanceo que consiste en administrar los tratamientos en orden distintos a cada participante en la investigación. Este contrabalanceo puede aplicarse aleatoriamente pudiendo ser completo o incompleto. Para profundizar en estos procedimientos es conveniente consultar alguna obra de metodología general (León &amp; Montero, 2002). El árbol de decisión para estos ensayos aparece en la siguiente figura. Figura 3.2: Árbol de decisión del diseño univariable de medidas repetidas "],["modelo-anova-1-factor-de-medidas-repetidas-a1mr.html", "3.1 Modelo ANOVA 1 factor de medidas repetidas (A1MR)", " 3.1 Modelo ANOVA 1 factor de medidas repetidas (A1MR) La estrategia de análisis para este tipo de diseños si se cumple el supuesto de normalidad de los residuales parte del siguiente modelo: \\[ Y_{ij} = \\mu + s_i + \\alpha_{j} + \\epsilon_i \\] donde \\(Y_{ij}\\) es la puntuación del sujeto i en la condición j; \\(\\mu\\) es la media total del diseño; \\(s_i\\) son efectos aleatorios del individuo i (\\(s_i = \\mu_i - \\mu\\)) y se suponen distribuidos según una ley normal (NID(0,\\(\\sigma^2\\))), \\(\\alpha_j = \\mu_j - \\mu\\) son efectos fijos constantes para cada nivel de tratamiento independientemente de los sujetos; \\(\\epsilon_i\\) es \\(Y_{ij} - s_i-\\alpha_{j}\\) son efectos residuales aleatorios específicos para cada puntuación. En este modelo se asumen que las covarianzas entre los tratamientos y entre los individuos no existen. La hipótesis nula de este modelo es: \\[ H_0: \\alpha_1 = \\alpha_1 = \\cdots = \\alpha_J =0 \\] Por tanto, el modelo asociado a esta hipótesis (modelo reducido) es el siguiente: \\[ Y_{ij} = \\mu + s_i + \\epsilon_i \\] Los estimadores por máxima verosimilitud de los componentes del modelo son: \\[ \\hat{\\mu} = \\bar{Y}_{++} \\] \\[ \\hat{\\alpha}_j = \\bar{Y}_{+j} - \\bar{Y}_{++} \\] \\[ \\hat{s}_i = \\bar{Y}_{i+} - \\bar{Y}_{++} \\] La variabilidad total de este estudio puede descomponerse en distintos elementos: 1) varianza intersujetos: debida a los tratamientos, 2) varianza intrasujeto debida a cada uno de los sujetos, y 3) varianza del error la que se da entre cada puntuación y sus correspondientes medias marginales. Estas tres fuentes de variación pueden calcularse mediante las siguientes expresiones: Varianza intersujetos: \\[ MC_A = n\\sum_{j}(\\bar{Y}_{+j}-\\bar{Y})^2/(J-1) \\] Varianza intrasujetos: \\[ MC_S = J\\sum_{i}(\\bar{Y}_{i+}-\\bar{Y})^2/(n-1) \\] Varianza del error: \\[ MC_E = \\sum_{i}\\sum_{j}(Y_{ij}- \\bar{Y}_{i+} - \\bar{Y}_{+j}-\\bar{Y})^2/[(J-1)(n-1)] \\] Al igual que ocurría en el diseño de 1 factor completamente aleatorizado, puede compararse la variabilidad debida a los tratamientos con la variabilidad debida al error mediante el estadístico F: \\[ F = \\frac{MC_A}{MC_E} \\sim \\mathcal{F}(j-1,[(j-1)(n-1)]) \\] donde \\(\\mathcal{F}\\) es la distribución teórica. Con este estadístico podemos contrastar que las J medias son iguales. "],["ejemplo-1-de-diseño-unifactorial-de-medidas-repetidas.html", "3.2 Ejemplo 1 de diseño unifactorial de medidas repetidas", " 3.2 Ejemplo 1 de diseño unifactorial de medidas repetidas A continuación, presentamos los resultados de un estudio en el que se quiso estudiar si el contenido emocional de una imagen provocaba cambios en una respuesta psicofisiológica: Tabla 3.1: Datos del ejemplo 3.1 tristeza neutra alegria 72 73 80 64 77 84 70 83 90 62 71 86 Tabla 3.1: Correlaciones del ejemplo 3.1 tristeza neutra alegria tristeza 1.00 0.37 -0.20 neutra 0.37 1.00 0.67 alegria -0.20 0.67 1.00 Figura 3.3: Boxplot del ejemplo 3.1 Calculamos las medias cuadráticas del diseño y tenemos: Varianza intersujetos: \\[ MC_A = n\\sum_{j}(\\bar{Y}_{+j}-\\bar{Y}_{++})^2/(j-1) = 4((-9)^2 + 0^2 + 9^2)/2 = 648/2 = 324 \\] Varianza intrasujetos: \\[ MC_S = J\\sum_{i}(\\bar{Y}_{i+}-\\bar{Y}_{++})^2/(n-1) = 3((-1)^2 + (-1)^2 + 5^2 + (-3)^2)/3 = 108/3 = 36 \\] Varianza del error: \\[ MC_E = \\sum_{i}\\sum_{j}(Y_{ij}- \\bar{Y}_{i+}- \\bar{Y}_{+j}-\\bar{Y}_{++})^2/[(j-1)(n-1)] = \\\\ ((72-75-67 + 76)^2 + \\cdots + (86-73-85 + 76)^2)/(2*3) = 96/6 = 16 \\] \\[ F = \\frac{MC_A}{MC_E}= \\frac{324}{16}= 20.25 \\] La región crítica en este caso corresponde a \\(P(\\mathcal{F}_{2,6} &gt; F)\\) = 0.002. Por tanto, rechazamos la hipótesis nula de que las medias de las distintas condiciones son iguales. Estos resultados coinciden con los obtenidos mediante el programa R. ## $univariate.tests ## Sum Sq num Df Error SS den Df F value Pr(&gt;F) ## (Intercept) 69312 1 108 3 1925.33 2.606e-05 *** ## emocion 648 2 96 6 20.25 0.002148 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 "],["comprobación-de-los-supuestos-1.html", "3.3 Comprobación de los supuestos", " 3.3 Comprobación de los supuestos Ya hemos mencionado que para que pueda aplicarse el modelo estadístico de A1FMR es necesario que se cumplan las condiciones de independencia, normalidad y homogeneidad de varianzas 5. Ahora bien, en este tipo de diseño la independencia hace referencia a que las medidas de un mismo sujeto no estén relacionadas. Asimismo, el efecto de la VI debe ser independiente de la interacción del sujeto con el tratamiento. Cuando se cumplen estos supuestos, el estadístico F se distribuye como un estadístico \\(\\mathcal{F}_{gl_1,gl_2}\\) donde \\(gl_1 = j - 1\\) y \\(gl_2 = (j - 1)(n - 1)\\). Estos supuestos están relacionados con las características de la matriz de varianzas-covarianzas del diseño que contiene la varianza de cada medida en la diagonal y las covarianzas entre cada par de medidas. Esta característica de la matriz se denomina simetría compuesta y decimos que la matriz es esférica cuando las varianzas y covarianzas son iguales. Sin embargo, es muy poco probable que los datos de las investigaciones psicológicas que utilizan este tipo de diseños lo cumplan, ya que es razonable considerar que los errores de un sujeto (y sus medidas) estén correlacionados entre sí en algún grado. Por tanto, para poder aplicar el modelo estadístico necesitamos relajar estos supuestos. Para ello, se considera suficiente que las covarianzas entre las medidas sean iguales. Más concretamente, es suficiente que las varianzas de las diferencias entre cada par de medidas sean iguales 6. Si se cumple esto, el estadístico F sigue la distribución teórica con \\(\\mathcal{F}_{gl_1,gl_2}\\) donde \\(gl_1 = j - 1\\) y \\(gl_2 = (j - 1)(n - 1)\\). En el caso de que el estadístico F no siga la distribución anteriormente mencionada se vuelve liberal (aumenta la probabilidad de cometer errores tipo I), y puede demostrarse que sigue una distribución \\(\\mathcal{F}_{gl_1,gl_2}\\) donde \\(gl_1 = \\epsilon(j - 1)\\) y \\(gl_2 = \\epsilon(j - 1)(n - 1)\\) (Maxwell &amp; Delaney, 2004). El valor de \\(\\epsilon\\), es desconocido y necesitamos estimarlo a partir de nuestros datos. Se han propuesto varias alternativas para estimarlo. Una de ellas, postulada por Greenhouse-Geisser, proponen una estimación de \\(\\hat{\\epsilon}\\) que es más conservadora que la propuesta por Huyndt-Feldt (\\(\\tilde{\\epsilon}\\)). No obstante, el estadístico F es el más conservador de los tres por lo que si éste estadístico es significativo los otros dos también lo serán. En el ejemplo anterior los valores de \\(\\epsilon\\) son los siguientes: ## $sphericity.tests ## Test statistic p-value ## emocion 0.66667 0.66667 ## $pval.adjustments ## GG eps Pr(&gt;F[GG]) HF eps Pr(&gt;F[HF]) ## emocion 0.75 0.006540799 1.333333 0.0021483 ## attr(,&quot;na.action&quot;) ## (Intercept) ## 1 ## attr(,&quot;class&quot;) ## [1] &quot;omit&quot; Observamos que el valor obtenido por Geiser-Greenhouse (0.75) es menor que el de Huyndt-Feldt (1.33 \\(\\simeq\\) 1). En ambos casos, los estadísticos nos llevan a rechazar la hipótesis nula de igualdad de medias. El resultado denominado sphericity.test con valor de 0.66 es el test de Mauchly, indicándonos que se acepta la hipótesis de esfericidad. Hay ocasiones en las que existen más medidas repetidas que sujetos. En estos casos, Pardo &amp; San Martin (2010) recomiendan utilizar el estadístico F modificando los grados de libertad con el menor valor de \\(\\epsilon\\). Si no se rechaza la hipótesis con esta opción, utilizar las estimaciones de \\(\\epsilon\\) propuestas anteriormente (la de Geisser-Greenhouse o Huyndt-Feldt, cualquiera de ellas vale). Pardo &amp; San Martin (2010) y Ato &amp; Vallejo (2015) señalan con acierto que los modelos ANOVA son muy robustos en situaciones donde las variables no siguen una distribución normal salvo en aquellas en las que los diseños no están balanceados o las distribuciones son muy asimétricas. Sin embargo, mantendremos aquí el cumplimiento de las dos condiciones con objeto de facilitar el aprendizaje.↩︎ Si en el diseño sólo hay dos medidas por sujeto siempre se cumple el supuesto de esfericidad.↩︎ "],["tamaño-del-efecto-2.html", "3.4 Tamaño del efecto", " 3.4 Tamaño del efecto Las medidas propuestas para el modelo de A1CA (\\(\\eta^2\\) corregida y \\(\\omega^2\\)), también son válidas aquí. La preferida por los investigadores (Pardo &amp; San Martin, 2010) es 7: \\[ \\omega^2 = \\frac{\\sigma_{\\alpha}^2}{\\sigma_{s}^2+\\sigma_{\\alpha}^2+\\sigma_{e}^2} \\] cuando se compara el efecto relativo del tratamiento con respecto a los efectos totales del diseño. \\(\\sigma_{\\alpha}^2\\) es la varianza debida a los tratamientos, \\(\\sigma_{s}^2\\) es la varianza debida a los sujetos y \\(\\sigma_{e}^2\\) es la varianza debida al error. Maxwell &amp; Delaney (2004) también proponen utilizar la medida de \\(\\omega^2\\) parcial: \\[ \\omega^2 = \\frac{\\sigma_{\\alpha}^2}{\\sigma_{\\alpha}^2+\\sigma_{e}^2} \\] \\(\\omega^2\\) parcial también puede calcularse a partir del estadístico F 8: \\[ \\omega^2 = \\frac{(j -1)(F - 1)}{(j - 1)(F - 1) + nj} \\] A partir de \\(\\omega^2\\) parcial también puede calcularse la medida de tamaño del efecto de Cohen: \\[ \\hat{\\delta} = \\sqrt{\\frac{\\hat{\\omega}^2}{( 1 - \\hat{\\omega}^2)}} \\] Al igual que en el tema anterior los valores para considerar un efecto bajo, medio o grande son 0.01, 0.06 y 0.14 respectivamente. En el caso de \\(\\hat{\\delta}\\) sus valores son 0.1, 0.25 y y 0.4 respectivamente. Aplicado a los datos del estudio de las emociones: \\[ \\omega^2 = \\frac{(3 -1)(20.25 - 1)}{(3 - 1)(20.25 - 1) + 4*3} =0.76 \\] \\[ \\hat{\\delta} = \\sqrt{\\frac{0.76}{( 1 - 0.76)}} = 1.79 \\] Ambos estadísticos indican que la relación entre el contenido emocional de las imágenes y la respuesta psicofisiológica es muy grande. Otra medida con resultados semejantes a \\(\\omega^{2}\\) es eta cuadrado generalizada (\\(\\eta_g^{2}\\)).↩︎ En el caso de que este cálculo resulte negativo se considera que \\(\\omega^2\\) vale cero↩︎ "],["potencia-de-la-prueba.html", "3.5 Potencia de la prueba", " 3.5 Potencia de la prueba La potencia de la prueba nos permite determinar la probabilidad de que en un estudio se rechace la hipótesis nula cuando es falsa.En un diseño A1MR depende de varios factores: 1) el nivel \\(\\alpha\\) de significación, la correlación entre las VVDD, el tamaño del efecto que se desea contrastar, el número de condiciones en el diseño, el valor estimado de \\(\\epsilon\\) y el número de sujetos. Aplicado al estudio de las emociones anteriormente presentado tenemos que hemos utilizado \\(\\alpha\\) = 0.05, la correlación media entre las distintas medidas es 0.37, el tamaño del efecto medido con el estadístico \\(\\delta\\) es 1.79, tenemos 3 condiciones, el valor más conservador de \\(\\epsilon\\) es 0.75 y hemos estudiado a 4 sujetos. Bajo estas condiciones y utilizando el programa Gpower (http://www.gpower.hhu.de) obtenemos un valor de potencia de 0.9949. Por lo general, suele considerarse razonable un valor de potencia de 0.8. Si hubiéramos utilizado tres sujetos la potencia también seguiría siendo razonable (0.901). "],["comparaciones-múltiples.html", "3.6 Comparaciones múltiples", " 3.6 Comparaciones múltiples La estrategia para realizar las comparaciones múltiples es similar a la utilizada en la estrategia transversal: \\[ \\phi_i = \\sum_{j} c_{j}Y_{ij} \\] donde \\(Y_{ij}\\) son las medidas de los individuos y \\(c_j\\) son los coeficientes multiplicadores. En el caso concreto, de las comparaciones a posteriori Pardo &amp; San Martin (2010) proponen utilizar la prueba t para cada una de las diferencias. Así, por ejemplo, en nuestro caso la comparación entre la condición de tristeza y neutra será: ## ## Paired t-test ## ## data: datos$tristeza and datos$neutra ## t = -3.182, df = 3, p-value = 0.05002 ## alternative hypothesis: true mean difference is not equal to 0 ## 95 percent confidence interval: ## -18.001317453 0.001317453 ## sample estimates: ## mean difference ## -9 Para el caso donde se compara la condición de tristeza y alegría será: ## ## Paired t-test ## ## data: datos$alegria and datos$tristeza ## t = 5.1962, df = 3, p-value = 0.01385 ## alternative hypothesis: true mean difference is not equal to 0 ## 95 percent confidence interval: ## 6.975683 29.024317 ## sample estimates: ## mean difference ## 18 Finalmente la comparación de la condición de neutra y alegría será: ## ## Paired t-test ## ## data: datos$alegria and datos$neutra ## t = 4.5, df = 3, p-value = 0.02049 ## alternative hypothesis: true mean difference is not equal to 0 ## 95 percent confidence interval: ## 2.635107 15.364893 ## sample estimates: ## mean difference ## 9 Aplicando la corrección de Bonferroni (p = 0.017), encontramos que la condición de alegría es la que muestra diferencias significativas con respecto a las otras dos. En este tipo de diseños cabe cualquier otro tipo de combinación lineal. Así, por ejemplo, podríamos plantearnos comparar si las medidas de la condición de tristeza y la de neutra tomadas conjuntamente muestran diferencias significativas con respecto a la condición de alegría. En este caso, la comparación a realizar tendría los siguientes coeficientes 9: \\[ \\phi_i = \\frac{1}{2}Y_{iT} + \\frac{1}{2}Y_{iN} - (1)Y_{iA} \\] Una vez calculado el contraste se aplicaría la prueba t de Student para 1 muestra 10. Los resultados son los siguientes: ## ## Shapiro-Wilk normality test ## ## data: cont ## W = 0.94466, p-value = 0.683 ## ## One Sample t-test ## ## data: cont ## t = -5.5114, df = 3, p-value = 0.01176 ## alternative hypothesis: true mean is not equal to 0 ## 95 percent confidence interval: ## -21.29537 -5.70463 ## sample estimates: ## mean of x ## -13.5 Atendiendo a los resultados, podemos considerar que la condición de alegría presenta cambios en la respuesta psicofisiológica significativos con respecto a las otras dos condiciones tomadas conjuntamente. Obsérvese que la suma de los coeficientes (1/2 + 1/2 - 1) suman cero.↩︎ Si no se cumple el supuesto de normalidad habría que aplicar la prueba W de Wilcoxon.↩︎ "],["prueba-de-friedman.html", "3.7 Prueba de Friedman", " 3.7 Prueba de Friedman Cuando la variable no cumple el supuesto de normalidad o la VD está medida de manera ordinal lo recomendable es utilizar la prueba de Friedman para estudiar si existen diferencias significativas entre las distintas condiciones. Esta prueba es una extensión de la prueba W de Wilcoxon para dos medidas.Para poder utilizar esta prueba las respuestas deben ser variables continuas y estar medidas por lo menos en una escala ordinal. La hipótesis nula que se contrasta es que las respuestas asociadas a cada uno de los “tratamientos” tienen la misma distribución de probabilidad o distribuciones con la misma mediana, frente a la hipótesis alternativa de que por lo menos la distribución de una de las respuestas difiere de las demás. Rechazar ésta hipótesis implica que los centros de la distribución proceden de poblaciones distintas. Para contrastar esta hipótesis hay que transformar las puntuaciones de cada individuo en puntuaciones ordenadas (rangos). Si hay empates las puntuaciones se sustituyen por el rango medio. 3.7.1 Ejemplo de diseño unifactorial de medidas repetidas no paramétrico Si aplicamos esta técnica a los datos del ejemplo 1: Tabla 3.2: Datos del ejemplo 1 tristeza neutra alegria 72 73 80 64 77 84 70 83 90 62 71 86 Los rangos por filas de estas observaciones son: Tabla 3.3: Rangos del ejemplo 3.1 V1 V2 V3 1 2 3 1 2 3 1 2 3 1 2 3 El estadístico Q de Friedman se calcula mediante la siguiente expresión: \\[ Q = \\frac{12}{nj(j+1)} \\sum_{1}^{j}R_{+j}^2 - 3n(j+1) \\] donde \\(R_{+j}^2\\) son los suma de los rangos por columna. El valor aplicado a nuestros datos es: \\[ Q = \\frac{12}{4*3(3+1)}(4^2 + 8^2 + 12^2) - 3*4*4 = \\frac{224}{4}-48 = 8 \\] tenemos que P(\\(\\chi^2(2)\\) &gt; Q) = 0.018 por lo que rechazamos la hipótesis nula de que los rangos medios de cada condición proceden de la misma población. Utilizando el programa R obtenemos los mismo resultados: ## ## Friedman rank sum test ## ## data: as.matrix(datos) ## Friedman chi-squared = 8, df = 2, p-value = 0.01832 "],["modelo-a1fmr-con-el-programa-jamovi.html", "3.8 Modelo A1FMR con el programa JAMOVI", " 3.8 Modelo A1FMR con el programa JAMOVI Tenemos que seleccionar Analysis + Repeated Measures ANOVA tal y como aparece en la figura que se presenta a continuación. Figura 3.4: Selección del modelo A1FMR en JAMOVI Definimos las variables e introducimos cada variable en su lugar. Asimismo, damos nombre a la variable dependiente: Figura 3.5: Definición de las variables en JAMOVI Para comprobar la normalidad de las variables tenemos que utilizar el módulo de “Exploración”. Figura 3.6: Supuesto de normalidad en JAMOVI La esfericidad se comprueba con la opción “Comprobación de supuestos”: Figura 3.7: Supuesto de esfericidad en JAMOVI Si se encuentran diferencias significativas utilizamos Bonferroni como procedimiento de comparaciones múltiples tal y como aparece en la siguiente figura. Figura 3.8: Comparaciones múltiples en JAMOVI "],["prueba-de-friedman-con-el-programa-jamovi.html", "3.9 Prueba de Friedman con el programa JAMOVI", " 3.9 Prueba de Friedman con el programa JAMOVI Seleccionamos la prueba Friedman: Figura 3.9: Prueba de Friedman en JAMOVI Introducimos las variables y marcamos las comparaciones múltiples de Durbin-Conover. Figura 3.10: Comparaciones post hoc de Friedman en JAMOVI "],["modelo-a1fmr-con-el-programa-spss.html", "3.10 Modelo A1FMR con el programa SPSS", " 3.10 Modelo A1FMR con el programa SPSS Tenemos que seleccionar Analizar + Modelo lineal general + Medidas repetidas tal y como aparece en la figura siguiente: Figura 3.11: Selección del modelo A1FMR en SPSS Le damos nombre al factor, introducimos el número de niveles y pulsamos el botón de añadir Figura 3.12: Modelo A1FMR en SPSS: Identificación de los factores y niveles Incluimos los tres niveles y marcamos el botón de Opciones. Este botón da lugar a otra ventana en la que podremos obtener descriptivos de cada variable, y pruebas para el diagnóstico del modelo. Marcando el botón de Gráficos podemos obtener el gráfico de las medias Figura 3.13: Modelo A1FMR en SPSS: Definición de los niveles El estudio descriptivo nos muestra los promedios de cada condición, su desviación estándar y el número de casos válidos Figura 3.14: Modelo A1FMR en SPSS: Descriptivos Introducimos el factor en el cuadro de Mostrar medias y elegimos la corrección de Bonferroni. Figura 3.15: Modelo A1FMR en SPSS: Corrección de Bonferroni Observamos que el estadístico de Mauchly nos indica que se cumplen los supuestos de esfericidad. En la tabla, además del estadístico W de Mauchly (W = 0.667), aparecen la aproximación Chi-cuadrado, sus grados de libertad (gl = 2) y la significación (p = 0.667). También aparecen las estimaciones de \\(\\epsilon\\) de Geisser-Greenhouse (\\(\\hat{\\epsilon}\\) = 0.75, el de Huyndt-Feldt \\(\\tilde{\\epsilon}\\) = 1 y la estimación del limite inferior = 0.5). Figura 3.16: Modelo A1FMR en SPSS: Prueba de Mauchly Dado que se cumplen los supuestos de esfericidad, consideramos válida la prueba F Figura 3.17: Modelo A1FMR en SPSS: Prueba F Observamos que existen diferencias significativas entre las imágenes que representaban emociones de alegría con las que representaban emociones neutras Figura 3.18: Modelo A1FMR en SPSS: Post hoc 3.10.1 Conclusiones según normas APA: Se encontró que hubo diferencias significativas entre las distintas emociones (F(2,6) = 20.25, p = .002, \\(\\eta_{parcial}^{2}\\) corregida = 0.44 indicando un efecto grande). La respuesta psicofisiológica fue significativamente menor cuando se presentaron estímulos alegres que cuando se presentaron los estímulos neutros (diferencia = -18, p = .042) "],["prueba-de-friedman-con-el-programa-spss.html", "3.11 Prueba de Friedman con el programa SPSS", " 3.11 Prueba de Friedman con el programa SPSS Esta disponible en Analizar + Pruebas no parametricas + Cuadro de diálogos antiguos + K muestras relacionadas Figura 3.19: Prueba de Friedman en SPSS Se introducen las variables en el cuadro central. Por defecto aparece marcada la opción de la prueba de Friedman. En el botón de Exacta podemos decidir si la significación se calcula mediante aproximación asintótica, de manera exacta o mediante simulación de montecarlo. El defecto es mediante aproximación asintótica. Figura 3.20: Prueba de Friedman en SPSS: Variables Los resultados indican que hubo diferencias significativas. Si aparecen diferencias significativas hay que realizar pruebas a posteriori mediante la prueba de Wilcoxon para dos muestras si la variable diferencia no sigue una ley normal o la prueba T para una muestra si las diferencias siguen una ley normal. En la figura 14 los resultados de la prueba de Friedman se incluyen, los rangos medios de cada una de las medidas, el número de casos válidos (N = 4), el valor del estadístico Friedman (Chi-cuadrado = 8), sus grados de libertad (gl = 2), y su significación asintótica (0.018) Figura 3.21: Prueba de Friedman en SPSS 3.11.1 Conclusiones Se encontró que la respuesta psicofisiológica varió en función del estímulo presentado (\\(\\chi^{2}\\)(2,N =4)= 8, p = 0.018). Los contrastes a posteriori no mostraron esas diferencias significativas una vez aplicada la corrección de Bonferroni. "],["diseños-factoriales-i-diseño-factorial-completamente-aleatorizado-a2fca.html", "4 Diseños factoriales (I): Diseño factorial completamente aleatorizado (A2FCA)", " 4 Diseños factoriales (I): Diseño factorial completamente aleatorizado (A2FCA) En este tema vamos a estudiar los diseños factoriales. A diferencia de los diseños anteriores donde estudiábamos el efecto de una VI categórica (factor) sobre una VD cuantitativa, aquí estudiaremos el efecto de 2 o más variables categóricas sobre una VD. En este primer tema nos centraremos en los diseños factoriales que utilizan una estrategia entresujetos en la que se crean grupos a los que se les asigna una condición del diseño experimental. La principal ventaja de estos diseños es que nos permite conocer el efecto conjunto de los dos factores ( interacción). Desde el punto de vista formal, el concepto de interacción entre dos factores hace referencia a que el efecto de uno de ellos sobre la VD no es el mismo en todos los niveles del otro factor. Para facilitar la comprensión de este tipo de diseños nos centraremos en los diseños con dos factores con dos variables A y B con 2 y 3 valores respectivamente,(diseño factorial 2x3). Por ejemplo, supongamos que queremos estudiar el efecto de la dieta (factor A, \\(a_1\\) = Dieta A, \\(a_2\\) = Dieta B) y las horas de sueño (factor B, \\(b_1\\) = 2 horas, \\(b_2\\) = 4 horas, \\(b_3\\) = 6 horas) en la actividad motora de una grupo de ratas. Las condiciones serían las siguientes: Figura 4.1: Condiciones de un diseño factorial 2x3 Mediante este diseño podemos conocer como afecta la dieta a la actividad motora (\\(a_{1+}\\) - \\(a_{2+}\\)). Asimismo, este diseño también nos permite conocer el efecto de las horas de sueño sobre la actividad motora comparando los distintos niveles de la variable B (por ejemplo, comparando \\(b_{1+}\\) - \\(b_{2+}\\)). Sin embargo, la principal ventaja es que también podemos conocer cómo afecta la dieta dependiendo del nivel que está presente de B (efecto de interacción). Esto se puede hacer por ejemplo comparando \\(a_1b_1\\) - \\(a_2b_1\\). En este caso, tendríamos la comparación que estudia el efecto de la dieta sobre la actividad motora cuando los animales sólo habían dormido dos horas. De la misma forma, podría conocerse el efecto de la dieta cuando los animales durmieron 4 o 6 horas. "],["modelo-estadístico.html", "4.1 Modelo estadístico", " 4.1 Modelo estadístico El modelo que se pretende contrastar es el siguiente: \\[ y_{ijk} =\\mu+\\alpha_i+\\beta_j+ +(\\alpha\\beta)_{jk}+\\epsilon_{ijk} \\left\\{\\begin{array}{ccrccr} i&amp;=&amp; 1,&amp;2,&amp;\\dots,&amp;a \\\\ j &amp;=&amp;1,&amp;2,&amp;\\dots,&amp;b \\\\ k &amp;=&amp;1,&amp;2,&amp;\\dots,&amp;n \\\\ \\end{array}\\right. \\] donde \\(y_{ijk}\\) es la medida del sujeto i en la condición jk, \\(\\mu\\) es una constante (la media global de todas las medidas), \\(\\alpha_i\\) es el efecto del tratamiento \\(a_i\\), \\(\\beta_j\\) es el efecto del tratamiento \\(b_j\\), \\((\\alpha\\beta)_{jk}\\) es el efecto de la combinación de tratamientos \\(a_jb_k\\), y \\(\\epsilon_{ijk}\\) es el término del error. Por tanto, en este diseño interesa estudiar dos efectos principales (efecto de A y efecto de B) y un efecto de interacción. Los efectos principales se definen de la siguiente forma: \\[ \\alpha_j = \\mu_{j+} - \\mu_{++} \\] \\[ \\beta_k = \\mu_{+k} - \\mu_{++} \\] donde \\(\\mu_{j+}\\) es la media marginal de la fila j, \\(\\mu_{+k}\\) es la media de la columna k y \\(\\mu_{++}\\) es la media total. En total, hay tantos coeficientes como filas (o columnas) haya en el diseño sometidos a la siguiente restricción: \\[ \\sum_{j=1}^{a} \\alpha_j = 0 \\] \\[ \\sum_{k=1}^{b} \\beta_k = 0 \\] El término de la interacción viene definido como: \\[ (\\alpha\\beta)_{jk}= \\mu_{jk} - (\\mu_{++} + \\alpha_{j} + \\beta_k) \\] O lo que es lo mismo, este efecto es la diferencia de la media de la casilla del diseño menos la media total, el efecto de la fila j y menos el efecto de la columna k. Al igual que en el caso de los efectos principales, la suma de los efectos interactivos por filas o por columnas suman cero: \\[ \\sum_{j=1}^{a} \\alpha\\beta_{jk} = 0 \\] \\[ \\sum_{k=1}^{b} \\alpha\\beta_{jk} = 0 \\] Las estimaciones mínimo cuadráticas de los coeficientes son: \\[ \\alpha_j = \\bar{Y}_{j+} - \\bar{Y}_{++} \\] \\[ \\beta_k = \\bar{Y}_{+k} - \\bar{Y}_{++} \\] \\[ (\\alpha\\beta)_{jk}= \\bar{Y}_{jk} - \\bar{Y}_{j} - \\bar{Y}_k +\\bar{Y}_{++} \\] Los efectos principales tienen (a - 1) y (b - 1) grados de libertad; la interacción tiene (a - 1) x (b - 1) grados de libertad. En este modelo se asume que existen JK poblaciones (una por cada grupo) que se distribuyen normalmente con la misma varianza. Por tanto, podemos distinguir dos tipos de variabilidad: 1) La varianza entregrupos y 2) la varianza intragupo. La única diferencia con el diseño factorial de 1 solo factor es que la varianza entregrupos puede descomponerse en el efecto de A, el de B y en el de interacción AB. La estimación de la varianza intragrupo se realiza promediando las varianzas de cada grupo. A este término se le denomina media cuadrática del error (MCE): \\[ MCE = \\bar{S}_{jk}^2 = \\sum_{j}\\sum_{k}S_{jk}^2/jk \\] La estimación de la varianza entresujetos se realiza comparando la media de cada grupo con la media total y se denomina media cuadrática entresujetos (MCI): \\[ MCI = n\\sum_{j}\\sum_{k}(\\bar{Y}_{jk} - \\bar{Y}_{++})^2/(jk-1) \\] La descomposición de esta varianza se hace para cada uno de sus componentes. Para el factor A sería: \\[ MCA = nk\\sum_{j}(\\bar{Y}_{j+} - \\bar{Y}_{++})^2/(j-1) \\] Para el factor B sería: \\[ MCB = nj\\sum_{k}(\\bar{Y}_{+k} - \\bar{Y}_{++})^2/(k-1) \\] Par el factor de la interacción será: \\[ MC_{AB} = n\\sum_{j}\\sum_{k}(\\bar{Y}_{ij} - \\bar{Y}_{j+}- \\bar{Y}_{+k} +\\bar{Y}_{++})^2/[(j-1)(k-1)] \\] "],["interacción-entre-factores.html", "4.2 Interacción entre factores", " 4.2 Interacción entre factores Conceptualmente podemos considerar que la interacción entre factores hace referencia a que el efecto de una VI sobre la VD depende del valor que está presente de la otra VI. Ahora bien, desde un punto de vista más formal el efecto interactivo se define de la siguiente forma: \\[ \\alpha\\beta_{jk}= \\mu_{jk} - (\\mu_{++} + \\alpha_{j} + \\beta_k) \\] Existe interacción cuando \\(\\alpha\\beta_{jk} \\neq\\) 0 para algún jk. Pardo &amp; San Martin (2010) consideran que esta expresión puede interpretarse de dos formas: Como desviación que experimentan las medias de las casillas con respecto a los efectos principales de los factores: No interacción: \\(\\mu_{jk} = \\mu_{j+} \\mu_{+k} - \\mu_{++}\\) (para todo j y k) Interacción: \\(\\mu_{jk} \\neq \\mu_{j+} \\mu_{+k} - \\mu_{++}\\) (para todo j y k) Como diferencias entre las medias de las casillas y las medias marginales: No interacción: \\(\\mu_{jk} - \\mu_{j&#39;k} = \\mu_{j+} - \\mu_{j&#39;+}\\) (para todo j, j’ ó k) Interacción: \\(\\mu_{jk} - \\mu_{j&#39;k} \\neq \\mu_{j+} - \\mu_{j&#39;+}\\) (para todo j, j’ ó k) Estas dos conceptualizaciones del efecto interactivo al final se traducen en el concepto anteriormente mencionado de la existencia de interacción siempre y cuando las diferencias de un factor (p. ejemplo, el factor A), son distintas dependiendo del valor que está presente del otro factor B. En la siguiente figura se presentan dos situaciones donde en la primera de ellas no existe interacción entre las variables y en la otra si. Figura 4.2: Medias de dos diseños factoriales con y sin interacción Gráficamente es muy fácil apreciar la existencia (o no) de interacción: cuando las líneas son paralelas no existe interacción. Por el contrario, si las líneas se cruzan o no son paralelas hablamos de interacción. En la figura 3 izquierda se observa con claridad que cualquier valor del factor B cambia en 2 unidades siempre, dependiendo de que esté presente el valor de \\(a_1\\) o \\(a_2\\). En cambio, en la figura de la derecha los cambios no son uniformes. Son mucho más elevados cuando el valor de B es \\(b_2\\) que cuando el valor de B es \\(b_1\\) ó \\(b_3\\). Es más, se observa que el cambio significativo en el factor A es sólo cuando está presente \\(b_2\\). Cuando están presentes los otros valores de B los cambios son muy parecidos. Figura 4.3: Representación gráfica de dos diseños factoriales sin y con interacción 4.2.1 Ejemplo 4.1 de diseño factorial Supongamos que estamos interesados en estudiar si el efecto de la dieta (\\(a_1\\) = Dieta A, \\(a_2\\) = Dieta B) sobre la actividad motora depende de las horas de sueño \\(b_1\\) = 2 horas, \\(b_2\\) = 4 horas y \\(b_3\\) = 6 horas). Los resultados de los sujetos aparecen a continuación: Figura 4.4: Datos del ejemplo 4.1 Analizando estos datos nos encontramos que existe un efecto interactivo entre la variable tipo de dieta y horas de sueño (ver tabla 4.1). Asimismo, podemos comprobar mediante el estadístico de Levene que se cumple el supuesto de homogeneidad (ver tabla 4.2). Tabla 4.1: Resultados del ejemplo 4.1 Df Sum Sq Mean Sq F value Pr(&gt;F) horas 2 37.27 18.63 5.11 0.01 Dieta 1 12.03 12.03 3.30 0.08 horas:Dieta 2 28.47 14.23 3.90 0.03 Residuals 24 87.60 3.65 NA NA Tabla 4.2: Prueba de Levene del ejemplo 4.1 Df F value Pr(&gt;F) group 5 0.14 0.98 24 NA NA En la siguiente figura observamos que el efecto de la dieta sobre la actividad motora es prácticamente inexistente cuando los animales durmieron 2 o 6 horas. Sin embargo, se produce una gran diferencia en la actividad motora con la dieta A cuando los animales durmieron 4 horas (condición \\(a_1b_2\\)). Figura 4.5: Representación gráfica del ejemplo 4.1 4.2.1.1 Cálculo de los efectos simples Una vez que se constata la existencia de interacción en el diseño, el paso siguiente consiste en interpretar la interacción mediante la comparación de los efectos simples. Un efecto simple calcula la diferencia de dos valores de la VD en presencia de dos valores distintos de la VI, manteniendo constante el valor de una tercera. Consideremos los valores medios de nuestro ejemplo 1: Tabla 4.3: Medias de las condiciones del ejemplo 4.1 \\(a_1b_1\\) \\(a_2b_1\\) \\(a_1b_2\\) \\(a_2b_2\\) \\(a_1b_3\\) \\(a_2b_3\\) Medias 6.6 11 6.6 6.4 7 7 Los efectos simples de la variable dieta serían: Comparar los valores medios de las dieta A con la dieta B cuando los animales durmieron 2 horas: \\[ Efecto \\; simple \\; de \\; A = \\overline{X}_{a_1b_1}- \\overline{X}_{a_2b_1} = 6.6 - 6.4 = 0.2 \\] Comparar los valores medios de las dieta A con la dieta B cuando los animales durmieron 4 horas: \\[ Efecto \\; simple \\; de \\; A = \\overline{X}_{a_1b_2} - \\overline{X}_{a_2b_2} = 11 - 7 = 4 \\] Comparar los valores medios de las dieta A con la dieta B cuando los animales durmieron 6 horas: \\[ Efecto \\; simple \\; de \\; A = \\overline{X}_{a_1b_3} - \\overline{X}_{a_2b_3} = 6.6 - 7 = -0.4 \\] Para determinar el efecto de la interacción necesitamos comparar entre sí todos estos efectos simples. Así, en este diseño tendremos tres comparaciones: Comparación del efecto simple de A (dieta) en \\(b_1\\) (2 horas de sueño) con el efecto simple de A en \\(b_2\\) (4 horas de sueño): \\[ \\phi_1 = \\overline{X}_{a_1b_1}- \\overline{X}_{a_2b_1} - (\\overline{X}_{a_1b_2}- \\overline{X}_{a_2b_2}) = 6.6 - 6.4 - 11 + 7 = -3.8 \\] Comparación del efecto simple de A (dieta) en \\(b_1\\) (2 horas de sueño) con el efecto simple de A en \\(b_3\\) (6 horas de sueño): \\[ \\phi_2 = \\overline{X}_{a_1b_1} - \\overline{X}_{a_2b_1} - (\\overline{X}_{a_1b_3} - \\overline{X}_{a_2b_3}) = 6.6 - 6.4 - 6.6 + 7 = 0.6 \\] Comparación del efecto simple de A (dieta) en \\(b_2\\) (4 horas de sueño) con el efecto simple de A en \\(b_3\\) (6 horas de sueño): \\[ \\phi_1 = \\overline{X}_{a_1b_1} - \\overline{X}_{a_2b_1} - (\\overline{X}_{a_1b_2}- \\overline{X}_{a_2b_2}) = 11 - 7 - 6.6 + 7 = 4.4 \\] Observamos que las diferencias se establecen cuando se comparan los efectos simples de A con el valor \\(b_2\\) (4 horas de sueño). En los otros las diferencias son pequeñas. No obstante, debemos considerar si esas diferencias encontradas cuando se comparan dos efectos simples se deben al azar. La comparación de los contraste anteriores y la varianza del error permite obtener un estadístico t que se distribuye como una distribución t teórica con 24 grados de libertad. Una estimación de la varianza del error puede hacerse promediando las varianzas de las condiciones del diseño. En nuestro caso, sería: Tabla 4.4: Desviaciones tipo de las condiciones del ejemplo 4.1 \\(s_{a_1b_1}\\) \\(s_{a_2b_1}\\) \\(s_{a_1b_2}\\) \\(s_{a_2b_2}\\) \\(s_{a_1b_3}\\) \\(s_{a_2b_3}\\) valor 1.95 1.82 2 1.87 1.95 1.87 \\[ Var_{error} = \\frac{1.95+1.82+2+1.87+1.95+1.87}{6} = 1.91 \\] Calculando el estadístico t para cada uno de los contrastes: Primer contraste: \\[ t = \\frac{\\phi_1}{Var_{error}} = \\frac{-3.8}{1.91} = 1.99 \\] Segundo contraste: \\[ t = \\frac{\\phi_2}{Var_{error}} = \\frac{0.6}{1.91} = 0.31 \\] Tercer contraste: \\[ t = \\frac{\\phi_3}{Var_{error}} = \\frac{4.4}{1.91} = 2.30 \\] El estadístico t para un nivel de confianza del 95% tiene un valor de 1.71 por lo que puede considerarse que el primer (\\(\\phi_1\\)) y el tercer contraste (\\(\\phi_3\\)) resultan significativos (F(2,24) = 3.9, p = .034, \\(\\eta_p\\) = 0.298, indicando un tamaño de efecto grande). Observando en la figura apreciamos que el efecto de la dieta con 4 horas de sueño fue significativamente distinto en comparación con el efecto de la dieta a las 2 horas (t(24) = 1.99, p =.029) y 6 horas de sueño (t(24) = 2.30, p =.015). 4.2.2 Ejemplo 4.2 de diseño factorial Supongamos que estamos interesados en estudiar si la memoria de trabajo de escolares se veía afectada por las variables edad y cantidad de zonas verdes próximas al centro educativo. Para realizar el análisis de los datos debemos asegurarnos de que se cumplen los supuestos de la prueba (normalidad y homogeneidad de las varianzas). En la siguiente figura se representan los datos por condición del estudio. Figura 4.6: Boxplot del ejemplo 4.2 Realizando el análisis de la varianza observamos que la interacción no resulta significativa, siendo la edad el único factor significativo. Por tanto, en este diseño lo único que hay que hacer es determinar si existen diferencias significativas entre las distintas edades. Para ello hay que realizar algún procedimiento post hoc como el de Tukey. Figura 4.7: Resultados del ejemplo 4.2 Figura 4.8: Prueba de Tuckey para las comparaciones de edad del ejemplo 4.2 Los resultados indican que sólo hubo un efecto significativo de la edad F(2,24) = 9.04, p = 0.001, \\(\\eta^2\\) parcial = 0.43 indicando un tamaño del efecto grande). Las comparaciones a posteriori de Tukey indicaron que hubo diferencias significativas en la memoria de trabajo a los 6 años (M = 11.40) comparado con los 7 años de edad (M = 16.60, p &lt; .001). "],["a2fc-con-el-programa-jamovi.html", "4.3 A2FC con el programa JAMOVI", " 4.3 A2FC con el programa JAMOVI Para analizar este tipo de diseños JAMOVI dispone de la opción “Modelo lineal general” dentro del módulo de “Linear Models”. Figura 4.9: Modelo lineal generalizado en JAMOVI Una vez introducidos los datos el programa nos indica el resultado de la interacción. Observamos que la diferencia entre las medias de las dietas fue distinta a las 2 que a las 4 horas. Figura 4.10: Resultado de la interacción del ejemplo 4.1 Figura 4.11: Resultado de la interacción del ejemplo 4.1 En la siguiente figura se aprecia que las diferencias entre las dietas aparecieron cuando a los sujetos se les permitió dormir 4 horas. El contraste de los efectos simples lo confirman. Figura 4.12: Representación gráfica del ejemplo 4.1 Figura 4.13: Comparación de los efectos simples del ejemplo 4.1 "],["a2fc-con-el-programa-spss.html", "4.4 A2FC con el programa SPSS", " 4.4 A2FC con el programa SPSS El análisis de los modelos factoriales con el programa SPSS se hace a partir de la opción de los modelos lineales generalizados (Analizar + Modelo lineal general + Univariante). Figura 4.14: Modelo lineal generalizado en SPSS Cuando se utiliza este procedimiento aparece una ventana en la que se introduce la VD y las VVII. El procedimiento es semejante al estudiado en el tema 2. La diferencia es que aquí introducimos dos o más variables en el cuadro de factores fijos. Asimismo, aparecen una serie de botones que permiten realizar las mismas comparaciones a posteriori que las realizadas en el tema 2. El botón de gráficos permite representar las medias de las distintas condiciones. Con el botón de guardar podemos almacenar las puntuaciones predichas, los errores, valores para el diagnóstico de los supuestos de la prueba. Por último, en el botón de opciones pueden obtenerse resúmenes estadísticos de las variables, pruebas de homogeneidad, estimaciones del tamaño del efecto y de la potencia entre otras posibilidades. Figura 4.15: Ventana para introducir las variables Los resultados obtenidos permiten contrastar la hipótesis del diseño (en este caso corresponden a los del ejemplo 1): Figura 4.16: Resultados del ANOVA factorial En el caso de que la interacción sea significativa es conveniente crear una nueva variable (denominada ab en la siguiente figura), para estudiar el efecto de la interacción: Figura 4.17: Creación de la variable ab Una vez creada la variable utilizamos el procedimiento de comparar medias estudiado en el tema 2 (Analizar + Comparar medias + ANOVA de 1 factor: Figura 4.18: Ventana para seleccionar la opción de modelo lineal El paso siguente consiste en introducir la variable dependiente de nuestro diseño (vd en el ejemplo 4.1) en el cuadro denominado Lista de dependientes y la variable ab. En esta ventana pulsaremos el botón de Contrastes para introducir las comparaciones de los efectos simples. Figura 4.19: Introducción de las variables Para introducir los coeficientes de los contrastes es necesario seguir el orden de las condiciones que aparecen en el fichero de datos. En nuestro caso, el orden es el siguiente para realizar la primera comparación (\\(\\phi_1\\)): \\(\\mu_{a_1b_1}\\) \\(\\mu_{a_2b_1}\\) \\(\\mu_{a_1b_2}\\) \\(\\mu_{a_2b_2}\\) \\(\\mu_{a_1b_3}\\) \\(\\mu_{a_2b_3}\\) 1 -1 -1 1 0 0 Figura 4.20: Coeficientes para el primer contraste Los resultados para este contraste aparecen en la siguiente figura: Figura 4.21: Coeficientes y resultados para el primer contraste "],["diseños-factoriales-ii-diseño-factorial-con-medidas-repetidas-a2fmr-y-a2fmx.html", "5 Diseños factoriales (II): Diseño factorial con medidas repetidas (A2FMR y A2FMX)", " 5 Diseños factoriales (II): Diseño factorial con medidas repetidas (A2FMR y A2FMX) En este tema vamos a estudiar los diseños factoriales con al menos un factor con medidas repetidas. Consideramos sólo los diseños balanceados (todas las condiciones del diseño tienen el mismo número de observaciones y/o sujetos). Asimismo, consideraremos sólo aquellos casos en los que todos los factores son fijos salvo los sujetos. Dos son los diseños que se estudiaran: 1) Diseño factorial axb de medidas repetidas donde se utiliza la estrategia longitudinal para estudiar ambas variables, y 2) diseño factorial mixto axb donde una variable se estudia mediante la estrategia intrasujeto y otra mediante la estrategia entresujetos. La principal ventaja de este tipo de diseños es que se necesitan un número menor de sujetos para realizar la investigación si lo comparamos con un diseño completamente aleatorizado. Asimismo, permite conocer la variabilidad debida a los sujetos por lo que puede eliminarse este factor. Su principal inconveniente es la presencia de VVEE relacionadas con el hecho de aplicar varias condiciones a un mismo sujeto. En este tipo de diseños interesa conocer tres fuentes de variación (debidas a los tratamientos A, B y el de la interacción AB). La determinación de la significación de estas fuentes de variación se realiza comparando la correspondiente fuente de variación con la del error. En los diseños de medidas repetidas también hay otra fuente de variación conocida que es la de los sujetos (S). Por lo general, se asume que las interacciones entre esta fuente de variación de los sujetos y la de los tratamientos deben ser nulas. Por tanto, las interacciones SXA, SxB y SxAB son componentes del error del diseño y cada efecto debe ser comparado con su correspondiente término de error para el cálculo del estadístico F. "],["diseño-factorial-intrasujeto.html", "5.1 Diseño factorial intrasujeto", " 5.1 Diseño factorial intrasujeto Presentamos primero el caso en el que los dos factores son estudiados mediante la estrategia del sujeto como propio control (medidas repetidas). Su modelo estadístico es: \\[ Y_{ijk} =\\mu+\\alpha_i+\\beta_j+ \\pi_k + (\\alpha\\beta)_{jk}+ (\\alpha\\pi)_{ji}+ (\\beta\\pi)_{ki}+(\\alpha\\beta\\pi)_{jki}+\\epsilon_{ijk} \\left\\{\\begin{array}{ccrccr} i&amp;=&amp; 1,&amp;2,&amp;\\dots,&amp;n \\\\ j &amp;=&amp;1,&amp;2,&amp;\\dots,&amp;a \\\\ k &amp;=&amp;1,&amp;2,&amp;\\dots,&amp;b \\\\ \\end{array}\\right. \\] donde \\(Y_{ijk}\\) es la medida de la VD del sujeto i en la condición jk; \\(\\mu\\) es una constante (la media global de todas las puntuaciones); \\(\\alpha_j\\) es el efecto del tratamiento \\(a_j\\); \\(\\beta_k\\) es el efecto del tratamiento \\(b_k\\); \\(\\pi_i\\) es el efecto asociado al sujeto i; \\((\\alpha\\beta)_{jk}\\) es el efecto de la combinación de tratamientos \\(a_jb_k\\); \\((\\alpha\\pi)_{ji}\\) es el efecto de la interacción entre el tratamiento j del factor A y el sujeto i; \\((\\beta\\pi)_{ji}\\) es el efecto de la interacción entre el tratamiento k del factor B y el sujeto i; \\((\\alpha\\beta\\pi)_{jki}\\) es el efecto de la interacción de tercer orden del nivel j del factor A, el k-ésimo nivel del factor B y el i-ésimo sujeto. Por último, \\(\\epsilon_{ijk}\\) es el término del error. Al igual que en otros modelos de ANOVA factoriales, el investigador está interesado en estudiar los efectos principales y de interacción. Cada uno de estos efectos se realiza mediante la prueba F donde se comparan las medias cuadráticas de cada uno de los efectos con sus correspondientes medias cuadráticas del error. 5.1.1 Ejemplo 1: Diseño factorial de medidas repetidas Un ejemplo de este tipo de diseños propuesto por Maxwell &amp; Delaney (2004) en el que se deseaba estudiar el efecto de la presencia de distractores (ruido o sin ruido) y la orientación del estímulo (0 , 4 y 8 grados) en una prueba de percepción donde se pretende discriminar entre dos estímulos. La VD fue el tiempo de reacción. Los resultados aparecen en la siguiente tabla: Figura 5.1: Modelo AF2MR: Boxplot del ejemplo 5.1 Figura 5.2: Modelo AF2MR: Resultados del ejemplo 5.1 Estos resultados son válidos siempre y cuando se cumpla el supuesto de esfericidad estudiado en el tema 3. Una vez comprobado este supuesto con la prueba de Mauchly pueden tomarse decisiones acerca de la significación de los efectos. En este ejemplo la prueba de esfericidad nos permite aceptar la hipótesis nula. Asimismo, se puede observar que los valores de \\(\\epsilon\\) de Geisser-Greenhouse (GG) son más pequeños que los de Huynh-Feldt (HF). Utilizar los valores de GG para realizar los contrastes de hipótesis supone aplicar una estrategia conservadora en el análisis de los datos. Figura 5.3: Modelo A2FMR: Prueba de esfericidad del ejemplo 5.1 5.1.1.1 Tamaño del efecto Aunque puede utilizarse el estadístico \\(\\eta^2\\) para determinar el tamaño del efecto, los autores prefieren estimar este estadístico mediante \\(\\omega^2\\). En este tipo de diseños la fórmula para calcularlo es: Para el efecto A (ruido): \\[ \\omega_{A}^2 = \\frac{(a-1)(F_A - 1)}{(a-1)(F_A - 1)+ nab} = \\frac{1*(33.766 - 1)}{1*(33.766 - 1) + 10*2*3} = 0.353 \\] Para el efecto B: \\[ \\omega_{B}^2 = \\frac{(b-1)(F_B - 1)}{(b-1)(F_B - 1)+ nab} = \\frac{(3-1)*(40.719 - 1)}{(3-1)*(40.719 - 1) + 10*2*3} = 0.569 \\] Para el efecto AB: \\[ \\omega_{AB}^2 = \\frac{(a-1)(b-1)(F_{AB} - 1)}{(a-1)(b-1)(F_{AB} - 1)+ nab} = \\frac{(3-1)*(45.310 - 1)}{(3-1)*(45.310 - 1) + 10*2*3} = 0.596 \\] Si hubiéramos utilizado el estadístico \\(\\eta^2\\) proporcionado por el programa SPSS obtendríamos los siguientes valores: 0.79 para el factor A, 0.819 para el factor B y 0.834 para el factor AB. Tal y como señalan Pardo &amp; San Martin (2010) estos valores están bastante inflados y no son un buen estimador del tamaño del efecto. Otra medida de tamaño del efecto equivalente al de \\(\\omega^{2}\\) es la de eta generalizada al cuadrado (\\(\\eta_g^{2}\\)) que utiliza el programa JAMOVI (The Jamovi Team, 2022). 5.1.1.2 Estudio de la interacción Al igual que se mencionó en el tema 4, para estudiar los efectos interactivos lo que tenemos que hacer es comparar los efectos simples. En este diseño tenemos 3 efectos simples por lo que habrá que realizar tres comparaciones. La primera comparación sería la siguiente: \\[ \\phi_1 = \\overline{X}_{ausente.a0} - \\overline{X}_{presente.a0} - (\\overline{X}_{ausente.a4} - (\\overline{X}_{presente.a4}) = \\overline{X}_{a_1b_1} - \\overline{X}_{a_2b_1} - (\\overline{X}_{a_1b_2} - \\overline{X}_{a_2b_2}) \\] Para introducir los coeficientes de los contrastes es necesario seguir el orden de las condiciones que aparecen en el fichero de datos. La forma más sencilla de realizar este contraste es creando una nueva variable con la suma de las puntuaciones de las medias que se comparan. En este caso, estamos realizando lo siguiente: Figura 5.4: Modelo A2FMR en JAMOVI: Comparación efectos simples Esta nueva variable creada (Contraste 1) supone una combinación lineal (suma de varias puntuaciones correspondientes a 4 variables). Bajo el modelo de hipótesis nula la suma de esta variable debe ser igual a 0. Para contrastar esta hipótesis podemos utiliazar la prueba T para una muestra (o la prueba de Wilcoxon en el caso de que no se cumpla el supuesto de normalidad). Los resultados de este análisis nos indican que rechazamos la hipótesis nula. Es decir existe un efecto interactivo significativo cuando se comparan los efectos simples de A en \\(b_1\\) y \\(b_2\\). Figura 5.5: Modelo A2FMR en JAMOVI: Resultado de la comparación del efecto de A en \\(b_1\\) y \\(b_2\\) Observamos que el efecto resulta ser significativo de tipo positivo por lo que el efecto de A es distinto en \\(b_1\\) que en \\(b_2\\). En concreto, en \\(b_1\\) el efecto de A vale -30. En cambio, en \\(b_2\\) el efecto de A vale -150. Esto se aprecia muy bien en la gráfica: Figura 5.6: Gráfico de medias del ejemplo 5.1 La segunda comparación entre los efectos simples sería la comparación del efecto de A cuando están presentes \\(b_1\\) y \\(b_3\\) \\[ \\phi_2 = \\overline{X}_{ausente.a0} - \\overline{X}_{presente.a0} -(\\overline{X}_{ausente.a8} - \\overline{X}_{presente.a8}) \\] Creamos una nueva variable que denominamos Contraste 2 de la siguiente forma: Figura 5.7: Modelo A2FMR en JAMOVI: Comparación de efectos simples Los resultados obtenidos con este segundo contraste se muestran en la siguiente figura: Figura 5.8: Modelo A2FMR en JAMOVI: Comparación de efectos simples Encontramos de nuevo que el efecto de A en \\(b_1\\) es mucho menor que cuando está presente \\(b_3\\). El efecto de A en \\(b_1\\) vale -30. En cambio, cuando está presente \\(b_3\\) el valor es -234. Para terminar nos queda ver si existen diferencias en el efecto de A cuando está presente \\(b_2\\) en comparación con la presencia de \\(b_3\\). Sabemos que cuando está presente \\(b_2\\) el efecto de A vale -150, mientras que cuando está presente \\(b_3\\) vale -234. Para determinar la significación de esta comparación el procedimiento es el mismo que en los casos anteriores. La comparación sería la siguiente: \\[ \\phi_3 = \\overline{X}_{ausente.a4} - \\overline{X}_{presente.a4} - (\\overline{X}_{ausente.a8} - (\\overline{X}_{presente.a8}) = \\overline{X}_{a_1b_2} - \\overline{X}_{a_2b_2} - (\\overline{X}_{a_1b_3} - \\overline{X}_{a_2b_3}) \\] Los resultados de este tercer contraste sería: Figura 5.9: Modelo A2FMR en JAMOVI: Comparación de efectos simples Figura 5.10: Modelo A2FMR en JAMOVI: Comparación de efectos simples Estos resultados indican que también existen diferencias en la comparación de los efectos simples. Conclusiones según normas APA: Se encontró la existencia de interacción entre las variables y el (F(2,18) = 45.31, p&lt; .001, \\(\\eta_p^{2}\\) = 0.67, indicando un TE grande). Se encontró que las diferencias en las medidas de presencia - ausencia con ángulo de 0 grados fueron menores cuando el ángulo fue de 4 grados (t(9)= 6.7, p &lt; .001). También la diferencia ausencia - presencia fue mayor a los 8 grados que a los 0 (t(9)= 9.1, p &lt; .001). Asimismo, el efecto de la presencia del ruido fue mayor a los 8 grados que a los 4 (t(9)= 3.5, p = .007). "],["diseño-factorial-mixto-a2fmx.html", "5.2 Diseño factorial mixto (A2FMX)", " 5.2 Diseño factorial mixto (A2FMX) Hay ocasiones en las que interesa al investigador conocer si el efecto de una VI que se ha estudiado mediante medidas repetidas cambia en función de alguna VI de agrupamiento (por ejemplo, diferencias entre hombres y mujeres o diferencias entre grupos de pacientes distintos). A este tipo de diseños se les denomina diseños factoriales mixtos, ya que combinan la estrategia longitudinal con la transversal. El caso más sencillo de estos diseños es el que combina una VI estudiada mediante la estrategia longitudinal y otra VI que se estudia mediante la estrategia transversal. 5.2.1 Ejemplo 2: Diseño factorial mixto Ilustramos este tipo de diseños con un estudio en el que se realizó un seguimiento de un grupo de pacientes lesionados en distintos momentos del tratamiento. La VI que se estudio de manera longitudinal es el paso del tiempo (inicio, mitad y final); la VI que agrupa a los sujetos es la localización de la lesión (hemisferio izquierdo versus hemisferio derecho). La VD fueron los resultados en una prueba de memoria. En la siguiente tabla se presentan los descriptivos para cada una de las condiciones: Figura 5.11: Descriptivos del ejemplo 5.2 Una representación gráfica de las medias por condición aparece en la siguiente figura. Figura 5.12: Gráfica del ejemplo 5.2 En la figura se aprecia que las diferencias en la prueba entre los dos grupos es pequeña en el momento final. También se aprecia que esas diferencias fueron máximas en la mitad de la intervención. Los resultados de este ejemplo indican la existencia de una interacción significativa: Figura 5.13: Modelo A2FMR en JAMOVI: Resultados del ejemplo 5.2 Para considerar como válidos estos resultados es necesario que se cumplan los supuestos del modelo (normalidad, homogeneidad de varianzas-covarianzas y esfericidad). En relación con el supuesto de esfericidad la prueba de Mauchly confirma su existencia: Figura 5.14: Modelo A2FMR en JAMOVI: Prueba de esfericidad del ejemplo 5.2 5.2.1.1 Medidas del tamaño del efecto El estadístico \\(\\eta^2\\) parcial nos permite obtener una estimación del tamaño del efecto aunque ya se ha mencionado que no es un buen estimador siendo preferible utilizar el estadístico \\(\\omega^2\\) o \\(\\eta_g^{2}\\). Para este diseño la fórmula es: Para el efecto de A: \\[ \\omega_{A}^2 = \\frac{gl_A(F_A - 1)}{gl_A(F_A - 1)+ nab} \\] Para el efecto B: \\[ \\omega_{B}^2 = \\frac{gl_B(F_B - 1)}{gl_B(F_B - 1)+ nab} \\] Para el efecto AB: \\[ \\omega_{AB}^2 = \\frac{gl_{AB}(F_{AB} - 1)}{gl_{AB}(F_{AB} - 1)+ nab} \\] 5.2.1.2 Comparaciones múltiples En el caso de que no resulte signficativo el efecto de la interacción AB y exista algún efecto principal (A y/o B) significativo habrá que continuar realizando las comparaciones a posteriori tal y como se indicó en los temas anteriores. Si el efecto interactivo AB resulta significativo será necesario realizar las comparaciones de los efectos simples. Para ello se calculan las diferencias entre las distintas medidas repetidas. Esas diferencias se comparan entre los distintos grupos tal y como se hizo en el tema 1 para la comparación de muestras independientes. En nuestro caso los resultados aparecen en la siguiente tabla: Figura 5.15: Modelo A2FMR en JAMOVI: Comparación de los efectos simples Encontramos que las comparaciones inicio - final y mitad-menos final fueron significativas. muestran diferencias significativas. Conclusiones según normas APA: Se encontró la existencia de interacción entre las variables tipo de lesión* y el tiempo (F(2,14) = 12.49, p&lt; .001, \\(\\eta_p^{2}\\) = 0.67, indicando un TE grande). Se encontró que las diferencias en las medidas inicio y final fueron mayores en los pacientes con lesión en el hemisferio derecho (t(8)= 3.92, p = .004). También la diferencia entre la mitad y final del estudio fue mayor en los lesionados en el hemisferio derecho (t(8)= 4.67, p = .002)}.* "],["análisis-de-los-ejemplos-con-spss.html", "5.3 Análisis de los ejemplos con SPSS", " 5.3 Análisis de los ejemplos con SPSS 5.3.1 Análisis del ejemplo 5.1 con SPSS Para analizar estos datos mediante el programa SPSS es necesario utilizar la opción Analizar + Modelo lineal general + Medidas repetidas tal y como aparece en la siguiente figura: Figura 5.16: Selección del modelo A2FMR en SPSS Al igual que en el tema 3 necesitamos introducir los datos. En este caso debemos indicar que son dos los factores que existen en el diseño uno con dos valores (factor A) y otro con 3 (factor B). Hay que asignar un nombre a cada variable e indicar el número de niveles: Figura 5.17: Definición de las variables en el modelo A2FMR en SPSS La prueba de esfericidad nos indica la idoneidad de estos resultados. Podemos observar que los valores de \\(\\epsilon\\) de Geisser-Greenhouse (GG) son más pequeños que los de Huynh-Feldt (HF). Utilizar los valores de GG para realizar los contrastes de hipótesis supone aplicar una estrategia conservadora en el análisis de los datos. Figura 5.18: Modelo A2FMR: Prueba de esfericidad En la siguiente figura aparece la tabla ANOVA con los resultados obtenidos en el estudio: Figura 5.19: Modelo A2FMR en SPSS: Tabla ANOVA 5.3.1.1 Estudio de la interacción Creamos las nuevas variables que denominamos “Contraste”. Para el *Contraste1” sería: Figura 5.20: Modelo A2FMR en SPSS: Comparación efectos simples Analizamos esta variable mediante el procedimiento de “Prueba T para una muestra”: Figura 5.21: Modelo A2FMR en SPSS: Comparación efectos simples: Contraste 1 Creamos una nueva variable que denominamos Contraste 2 de la siguiente forma: Figura 5.22: Modelo A2FMR en SPSS: Comparación de efectos simples Los resultados obtenidos con este segundo contraste se muestran en la siguiente figura: Figura 5.23: Modelo A2FMR en SPSS: Comparación de efectos simples El tercer contraste es la diferencia en el efecto de A cuando está presente \\(b_2\\) en comparación con la presencia de \\(b_3\\). Para determinar la significación de esta comparación el procedimiento es el mismo que en los casos anteriores. Los resultados de este tercer contraste serían: Figura 5.24: Modelo A2FMR en SPSS: Comparación de efectos simples 5.3.1.2 Efectos simples con el programa SPSS El programa SPSS presenta una tabla donde aparecen las comparaciones entre los distintos efectos simples: Figura 5.25: Modelo A2FMR en SPSS: Comparación de efectos simples En la tabla se comparan los efectos simples de A en los niveles 2 y 3 de B con respecto al nivel 1 de B. Ambos resultan ser significativos. Sólo falta la comparación del nivel 3 con el 2. Este se consigue en la ventana de contrastes marcando en vez de la primera categoría, la última: Figura 5.26: Modelo A2FMR en SPSS: Comparación de efectos simples Figura 5.27: Modelo A2FMR en SPSS: Comparación de efectos simples Vemos que también los resultados son significativos para esta tercera comparación. Asimismo, puede comprobarse que el valor de los estadísticos es igual, ya que el estadístico F es equivalente al estadístico t elevado al cuadrado. Así, en el contraste del nivel 2 con el nivel 3 el estadístico F vale 12,25 y el valor de \\(t^2\\) = \\(3.5^2\\) = 12.25. 5.3.1.3 Estudio de los efectos principales En el caso de que el efecto de la interacción no sea significativo este diseño también permite estudiar los efectos principales. Para ello, se aplica el mismo procedimiento que el utilizado cuando se estudiaron los diseños de medidas repetidas con 1 factor. Así, compararemos las diferencias entre las medias (varianza del tratamiento A o B) con su correspondiente término de error (interacción SxA en el caso de la VI A e interacción SxB en el caso de la VI B). Si estos contrastes resultan significativos habrá que continuar para determinar la significación entre las distintas condiciones. Esto puede hacerse con el procedimiento estudiado anteriormente en el programa SPSS (opción de Contrastes + Comparaciones simples), en el caso de que se cumpla el supuesto de la esfericidad. Si no se cumple este supuestos, pueden realizarse comparaciones dos a dos corrigiendo el nivel de riesgo con el criterio de Bonferroni. Figura 5.28: Modelo A2FMR en SPSS: Comparación de efectos simples 5.3.2 Análisis del ejemplo 5.2 con SPSS Un resumen de las distintas condiciones en el diseños son las siguientes: Figura 5.29: Descriptivos Para realizar el análisis de este tipo de diseños mediante el programa SPSS se utiliza el mismo procedimiento que en los diseños anteriores: Analizar + Modelo lineal general + Medidas repetidas. La única diferencia es que en este caso al introducir las variables es necesario incluir el factor entresujetos tal y como aparece en la siguiente figura: Figura 5.30: Modelo A2FMR en SPSS: Datos del ejemplo 5.2 Los resultados de este ejemplo indican la existencia de una interacción significativa: Figura 5.31: Modelo A2FMR en SPSS: Resultados del ejemplo 2 Para considerar como válidos estos resultados es necesario que se cumplan los supuestos del modelo (normalidad, homogeneidad de varianzas-covarianzas y esfericidad). En relación con el supuesto de esfericidad la prueba de Mauchly confirma su existencia: Figura 5.32: Modelo A2FMR en SPSS: Prueba de esfericidad del ejemplo 2 La prueba de Box-Cox nos permite contrastar el supuesto de homogeneidad de varianzas-covarianzas. Esta prueba se obtiene dentro de la ventana de opciones marcando la opción de pruebas de homogeneidad. Observamos que p = 0.05 por lo que aceptamos la hipótesis de que las matrices de varianzas y covarianzas del grupo con lesión en hemisferio izquierdo es semejante a la del hemisferio izquierdo. Figura 5.33: Modelo A2FMR en SPSS: Prueba de boxcox del ejemplo 5.2 Encontramos que la comparación entre el efecto simple del nivel 1 frente al 3 muestran diferencias significativas. Para encontrar la comparación entre el nivel 2 y el nivel 3 solo basta cambiar la categoría de referencia (hay que poner la última) en la opción de contrastes. También resulta significativa. Figura 5.34: Modelo A2FMR en SPSS: Comparación de los efectos simples "],["modelos-de-regresión-lineal.html", "6 Modelos de regresión lineal", " 6 Modelos de regresión lineal El objetivo de este tema es introducir los conceptos básicos de la técnica estadística denominada . Esta técnica pretende establecer relaciones entre una variable dependiente cuantitativa y una variable predictora () o varias variables predictoras (). Aunque el objetivo principal es estudiar relaciones entre variables cuantitativas, el modelo de regresión lineal también puede utilizarse con variables cualitativas. Asimismo, realizaremos algunas incursiones dentro del análisis de “vias” ( en el que se utilizan varias modelos de regresión para el estudio de relaciones moderadas y mediadas. El análisis de la regresión está estrechamente relacionado con el análisis de la correlación. La diferencia entre ambos depende de los objetivos del investigador. En el primer caso, se pretende establecer algún tipo de predicción, mientras que en el segundo se pretende medir el grado de relación entre dos variables. Con objeto de aplicar los conceptos aquí expuestos sobre una tema concreto presentamos los resultados de una investigación sobre envejecimiento. Este estudio versa sobre el deterioro cognitivo de las personas mayores. En el fichero reserva.sav se presentan los resultados de una investigación en la que se estudió una muestra de personas mayores a las que se les midió las siguientes variables: edad, sexo, tiempo en TMT-B en el momento 1, puntuación total en el test de denominación de Boston, años de escolarización, puntuación en el test de Yesavage para medir depresión, tiempo en la prueba del TMT-B en un segundo momento, puntuación total en el test de Boston en un segundo momento, la puntuación en la prueba de actividades estimulantes de la cognición (AEC) y la diferencia entre las dos puntuaciones del Boston. Los primeros 6 sujetos de las primeras 8 variables se presentan a continuación: Tabla 6.1: Primeros sujetos del fichero de datos edad sexo TMTB1 TMTB2 BOSTON1 BOSTON2 AEC edu 81 mujer 556 999 46 32 22 8 81 mujer 370 388 50 50 34 8 71 hombre 117 101 55 55 38 10 64 mujer 204 243 46 47 48 2 75 mujer 130 150 48 45 23 20 63 mujer 146 999 54 53 23 6 "],["correlación.html", "6.1 Correlación", " 6.1 Correlación En la figura siguiente puede apreciarse visualmente que existe una relación entre la variable edad y la puntuación obtenida en el test de Boston. Se aprecia con cierta claridad que a medida que aumenta la edad disminuye la puntuación obtenida por las personas mayores en el test. Figura 6.1: Relación entre la edad y BOSTON 1 Por lo general, en la investigación científica se desea cuantificar las relaciones entre variables, siendo el coeficiente de correlación de Pearson una de las pruebas más utilizadas. Se define de la siguiente forma: \\[ r_{xy} \\approx \\frac{\\sum_{j=1}^{n}Z_{X_{j}}Z_{Y_{j}}}{n} \\] donde \\(Z_{X_{j}}\\) y \\(Z_{Y_{j}}\\) son las medidas del sujeto en las variables X e Y expresadas como desviaciones típicas con respecto a su media muestral: \\[ Z_{X_{j}}= \\frac{X_{j}-\\bar{X}}{SD_{X}} \\hspace{10mm} Z_{Y_{j}}= \\frac{Y_{j}-\\bar{Y}}{SD_{Y}} \\] donde \\(SD_{X}\\) y \\(SD_{Y}\\) son la desviación tipo de las variables X e Y respectivamente; \\(\\bar{X}\\) y \\(\\bar{Y}\\) son las medias muestrales. El coeficiente de correlación (\\(r_{xy}\\)) oscila entre -1 y 1, siendo su valor cero cuando no existe relación entre las variables. Si la relación es , \\(r_{xy}&gt; 0\\). Por el contrario, si la relación es \\(r_{xy}&lt;0\\). Si elevamos al cuadrado \\(r_{xy}\\) obtenemos el . Nos indica el grado de varianza compartida entre las dos variables. En el caso de la relación entre la variable edad y primera medida del test de Boston el valor sería 0.2158. Es decir, ambas variables comparten el 22% de varianza. El coeficiente de correlación nos permite conocer el valor de la variable Y a partir de los valores de X. Es decir, si conocemos que una persona está 0,5 desviaciones típicas en edad (es mayor que la media), podremos predecir si esta persona se encuentra por encima o por debajo de la media en la puntuación en el Boston. Para ello, hacemos uso de la siguiente expresión: \\[ \\hat{Z}_{Y_{j}}= r_{xy}Z_{X_{j}} = -0.465*0.5 = -0,233 \\] Puede afirmarse que este sujeto se encuentra por debajo del valor medio en la puntuación del test de Boston. De hecho, su edad será: \\[ \\hat{Z}_{X_{j}}=0.5 = \\frac{X_{j} -67.93}{7.319}= 0.5*7.319 + 67.93 = 71.59 \\approx 72 \\; años \\;de \\; edad \\] Por tanto, dado que la media de la puntuación en el test de Boston en la primera medición es 49,90 y su desviación tipo es 5,381 podemos hacer una primera estimación del valor que obtendría este sujeto: \\[ \\hat{Z}_{Y_{j}}=-0.233 = \\frac{Y_{j} -49,90}{5.381}= -2.33*5.381 + 49.40 = 37.346= Y_{j} \\] Este resultado supone una primera aproximación al valor real del sujeto. Podemos considerar que es el valor esperado. Sin embargo, existirán diferencias entre los distintos sujetos que presenten una misma edad. Para determinar el intervalo de confianza será necesario introducirnos en los modelos de regresión. "],["regresión-simple.html", "6.2 Regresión simple", " 6.2 Regresión simple El modelo de regresión simple es: \\[ Y_{i}= \\beta_{0} + \\beta_{1}* x_{i} + \\epsilon_{i} \\] donde \\(Y_{i}\\) es el valor de la respuesta del sujeto , \\(\\beta_{0}\\) y \\(\\beta_{1}\\) son los parámetros de la recta de regresión, \\(x_{i}\\) es el valor del sujeto en la variable predictora. Por último, \\(\\epsilon_{i}\\) es el término de error correspondiente al sujeto . Este modelo de regresión pretende relacionar dos variables de manera lineal. De hecho, la ecuación de la recta de regresión no es otra cosa que la ecuación de una recta que presenta la propiedad de ser la que menor error produce a la hora de representar las puntuaciones de los sujetos. A la variable X se le denomina variable o variable y a la variable Y se le denomina variable o . El valor del parámetro \\(\\beta_{0}\\) nos indica el punto de corte de la recta de regresión sobre el eje de ordenadas. En cambio, el valor del parametro \\(\\beta_{1}\\) nos indica la pendiente de la recta. Si este parámetro vale cero nos encontramos con la ausencia de relación entre las dos variables. Asimismo, si este parámetro es de signo negativo tenemos una relación de tipo . Por el contrario, si el parámetro es positivo tendremos una relación entre las dos variables. Utilizando la base de datos anterior vamos a estudiar las relaciones entre la edad y la puntuación en el test de Boston en la primera medición. Asimismo, queremos estudiar si están relacionadas las dos medidas del Boston. Figura 6.2: Relación entre las dos medidas del Boston Podemos observar que entre las dos variables de la figura 1 existe una relación lineal de tipo . Es decir, a medida que aumenta la edad, los sujetos tienden a obtener una menor puntuación en el test de Boston. La recta de regresión obtenida para la figura 1 es: En otras ocasiones nos encontraremos con relaciones directas como en el caso de la figura 2. Se obtiene que a medida que aumenta la puntuación en el primer momento del Boston también aumenta la puntuación obtenida en el Boston en el momento 2. La recta de regresión obtenida para la figura 2 es: En ambos casos el parámetro \\(\\beta_{1}\\) es significativo. La única diferencia es que en la relación de la figura 2 el valor es negativo (- 0.325) y en el de la figura 2 positivo (1.041). En el caso de la figura 2, la recta de regresión obtenida es: \\[ \\hat{Y}_{i} = 71.90 - 0.325 *x_{i} \\] Según estos resultados, el punto de corte de la recta con el eje de ordenadas está en 71.9 y a medida que las personas aumentan un año de edad su puntuación en el test de Boston disminuye 0.325 unidades. 6.2.1 Predictores categóricos En el modelo de regresión las variables predictoras pueden ser cuantitativas o cualitativas. Un ejemplo de variable cualitativa puede ser el género. Los resultados del modelo de regresión que relaciona el género con la puntuación en el test de Boston aparece en la siguiente tabla: Observamos que la recta de regresión obtenida es: \\[ Y_{j} = 49,10 + 2,68* sexo \\hspace{15mm} para\\; hombres \\;y \\] \\[ Y_{j} = 49,10 \\hspace{35mm} para\\; mujeres \\] Como la variable sexo se ha categorizado como 0 si la persona es mujer y 1 si es hombre el valor esperado en la prueba de Boston será mayor para los hombres que para las mujeres. O lo que es lo mismo, la “intercept” de la ecuación de regresión se corresponde con el valor medio de las mujeres en el test de Boston. Esta es la codificación recomendable para facilitar la interpretación del modelo. Sin embargo, cualquier otra codificación es posible. No obstante, Hayes (2013) recomienda usar valores cuya diferencia entre los valores sea igual a 1. De este modo, el coeficiente \\(\\beta_{1}\\) se podrá interpretar como la diferencia exitente entre las medias de los dos grupos. 6.2.2 Realización de pronósticos Una de las utilidades del modelo de regresión es la de permitir realizar predicciones. Continuando con el ejemplo 1, puede interesarnos conocer la puntuación que obtendría en el test de Boston una persona con 70 años. Utilizando la recta de regresión obtendríamos: \\[ 49.15 = 71.90 - 0.325*70 \\] Esta estimación representa el valor más probable, pero puede interesarnos realizar una estimación mediante intervalos de confianza. Pueden construirse dos tipos de intervalos dependiendo de que queramos realizar estimaciones con respecto a la media o con respecto al individuo. En el primer caso, la estimación se hace para todos los sujetos que tienen una misma puntuación. En cambio, cuando nos interesa realizar un pronóstico individual el pronóstico se interpreta como la estimación asignada a un sujeto concreto con un valor concreto en la variable predictora (Pardo &amp; San Martin, 2011). Estas estimaciones son más amplias que las de la media. La siguiente expresión nos permiten calcular el intervalo de confianza (IC) para los pronósticos individuales: \\[ S_{\\hat{Y_{i}}|X_{i}}^{2} = MCE[1+\\frac{1}{n}+(X_{i}-\\bar{X})^2/\\sum (X_{i}-\\bar{X})^2] \\] Para el pronóstico con respecto a la media será la siguiente expresión: \\[ S_{\\hat{\\mu_{i}}|X_{i}}^{2} = MCE[\\frac{1}{n}+(X_{i}-\\bar{X})^2/\\sum (X_{i}-\\bar{X})^2] \\] Aplicados a los datos de nuestro ejemplo obtenemos (47,24;51,02) para el IC de la media y (39,13;59,13) para el pronóstico individual. Queda manifiesto que los pronósticos individuales son menos exactos que los referidos a la media. "],["regresión-lineal-múltiple.html", "6.3 Regresión lineal múltiple", " 6.3 Regresión lineal múltiple La realidad supone la existencia de relaciones complejas por lo que el investigador suele estar interesado en modelos que incluyan más de dos variables. Para ello, dispone de una serie de herramientas estadísticas dentro de las cuales se incluyen las técnicas de regresión múltiple. El modelo estadístico para esta técnica se representa de la siguiente forma: \\[ Y_{i}= \\beta_{0} + \\beta_{1}* x_{1i} + \\beta_{2}* x_{2i}+ \\cdots + \\beta_{p+1}* x_{pi}+\\epsilon_{i} \\] donde \\(x_{pi}\\) son las variables predictoras y \\(\\beta_{p+1}\\) son los coeficientes de regresión. Supongamos que estamos interesados en estudiar si la puntuación en la primera medida del Boston depende de la edad y del género. En la siguiente figura aparece representada la relación entre edad y medida 1 del Boston para hombres y para mujeres: Figura 6.3: Relación entre la edad y el Boston 1 en función del género La gráfica indica que la relación entre edad y puntuación en el Boston sigue siendo negativa. Sin embargo, esta relación presenta mayor o menor intensidad dependiendo del género. En las mujeres la recta de regresión está menos inclinada indicando que la disminución en la puntuación en el Boston es bastante más atenuada que en los hombres. Realizando el análisis de regresión obtenemos los siguientes resultados: Estos resultados indican que el género tiene un efecto protector sobre el deterioro de la memoria semántica en las personas mayores. Comparando hombres y mujeres de la misma edad, las mujeres obtendrán una menor puntuación que los hombres en el test de denominación de Boston. 6.3.1 Selección de modelos Cuando se construye un modelo es deseable encontrar el mejor subconjunto de predictores que expliquen los datos de manera aceptable. Existen dos estrategias para la selección de variables (Ugarte et al., 2008): 1) mediante la estimación de todos los modelos, y 2) mediante un procedimiento de pasos sucesivos. 6.3.1.1 Estimación de todos los modelos Es un método que requiere el uso intensivo del ordenador, ya que cuando el número de variables es relativamente grande resulta difícil poder llevarlo a cabo de manera manual. Se elige el modelo que presenta el mejor criterio de ajuste. Ugarte et al. (2008) consideran como criterios válidos el estadístico \\(R^{2}\\) ajustado o el índice \\(C_{p}\\). El programa R (R Core Team, 2016) permite calcular todos los modelos posibles. Para ver este procedimiento seguiremos con el mismo ejemplo y calcularemos todos los modelos de regresión posibles para estudiar el efecto de la edad, la educación y las AEC sobre la puntuación en la primera medida del test de Boston. Los resultados obtenidos son los siguientes: ## Subset selection object ## Call: regsubsets.formula(BOSTON1 ~ edad + edu + AEC, data = dat) ## 3 Variables (and intercept) ## Forced in Forced out ## edad FALSE FALSE ## edu FALSE FALSE ## AEC FALSE FALSE ## 1 subsets of each size up to 3 ## Selection Algorithm: exhaustive ## edad edu AEC ## 1 ( 1 ) &quot;*&quot; &quot; &quot; &quot; &quot; ## 2 ( 1 ) &quot;*&quot; &quot;*&quot; &quot; &quot; ## 3 ( 1 ) &quot;*&quot; &quot;*&quot; &quot;*&quot; ## [1] 0.1775712 0.2545712 0.2489539 Se aprecia en la tabla que el modelo con mejor ajuste (\\(R^{2}\\)) es el que incluye sólo las variables edad y educación (\\(R^{2}\\) ajustado = 0.255). El modelo que incluyó sólo la edad presenta un \\(R^{2}\\) ajustado de 0.178. Es decir, la variable edad explica aproximadamente un 17,8% de la varianza de la variable puntuación en el test de Boston en la primera medición. Estos resultados contrastan con los obtenidos con el modelo que incluyó dos variables (edad y educación) cuyo valor de \\(R^{2}\\) ajustado fue mayor (0.255). Una forma de valorar esta contribución consiste en cuantificar el grado de cambio al incorporar (eliminar) una variable a la ecuación: \\[ R_{cambio|X_{j}}^{2} = R_{12 \\ldots p}^{2}-R_{12 \\ldots p-1}^{2} = 0.255-0.178= 0.077 \\] La significación estadística del cambio en \\(R^{2}\\) viene dada por la siguiente expresión: \\[ F_{cambio|X_{j}}= \\frac{R_{cambio|X_{j}}^{2}(n-p-1)}{1 - R_{12 \\ldots p}^{2}} = \\frac{0.077*(30-2-1)}{1-0.255}=2.79 \\] Comparando este resultado con el de las tablas para una F con 1 y grados de libertad y un nivel de significación del 0.05 obtenemos un valor de F(1,27) = 4.21. Por tanto, a pesar de aumentar el índice de ajuste, éste incremento no resulta ser significativo por lo que el modelo óptimo no cambia. Introducir la variable educación en el modelo no mejora significativamente el ajuste. Si calculamos la raiz cuadrada de 2.79 obtenemos el valor del estadístico t correspondiente al parámetro beta de la variable educación tal y como puede observarse en la siguiente tabla: Mediante este procedimiento sólo se incluye la variable edad, ya que es la única que cumple el criterio de significación (\\(p(F) \\leq.05)\\). El resto de variables son excluidas por no cumplir dicho criterio. El método de selección hacia atrás comienza con el modelo máximo. En cada paso, se elimina aquel predictor que cumple el criterio de eliminación (p(F)&gt;.05) y con mayor nivel de significación. Este procedimiento continua hasta que todos los predictores presentes en el modelo son significativos o no quedan predictores en el modelo. Utilizando el mismo modelo, pero cambiando la opción de Método por tal y como aparece en la siguiente figura nos encontramos que el programa comienza con el modelo máximo (las tres variables incluidas) y elimina aquellas variables que no cumplen el criterio (p(F)&lt; .1). el resultado final es el mismo que en el caso anterior: library(stats) dat1&lt;- dat[,c(1,5,7,8)] names(dat1) ## [1] &quot;edad&quot; &quot;BOSTON1&quot; &quot;AEC&quot; &quot;edu&quot; n.model&lt;- lm(BOSTON1~1,data=dat1) all&lt;- lm(BOSTON1~.,data=dat1) models&lt;-step(all, scope =formula(all),direction =&quot;backward&quot;) ## Start: AIC=95.04 ## BOSTON1 ~ edad + AEC + edu ## ## Df Sum of Sq RSS AIC ## - AEC 1 16.756 562.64 93.943 ## &lt;none&gt; 545.89 95.036 ## - edu 1 56.671 602.56 96.000 ## - edad 1 77.264 623.15 97.008 ## ## Step: AIC=93.94 ## BOSTON1 ~ edad + edu ## ## Df Sum of Sq RSS AIC ## &lt;none&gt; 562.64 93.943 ## - edu 1 81.11 643.75 95.983 ## - edad 1 131.75 694.39 98.255 summary(models) ## ## Call: ## lm(formula = BOSTON1 ~ edad + edu, data = dat1) ## ## Residuals: ## Min 1Q Median 3Q Max ## -9.4374 -2.7496 -0.2429 2.8433 8.2001 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 66.2679 8.3251 7.960 1.48e-08 *** ## edad -0.2921 0.1162 -2.514 0.0182 * ## edu 0.3424 0.1736 1.973 0.0588 . ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 4.565 on 27 degrees of freedom ## Multiple R-squared: 0.306, Adjusted R-squared: 0.2546 ## F-statistic: 5.952 on 2 and 27 DF, p-value: 0.00722 El método de selección por pasos sucesivos es una combinación de ambos y que presenta la diferencia de que en este procedimiento una variable que ha salido en una etapa puede volver a entrar en otra. Lo mismo puede decirse para aquellas variables que han entrado en un momento determinado y que pueden salir en una fase posterior (Ato &amp; Vallejo, 2007). Los resultados aplicados al modelo anterior son idénticos. library(stats) dat1&lt;- dat[,c(1,5,7,8)] n.model&lt;- lm(BOSTON1~1,data=dat1) all&lt;- lm(BOSTON1~.,data=dat1) models&lt;-step(n.model, scope =formula(all),direction =&quot;both&quot;) ## Start: AIC=100.9 ## BOSTON1 ~ 1 ## ## Df Sum of Sq RSS AIC ## + edad 1 166.95 643.75 95.983 ## + AEC 1 128.99 681.71 97.702 ## + edu 1 116.31 694.39 98.255 ## &lt;none&gt; 810.70 100.901 ## ## Step: AIC=95.98 ## BOSTON1 ~ edad ## ## Df Sum of Sq RSS AIC ## + edu 1 81.110 562.64 93.943 ## &lt;none&gt; 643.75 95.983 ## + AEC 1 41.194 602.56 96.000 ## - edad 1 166.948 810.70 100.901 ## ## Step: AIC=93.94 ## BOSTON1 ~ edad + edu ## ## Df Sum of Sq RSS AIC ## &lt;none&gt; 562.64 93.943 ## + AEC 1 16.756 545.89 95.036 ## - edu 1 81.110 643.75 95.983 ## - edad 1 131.747 694.39 98.255 summary(models) ## ## Call: ## lm(formula = BOSTON1 ~ edad + edu, data = dat1) ## ## Residuals: ## Min 1Q Median 3Q Max ## -9.4374 -2.7496 -0.2429 2.8433 8.2001 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 66.2679 8.3251 7.960 1.48e-08 *** ## edad -0.2921 0.1162 -2.514 0.0182 * ## edu 0.3424 0.1736 1.973 0.0588 . ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 4.565 on 27 degrees of freedom ## Multiple R-squared: 0.306, Adjusted R-squared: 0.2546 ## F-statistic: 5.952 on 2 and 27 DF, p-value: 0.00722 En resumen pues, los tres procedimientos obtienen al final un modelo de regresión en el que la única variable predictora significativa es la edad. Los resultados finales del modelo se incluyen en la siguiente tabla: 6.3.2 Importancia de las variables No existe un criterio unívoco para determinar la importancia de una variable en el modelo de regresión. En ocasiones, se considera como más importante el que una variable contribuya en mayor medida al cambio esperado en la variable Y. En este caso, la importancia de las variables se puede establecer en base a los (Pardo &amp; San Martin, 2011). Sin embargo, también puede utilizarse el criterio de aquella variable que más contribuye al ajuste global. Si es el caso, entonces ha de utilizarse el cuadrado del para determinar la importancia de cada variable. "],["supuestos-del-modelo.html", "6.4 Supuestos del modelo", " 6.4 Supuestos del modelo Una vez obtenido el modelo conviene preguntarse por la idoneidad del mismo. Además del supuesto de linealidad ya mencionado, tambien es conveniente evaluar la normalidad de los errores y la homogeneidad de las varianzas de la variable Y condicionada a cada valor de X. Asimismo, es conveniente determinar la independencia de las observaciones. 6.4.0.1 Linealidad El modelo de regresión asume que las relaciones entre las variables son lineales. Sin embargo, no todas las relaciones son de esta naturaleza. Por ejemplo, en la figura 4 podemos observar un tipo de relación curvilínea: Figura 6.4: Relación entre los años de escolarización y el tiempo en realizar el TMT-B Cuando se dan este tipo de relaciones no es adecuado aplicar los modelos de regresión aqui estudiados, ya que la interpretación del parámetro \\(\\beta_{1}\\) sería incorrecta (Hayes, 2013). En estas circunstancias han de utilizarse otro tipo de técnicas que superan el alcance de este texto. 6.4.0.2 Normalidad Este supuesto implica que los errores del modelo siguen una distribución normal. Para evaluar este modelo basta con calcular la diferencia entre las puntuaciones observadas y las predichas (residuales) y aplicar el estadístico de Shapiro-Wilks (SW) para determinar su significación. Aplicado al ejemplo de la figura 1 tenemos: shapiro.test(mod$residuals) ## ## Shapiro-Wilk normality test ## ## data: mod$residuals ## W = 0.96605, p-value = 0.4376 Obsérvese que el test SW no permite rechazar la hipótesis nula de normalidad de la distribución. 6.4.0.3 Homocedasticidad Este supuesto indica que los errores son homogéneos para cada \\(\\hat{Y}\\). Existen varios métodos para evaluar este supuesto. El más fácil es mediante la evaluación visual de los residuales del modelo representados gráficamente frente a los valores predichos tal y como se realiza en la figura 5. Puede observarse que los errores se reparten de manera homogénea entre los distintos valores predichos.El resultado del incumplimiento de este supuesto implica reducir la potencia estadística de la prueba. Asimismo, los intervalos de confianza se ven afectados por la de los errores. Figura 6.5: Gráfico para estudiar la homocedasticidad 6.4.0.4 Independencia Este supuesto implica que la información contenida en los errores de un sujeto no me proporciona información sobre los errores de otros sujetos presentes en el estudio. Por ejemplo, supongamos que estamos interesados en estudiar el rendimiento de los niños en un centro escolar. Obviamente, el rendimiento de los escolares puede depender de la calidad del profesor del que reciben sus enseñanzas. En este caso, todos los alumnos procedentes de un buen profesor presentarán mejores resultados que aquellos que recibieron sus enseñanzas con otro profesor de peor calidad. A la hora de analizar este modelo, los alumnos del profesor “excelente” tenderán a ser positivos en comparación con los de otros niños con peores profesores. El incumplimiento del supuesto de independencia afecta a los supuestos del modelo de regresión de la misma forma que el supuesto de homocedasticidad. Los errores estándar son infraestimados afectándose también los intervalos de confianza. 6.4.0.5 Colinealidad Existe colinealidad cuando dos variables predictoras presentan correlaciones my altas. O lo que es lo mismo, existe una relación funcional entre ambas. Si dos variables muestran una colinealidad alta, el procedimiento de estimación de mínimos cuadrados se verá afectado obteniéndose parámetros sesgados (la varianza de los coeficientes de regresión aumentan de manera considerable). Para detectar la existencia de una colinealidad alta el programa estadístico SPSS muestra dos estadísticos denominados (1 - \\(R_{j}^{2}\\)) y los (FIV). Este último es el inverso del índice de tolerancia. Valores mayores que 10 en los FIV suelen ir acompañados de los problemas de estimación asociados a un exceso de colinealidad. Para solucionar los problemas de colinealidad se tienen varias propuestas. Eliminar alguna de las variables que presentan una relación muy alta (esta es la opción que utiliza SPSS cuando se elige la opción de pasos sucesivos). Otra posibilidad es agrupar las distintas variables altamente relacionadas mediante la técnica de . Por último, señalar que cuando se incluyen interacciones entre variables en el modelo es frecuente que aparezcan valores altos de FIV (factores de inflación de la varianza) que pueden solventarse centrando las variables predictoras. 6.4.0.6 Casos atípicos Para considerar adecuada una recta de regresión, las predicciones deben ser adecuadas. Mediante una primera exploración puede determinarse la existencia de anomalías tanto en la variable dependiente como en las variables predictoras. Para determinar anomalías en la variable dependiente conviene realizar una exploración de los residuos (\\(E_{j}= Y_{j} - \\hat{Y}_{j}\\)). Por lo general, son aceptables todos aquellos residuos que se encuentren por debajo de 3 (en valor absoluto). Una forma de evaluar que se cumple este supuesto es mediante un gráfico de las puntuaciones predichas y estos residuos. Aplicado al modelo estudiado anteriormente (relación entre edad y medida 1 del Boston) la gráfica es la siguiente: Figura 6.6: Residuales del modelo Observamos que todos los residuales se encuentran dentro del rango (-1,2), indicando un buen ajuste del modelo. Al igual que existen puntuaciones anómalas con respecto a la variable Y, también pueden aparecer puntuaciones atípicas en relación con los valores de los predictores (). Para medir la influencia de este tipo de puntuaciones se calculan los índices \\(h_{i}\\) que representan el grado de alejamiento del conjunto de puntuaciones de un caso con respecto a las puntuaciones medias de todos los casos. Pardo &amp; San Martin (2011) señalan que una regla que funciona bien es revisar aquellos valores \\(h_{i}\\) por encima de 0.5. Una medida basada en los \\(h_{i}\\) es el estadístico distancias de Cook (\\(D_{i}\\)) que es la suma de los cambios que se producen en los coeficientes de regresión al ir eliminando cada caso del análisis: \\[ D_{i}=[h_{i}E_{S_{i}^{2}}]/[(p+1)(1-h_{i})] \\] donde \\(h_{i}\\) es el grado en el que una puntuación es atípica en \\(X_{j}\\) y \\(E_{S_{i}}\\) es el grado en que una puntuación es atípica con respecto a Y. Se considera que un caso debe ser considerado influyente cuando \\(D_{i} &gt; 1\\). En la siguiente gráfica observamos que las puntuaciones \\(D_{i}\\) del modelo que relaciona la edad con la medida 1 del Boston se encuentran por debajo de 1: Figura 6.7: Leverage del modelo 6.4.0.7 Interpretación de la regresión múltiple Para ver como se interpretan los parámetros de un modelo de regresión múltiple vamos a utilizar el ejemplo desarrollado anteriormente en el que se quiso relacionar el efecto el efecto de la educación formal y las actividades estimulantes (AEC) sobre la atención. El modelo a contrastar sería el siguiente: \\[ Atención_{i} = \\beta_{0} + \\beta_{1}*edad + \\beta_{2}*educación + \\epsilon_{i} \\] Los resultados que proporciona el programa R son los siguientes: mod4&lt;- lm(atencion~AEC+edu,data=dat) summary(mod4) ## ## Call: ## lm(formula = atencion ~ AEC + edu, data = dat) ## ## Residuals: ## Min 1Q Median 3Q Max ## -68.157 -7.562 -0.822 9.575 38.855 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -170.1927 19.0477 -8.935 1.50e-09 *** ## AEC 5.1627 0.5497 9.392 5.35e-10 *** ## edu 17.2696 0.9582 18.024 &lt; 2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 24.23 on 27 degrees of freedom ## Multiple R-squared: 0.9549, Adjusted R-squared: 0.9516 ## F-statistic: 286.1 on 2 and 27 DF, p-value: &lt; 2.2e-16 En base a estos resultados el modelo quedaría explicitado mediante la siguiente ecuación: \\[ Atención_{i} = -170.193 + 5.163*aec + 17.270*educación + \\epsilon_{i} \\] Por tanto, las conclusiones siguiendo normas APA serían las siguientes: "],["regresión-múltiple-y-modelos-estadísticos.html", "6.5 Regresión múltiple y modelos estadísticos", " 6.5 Regresión múltiple y modelos estadísticos Supongamos que estamos interesados en estudiar como influyen la edad y la educación en el tiempo de ejecución de la prueba TMT-B en el momento 1. Caben distintas hipótesis para entender las relaciones entre estas tres variables. La primera es considerar que el efecto de cada predictor es independiente. En este caso un modelo gráfico lo representaría de la siguiente forma: Figura 6.8: Modelo de regresión múltiple con relaciones directas La representación estadística de este modelos sería la siguiente: \\[ Y_{i} = \\beta_{0} + \\beta_{1}*edad + \\beta_{2}*educación + \\epsilon_{i} \\] Sin embargo, con estas mismas tres variables es posible plantearse otras posibilidades de relación. Así, por ejemplo, podemos plantearnos que la relación entre la edad y el tiempo necesitado en completar la prueba del TMT-B está por la variable educación. En la siguiente figura aparece reflejado este modelo: Figura 6.9: Modelo de moderación El modelo estadístico que representa esta hipótesis es el siguiente: \\[ Y_{i} = \\beta_{0} + \\beta_{1}*edad + \\beta_{2}*educación + \\beta_{3}*educación*edad + \\epsilon_{i} \\] Otra posibilidad sería considerar que la variable educación la relación entre la edad y la respuesta al TMT-B en la primera medición. Este modelo vendría representado de la siguiente forma: Figura 6.10: Modelo de mediación Para contrastar este modelo son necesarias varias ecuaciones de regresión. En primer lugar, es necesario determinar la regresión de la variable mediadora (educación) sobre la variable predictora (edad): \\[ educación_{i} = \\beta_{0} + \\beta_{1}*edad + \\epsilon_{i} \\] Asimismo, es necesario obtener la recta de regresión de la variable dependiente (TMTB) sobre la variable edad, estando presente la variable mediadora (educación): \\[ Y_{i} = \\beta_{0} + \\beta_{1}*edad + \\beta_{2}*educación + \\epsilon_{i} \\] A partir de estas dos regresiones puede determinarse los efectos mediadores de la variable educación. Los conceptos de y así como los procedimientos estadísticos para determinarlos se estudiarán en los siguientes apartados. 6.5.0.1 Moderación con variables cualitativas Consideremos que nos encontramos en una situación donde la variable actividades estimulantes (aec) es de naturaleza categórica (0 = baja estimulación y 1= alta estimulación), el modelo de regresión donde se quiere estudiar el efecto interactivo entre esta variable y la educación formal sería el siguiente: \\[ Atención_{i} = \\beta_{0} + \\beta_{1}*aec + \\beta_{2}*educación + \\beta_{2}*aec*educación + \\epsilon_{i} \\] Los resultados obtenidos al aplicar este modelo son los siguientes: En base a estos resultados podemos observar que existe una interacción significativa entre ambas variables. La ecuación de regresión para aquellos que tienen estimulación cognitiva alta (aec = 1) sería: \\[ Atención_{i} = 9.956 -28.389*1 + 13.052*educación + 9.874*educación*1 + \\epsilon_{i} \\] Resolviendo: \\[ Atención_{i} = -18.433 + 22.926*educación + \\epsilon_{i} \\] En el caso de las personas con baja estimulación cognitiva (aec = 0) la ecuación de regresión sería la siguiente: \\[ Atención_{i} = 9.956 -28.389*0 + 13.052*educación + 9.874*educación*0 + \\epsilon_{i} \\] Simplificando: \\[ Atención_{i} = 9.956 + 13.052*educación + \\epsilon_{i} \\] Figura 6.11: Relación entre la educación y la atención en función de las AEC Gráficamente puede verse como la pendiente de la relación entre la educación formal y la atención es mayor cuando existe estimulación cognitiva alta que cuando la estimulación cognitiva es baja. O lo que es lo mismo: niveles altos y bajos de educación presentan mayores niveles de atención cuando la estimulación cognitiva es alta en comparación con los niveles bajos de estimulación cognitiva. 6.5.0.2 Moderación con variables cuantitativas El procedimiento para interpretar la interacción en un modelo de regresión múltiple cuando todas las variables son cuantitativas es bastante parecido al caso en el que una variable predictora que participa en la interacción es cualitativa. Lo que se hace es dicotomizar la variable predictora cuantitativa. Por convenio, se suele considerar el valor de \\(1.5\\sigma\\) como un valor alto en la variable predictora y \\(-1.5\\sigma\\) como valor bajo de la variable. La única diferencia con el procedimiento anterior es que para evitar los problemas de colinealidad se recomienda centrar las variables (Cohen et al., 2003). Los resultados del análisis con las variables centradas aparecen a continuación: Observamos que los niveles de colinealidad son tolerables y que el efecto de la interacción es significativo. Para poder interpretar esta interacción seleccionamos un valor alto de estimulación cognitiva (\\(\\mu\\) +1.5\\(\\sigma\\) = 34.7 +1.58.605 = 47.685) y un valor bajo de estimulación cognitiva (\\(\\mu\\) -1.5\\(\\sigma\\) = 8.09 -1.5.842 = 21.855). Aplicando los valores de estimulación alta a la recta de regresión obtenida obtenemos los siguientes resultados: \\[ Atención_{i} = 174.124 +4.967*47.685 + 17.409*ceducación + .485*ceducación*47.685 + \\epsilon_{i} \\] Simplificando: \\[ Atención_{i} = 410.975 + 40.536*ceducación + \\epsilon_{i} \\] La ecuación para los individuos con niveles bajos de estimulación será: \\[ 174.124 +4.967*21.855 + 17.409*ceducación + .485*ceducación*21.855 + \\epsilon_{i} \\] Agrupando los términos nos queda: \\[ Atención_{i} = 282.678 + 28*educación + \\epsilon_{i} \\] Gráficamente podemos representar la interacción de la siguiente forma: Figura 6.12: Relación entre la educación y la atención en función de las AEC Mediante el gráfico puede interpretarse la interacción de la siguiente forma: 6.5.0.3 Modelo simple de mediación Existe un modelo teórico que considera que el efecto de la edad sobre el deterioro cognitivo no es constante en todos los individuos. En algunos casos, las curvas de deterioro son muy pronunciadas, mientras que en otros el deterioro es menor e, incluso, plano (Salthouse et al., 2002). La interrelación entre estas variables puede estudiarse mediante los modelos de mediación y/o moderación. En este apartado nos centraremos en el estudio del modelo simple de mediación. Bajo esta hipótesis, las personas mayores que mantienen niveles altos de estimulación cognitiva podrían retrasar (compensar) el deterioro físico natural que se produce con el paso del tiempo. Es decir, mientras más estimulación cognitiva realizan las personas mayores, mayores conexiones sinápticas se producirán en el cerebro que compensarán la pérdida natural de neuronas. Por tanto, mientras más estimulación cognitiva tengamos mejor será nuestro funcionamiento cognitivo. Con este planteamiento estamos considerando que la estimulación cognitiva está mediando la relación existente entre la edad y el funcionamiento cognitivo. En la siguiente figura aparece el diagrama que representa este modelo: Figura 6.13: Modelo de mediación Este modelo incluye dos variables dependientes ( y ) y dos variables predictoras ( y actividades estimulantes). Bajo la hipótesis de la mediación simple se considera que el efecto de la edad se ve potenciado (o disminuido) por la realización de actividades estimulantes. El modelo asume una relación causal entre la variable edad sobre las actividades estimulantes y sobre la atención. Asimismo, en este modelo se considera también que existe una relación causal de la variable actividades estimulantes sobre la atención. Para estudiar este modelo resulta necesario estimar tanto el efecto directo como el indirecto de la variable AEC. El es el coeficiente de regresión \\(\\beta_{1}\\) de la recta de regresión Y (atención) sobre X (edad): \\[ Y_{i} = \\beta_{0} + \\beta_{1}*edad + \\epsilon_{i} \\] Los resultados de esta ecuación de regresión son: El cálculo del efecto indirecto se obtiene mediante el producto de dos coeficientes de regresión ( y ). El coeficiente es la pendiente de la recta de regresión de la variable mediadora (AEC) sobre la variable predictora (en nuestro caso la edad). Los resultados de esta recta de regresión se muestran a continuación: El coeficiente se obtiene mediante la recta de regresión de la variable dependiente (atencion) sobre las predictoras (AEC y edad). El coeficiente es el valor de que multiplica a la variable mediadora. En nuestro ejemplo sería el siguiente: El valor del coeficiente es de 7.898. Por tanto, el efecto indirecto será el producto de los coeficientes . El resultado será en nuestro caso (-0.503) \\(\\times\\) 7.898 = -3.97. Este resultado representa el efecto indirecto en nuestra muestra. Para determinar el efecto indirecto a nivel poblacional tenemos que hacer una serie de suposiciones (distribución normal del efecto indirecto) que nos permite contrastarlo con la siguiente expresión: \\[ z = \\frac{a \\times b}{se_{ab}} \\sim N(\\mu,\\sigma^{2}) \\] siendo \\(se_{ab}\\) el estimador del error estándar: \\[ se_{ab} = \\sqrt{a^{2}se_{b}^2 + b^{2}se_{a}^{2}} \\] donde \\(se_{b}\\) es el error estándar del parámetro y \\(se_{a}\\) es el error estándar del parámetro a. En nuestro ejemplo, \\(se_{a}\\) = .192 y \\(se_{b}\\) = 1.979. Por tanto: \\[ se_{ab} = \\sqrt{a^{2}se_{b}^2 + b^{2}se_{a}^{2}} = \\sqrt{(-0.503)^2*2.086^2 +7.898^2* 0.199^2} = 1.89 \\] El valor del estadístico z es el siguiente: \\[ z = \\frac{-3.97} {1.89}= -2.10 &gt; |1.96| \\] Observamos que existe un efecto indirecto significativo con un nivel de riesgo \\(\\alpha\\) = 0.05 (\\(p(z \\pm 2.10)\\) = .036). Existen distintos programas que permiten realizar los cálculos del modelo de mediación simple. Por internet existen calculadoras como la siguiente: https://datatab.es/statistics-calculator/moderator-mediator-analysis "],["análisis-de-regresión-mediante-el-programa-jamovi.html", "6.6 Análisis de regresión mediante el programa JAMOVI", " 6.6 Análisis de regresión mediante el programa JAMOVI En el programa de JAMOVI existen dos modulos para realizar el análisis de regresión lineal (módulos de regresión y de modelo lineal). En el modulo de “Regresión” existen distintas opciones para estudiar las correlaciones entre variables además de poder realizar análisis de regresión lineal. Aquí nos centraremos en el modulo de modelo lineal general: Figura 6.14: Modelo lineal general en JAMOVI Si se quiere estudiar la relación entre las dos medidas del test de Boston habrá que introducir una de las medidas en el cuadro de variables dependientes y la predictora en el cuadro de “Covariates” en el caso de que la variable sea continua o en el cuadro de “Factors” si la variable es cualitativa. Los resultados serían los siguientes: Figura 6.15: Modelo regresión lineal simple en JAMOVI Este modulo dispone de opciones para estudiar la normalidad de los residuales Figura 6.16: Modelo regresión lineal simple en JAMOVI: Supuestos En el modulo de “Regresión lineal” también se incluyen más opciones para evaluar los supuestos del modelo: Figura 6.17: Modelo regresión lineal simple en JAMOVI: Supuestos 6.6.1 Regresión lineal múltiple El procedimiento de regresión lineal múltiple se realiza de la misma manera que el de regresión lineal simple. Basta con añadir más variables en los cuadros de las variables predictoras: (#fig:rlm_jam1)Modelo de regresión lineal múltiple en JAMOVI Además de poder evaluar los supuestos, el programa de JAMOVI permite guardar los residuales y los valores predichos del modelo: (#fig:rlm_jam2)Modelo de regresión lineal múltiple en JAMOVI: Predicciones No obstante, también pueden realizarse las predicciones en JAMOVI con el modulo R que permite disponer de muchas funciones de R mediante su sintaxis. En el siguiente ejemplo vemos las instrucciones para obtener tres tipos de predicciones de un individuo con una edad de 45 años y con 20 años de estudios reglados: 1) Valor esperado (línea 6), 2) pronóstico con respecto a la media (línea 7) y 3) pronóstico con respecto al individuo (línea 8): (#fig:rlm_jam3)Modelo regresión lineal múltiple en JAMOVI: Predicciones 6.6.2 Modelo de moderación con JAMOVI El estudio de la moderación con JAMOVI se realiza con el mismo modulo de”Modelo lineal general”, indicándole al programa que se desea estudiar la interacción en la pestaña de “Model”: (#fig:mod_jam1)Modelo de moderación en JAMOVI Los resultados obtenidos para estudiar si el efecto de la educación modera la relación entre la edad y la atención se presentan a continuación: (#fig:mod_jam2)Modelo de moderación en JAMOVI Este programa nos permite representar gráficamente la moderación considerando que la variable moderadora presenta 3 niveles: bajo, medio y alto. Asimismo, el programa nos proporciona información de las pendientes de cada uno de estos niveles: (#fig:mod_jam3)Modelo de moderación en JAMOVI: Efectos simples 6.6.3 Modelos de mediación En el programa de JAMOVI existe un modulo específico para el estudio de los modelos de mediación denominado “GLM Mediation Model”: (#fig:med_jam1)Modelo de mediación simple en JAMOVI Para realizar el análisis de mediación simple basta con introducir la variable independiente en el cuadro de “Covariates” si la variable predictora es continua y en el de “Factors” si la variable es cualitativa. En el cuadro de “Dependent Variable” introduciremos la variable dependiente y en el cuadro de “Mediators” la(s) variable(s) mediadora. (#fig:med_jam2)Modelo de mediación simple en JAMOVI Los resultados del modelo se presentan en la siguiente figura: (#fig:med_jam3)Resultados del modelo de mediación simple en JAMOVI "],["análisis-de-regresión-mediante-el-programa-spss.html", "6.7 Análisis de regresión mediante el programa SPSS", " 6.7 Análisis de regresión mediante el programa SPSS El programa SPSS tiene un modulo específico para obtener las correlaciones: Figura 6.18: Módulo para realizar correlaciones en SPSS Figura 6.19: Resultados de la opción de correlaciones en SPSS Para realizar el modelo de regresión lineal hay que seleccionar la opción siguiente: Figura 6.20: Modulo para realizar el modelo de regresión en SPSS Figura 6.21: Resultados el modelo de regresión en SPSS 6.7.1 Selección de modelos en el programa SPSS 6.7.1.1 Procedimiento de pasos sucesivos Este procedimiento puede realizarse hacia adelante, hacia atrás o mediante una combinación de ambos procedimientos. El método de selección hacia adelante comienza con el modelo nulo (modelo sin predictores) empezando a introducir los predictores uno a uno. Se incluye el predictor con menor nivel de significación (p(F)&lt;.05). A continuación, se vuelve a realizar un análisis de regresión sobre la variable dependiente sin el predictor previamente introducido. Nuevamente, se vuelve a seleccionar aquel predictor con menor nivel de significación. Este procedimiento continua hasta que los predictores restantes no cumplen el criterio o no queda ningún predictor por introducir. Continuando con el mismo ejemplo, vamos a utilizar el procedimiento hacia adelante para seleccionar un modelo. Para ello, seleccionamos en el SPSS y marcamos las opciones de tal y como aparece en la siguiente figura: Mediante este procedimiento sólo se incluye la variable edad, ya que es la única que cumple el criterio (SPSS utiliza como criterio de entrada \\(p(F) \\leq.05)\\). El resto de variables son excluidas por no cumplir dicho criterio. El método de selección hacia atrás comienza con el modelo máximo. En cada paso, se elimina aquel predictor que cumple el criterio de eliminación (p(F)&gt;.05) y con mayor nivel de significación. Este procedimiento continua hasta que todos los predictores presentes en el modelo son significativos o no quedan predictores en el modelo. Utilizando el mismo modelo, pero cambiando la opción de Método por tal y como aparece en la siguiente figura nos encontramos que el programa comienza con el modelo máximo (las tres variables incluidas) y elimina aquellas variables que no cumplen el criterio (p(F)&lt; .1). el resultado final es el mismo que en el caso anterior: El método de selección por pasos sucesivos es una combinación de ambos y que presenta la diferencia de que en este procedimiento una variable que ha salido en una etapa puede volver a entrar en otro. Lo mismo puede decirse para aquellas variables que han entrado en un momento determinado y que pueden salir en una fase posterior (Ato &amp; Vallejo, 2007). Los resultados aplicados al modelo anterior son idénticos. En resumen pues, los tres procedimientos obtienen al final un modelo de regresión en el que la única variable predictora significativa es la edad. Los resultados finales del modelo se incluyen en la siguiente tabla: 6.7.2 Modelos de moderación y mediación con el módulo de PROCESS (Hayes, 2013) Una vez instalado aparece dentro del módulo de regresión de SPSS: Figura 6.22: Módulo de PROCESS en SPSS Para estudiar ambos modelos se introducen las variables y se selecciona el modelo 1 para estudiar la moderación y el 4 para el modelo simple de mediación. Figura 6.23: Interfaz del módulo PROCESS en SPSS para el estudio de la moderación El módulo de PROCESS para analizar la interacción presenta una serie de opciones para ajustar los resultados del modelo de moderación en función de las características de nuestros datos: Figura 6.24: Opciones del modelo de moderación en SPSS con el módulo PROCESS Los resultados de este modelo se presentan a continuación: Figura 6.25: Resultados del modelo de moderación en SPSS con el módulo PROCESS El modelo de mediación presenta la misma interfaz que el modelo de moderación. Simplemente habrá que indicar la opción 4 en el cuadro de “Model number”: Figura 6.26: Resultados del modelo de moderación en SPSS con el módulo PROCESS Los resultados del modelo de mediación se presentan a continuación: Figura 6.27: Resultados del modelo de moderación en SPSS con el módulo PROCESS "],["referencias.html", "7 Referencias", " 7 Referencias Anguera, M., Arnau, J., Ato, M., Martinez, M., &amp; Vallejo, G. (1995). Métodos de investigación en psicología. Sintesis. Ato, M., &amp; Vallejo, G. (2007). Diseños experimentales en psicología. Pirámide. Ato, M., &amp; Vallejo, G. (2015). Diseños de investigación en psicología. Pirámide. Cohen, J., Cohen, P., West, S. G., &amp; Aiken, L. S. (2003). Applied multiple regression/correlation analysis for the behavioral sciences (Third). LEA. Hayes, A. F. (2013). Introduction to mediation, moderation, and conditional process analysis. Guilford. IBM-Corp. (2023). IBM SPSS statistics for windows (version 29). IBM Corp. León, O., &amp; Montero, I. (2002). Métodos de investigación en psicología y educación. McGraw-Hill. Luque Calvo, P. L. (2017). Escribir un trabajo fin de estudios con r markdown. http://destio.us.es/calvo Maxwell, S. E., &amp; Delaney, H. D. (2004). Designing experiments and analyzing data: A model comparison perspective (2nd ed.). Taylor-Francis. Pardo, A., &amp; San Martin, R. (2010). Análisis de datos en ciencias sociales y de la salud II. Síntesis. Pardo, A., &amp; San Martin, R. (2011). Análisis de datos en las ciencias sociales y de la salud II. Síntesis. R Core Team. (2016). R: A language and environment for statistical computing. R Foundation for Statistical Computing. https://www.R-project.org/ Salthouse, T. A., Berish, D. E., &amp; Miles, J. D. (2002). The role of cognitive stimulation on the relations between age and cognitive functioning. Psychology and Aging, 4, 548–557. The Jamovi Team. (2022). Jamovi (versión 2.3) [computer software]. ([Computer Software]). Retrieved from https://www.jamovi.org Ugarte, M., Militino, A., &amp; Arnholt, A. (2008). Probability and statistics with R. Chapman. Xie, Y. (2023). Knitr: A general-purpose package for dynamic report generation in r. https://yihui.org/knitr/ "]]
